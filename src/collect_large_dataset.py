#!/usr/bin/env python3
"""
å¤§è§„æ¨¡æ•°æ®é‡‡é›†è„šæœ¬ - æ”¯æŒè·å–10ä¸‡+æ•°æ®
ä½¿ç”¨çœŸå®APIæˆ–æ‰©å±•æ¨¡æ‹Ÿæ•°æ®
"""

import sys
import os
import time
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import argparse

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import DataConfig, LabelConfig, DataSplitConfig, FeatureConfig
from tiger1 import get_kline_data, calculate_indicators, FUTURE_SYMBOL


class LargeDatasetCollector:
    """å¤§è§„æ¨¡æ•°æ®é›†é‡‡é›†å™¨"""
    
    def __init__(self, use_real_api=False, days=30, max_records=100000):
        """
        åˆå§‹åŒ–
        
        Args:
            use_real_api: æ˜¯å¦ä½¿ç”¨çœŸå®API
            days: è·å–å¤©æ•°
            max_records: æœ€å¤§è®°å½•æ•°
        """
        self.use_real_api = use_real_api
        self.days = days
        self.max_records = max_records
        
        # æ›´æ–°é…ç½®
        DataConfig.USE_REAL_API = use_real_api
        DataConfig.DAYS_TO_FETCH = days
        DataConfig.MAX_RECORDS = max_records
        DataConfig.COUNT_1MIN = min(days * DataConfig.BARS_PER_DAY_1MIN, max_records)
        DataConfig.COUNT_5MIN = min(days * DataConfig.BARS_PER_DAY_5MIN, max_records // 5)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = DataConfig.OUTPUT_DIR
        os.makedirs(self.output_dir, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.log_file = os.path.join(self.output_dir, f'collection_log_{timestamp}.txt')
        
        self._log("=" * 80)
        self._log("ğŸ“¥ å¤§è§„æ¨¡æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–")
        self._log("=" * 80)
        self._log(f"ä½¿ç”¨çœŸå®API: {self.use_real_api}")
        self._log(f"ç›®æ ‡å¤©æ•°: {self.days}")
        self._log(f"æœ€å¤§è®°å½•æ•°: {self.max_records}")
        self._log(f"1åˆ†é’ŸKçº¿ç›®æ ‡: {DataConfig.COUNT_1MIN}")
        self._log(f"5åˆ†é’ŸKçº¿ç›®æ ‡: {DataConfig.COUNT_5MIN}")
        self._log("=" * 80)
    
    def _log(self, message):
        """è®°å½•æ—¥å¿—"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_message = f"[{timestamp}] {message}"
        print(log_message)
        
        with open(self.log_file, 'a', encoding='utf-8') as f:
            f.write(log_message + '\n')
    
    def fetch_kline_data_with_retry(self, period, count, max_retries=3):
        """
        å¸¦é‡è¯•çš„Kçº¿æ•°æ®è·å–
        
        Args:
            period: å‘¨æœŸ
            count: æ•°é‡
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        
        Returns:
            DataFrame
        """
        for attempt in range(max_retries):
            try:
                self._log(f"  å°è¯•è·å– {period} æ•°æ® (ç¬¬ {attempt + 1}/{max_retries} æ¬¡)...")
                
                if self.use_real_api:
                    # ä½¿ç”¨çœŸå®APIè·å–æ•°æ®
                    # å¦‚æœcountå¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦åˆ†æ‰¹è·å–
                    if count > 10000:
                        self._log(f"    æ•°æ®é‡å¤§äº10000ï¼Œå°†åˆ†æ‰¹è·å–...")
                        return self._fetch_in_batches(period, count)
                    else:
                        df = get_kline_data([FUTURE_SYMBOL], period, count=count)
                else:
                    # æ¨¡æ‹Ÿæ¨¡å¼ï¼šæ‰©å±•ç”Ÿæˆæ•°æ®
                    self._log(f"    æ¨¡æ‹Ÿæ¨¡å¼ï¼šç”Ÿæˆ {count} æ¡æ•°æ®...")
                    df = self._generate_mock_data(period, count)
                
                if not df.empty:
                    self._log(f"  âœ… æˆåŠŸè·å– {len(df)} æ¡ {period} æ•°æ®")
                    return df
                else:
                    self._log(f"  âš ï¸ æ•°æ®ä¸ºç©ºï¼Œé‡è¯•...")
                    time.sleep(2)
                    
            except Exception as e:
                self._log(f"  âŒ è·å–å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    import traceback
                    self._log(traceback.format_exc())
        
        return pd.DataFrame()
    
    def _fetch_in_batches(self, period, total_count, batch_size=10000):
        """
        åˆ†æ‰¹è·å–å¤§é‡æ•°æ®
        
        Args:
            period: å‘¨æœŸ
            total_count: æ€»æ•°é‡
            batch_size: æ‰¹æ¬¡å¤§å°
        
        Returns:
            DataFrame
        """
        all_data = []
        num_batches = (total_count + batch_size - 1) // batch_size
        
        self._log(f"    å°†åˆ† {num_batches} æ‰¹è·å–ï¼Œæ¯æ‰¹ {batch_size} æ¡")
        
        for i in range(num_batches):
            count = min(batch_size, total_count - i * batch_size)
            self._log(f"    æ‰¹æ¬¡ {i+1}/{num_batches}: è·å– {count} æ¡...")
            
            # è®¡ç®—æ—¶é—´èŒƒå›´
            end_time = datetime.now() - timedelta(days=i * batch_size // DataConfig.BARS_PER_DAY_1MIN)
            start_time = end_time - timedelta(days=count // DataConfig.BARS_PER_DAY_1MIN)
            
            df = get_kline_data(
                [FUTURE_SYMBOL], 
                period, 
                count=count,
                start_time=start_time,
                end_time=end_time
            )
            
            if not df.empty:
                all_data.append(df)
                self._log(f"      âœ… è·å– {len(df)} æ¡")
            else:
                self._log(f"      âš ï¸ æ‰¹æ¬¡ {i+1} æ•°æ®ä¸ºç©º")
            
            time.sleep(1)  # é¿å…APIé™æµ
        
        if all_data:
            result = pd.concat(all_data, ignore_index=False)
            result = result.sort_index()
            result = result[~result.index.duplicated(keep='first')]
            return result
        else:
            return pd.DataFrame()
    
    def _generate_mock_data(self, period, count):
        """
        ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºDemoæ¨¡å¼ï¼‰
        
        Args:
            period: å‘¨æœŸ
            count: æ•°é‡
        
        Returns:
            DataFrame
        """
        # ç”Ÿæˆæ—¶é—´ç´¢å¼•
        if period == '1min':
            freq = '1T'
        elif period == '5min':
            freq = '5T'
        else:
            freq = '1H'
        
        end_time = datetime.now()
        time_index = pd.date_range(end=end_time, periods=count, freq=freq)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®ï¼ˆå¸¦è¶‹åŠ¿å’Œæ³¢åŠ¨ï¼‰
        base_price = 90.0
        trend = np.linspace(0, 5, count)  # ä¸Šå‡è¶‹åŠ¿
        volatility = np.random.randn(count) * 0.5  # éšæœºæ³¢åŠ¨
        prices = base_price + trend + volatility
        
        # ç”ŸæˆOHLCæ•°æ®
        data = {
            'open': prices,
            'high': prices + np.abs(np.random.randn(count) * 0.2),
            'low': prices - np.abs(np.random.randn(count) * 0.2),
            'close': prices + np.random.randn(count) * 0.1,
            'volume': np.random.randint(100, 1000, count)
        }
        
        df = pd.DataFrame(data, index=time_index)
        return df
    
    def calculate_features_optimized(self, df_5m, df_1m):
        """
        ä¼˜åŒ–çš„æ‰¹é‡ç‰¹å¾è®¡ç®—
        
        Args:
            df_5m: 5åˆ†é’Ÿæ•°æ®
            df_1m: 1åˆ†é’Ÿæ•°æ®
        
        Returns:
            DataFrame
        """
        self._log("\n" + "=" * 80)
        self._log("å¼€å§‹æ‰¹é‡è®¡ç®—ç‰¹å¾ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
        self._log(f"5åˆ†é’Ÿæ•°æ®: {len(df_5m)} æ¡")
        self._log(f"1åˆ†é’Ÿæ•°æ®: {len(df_1m)} æ¡")
        
        features_list = []
        min_len = DataConfig.MIN_REQUIRED_BARS
        window_size = DataConfig.WINDOW_SIZE
        
        if len(df_5m) < min_len or len(df_1m) < min_len:
            self._log(f"âš ï¸ æ•°æ®ä¸è¶³ï¼Œéœ€è¦è‡³å°‘ {min_len} æ¡")
            return pd.DataFrame()
        
        total = len(df_5m) - min_len
        last_progress = 0
        
        for i in range(min_len, len(df_5m)):
            # è¿›åº¦æ›´æ–°
            progress = int((i - min_len) / total * 100)
            if progress >= last_progress + 10:
                self._log(f"  è¿›åº¦: {progress}% ({i-min_len}/{total})")
                last_progress = progress
            
            try:
                window_5m = df_5m.iloc[max(0, i-window_size):i+1]
                timestamp_5m = df_5m.index[i]
                df_1m_slice = df_1m[df_1m.index <= timestamp_5m]
                
                if len(df_1m_slice) < min_len:
                    continue
                
                window_1m = df_1m_slice.iloc[-window_size:]
                # æ³¨æ„ï¼šcalculate_indicators å‚æ•°é¡ºåºæ˜¯ (df_1m, df_5m)
                inds = calculate_indicators(window_1m, window_5m)
                
                if '5m' not in inds or '1m' not in inds:
                    continue
                
                price_current = inds['1m']['close']
                atr = inds['5m']['atr']
                rsi_1m = inds['1m']['rsi']
                rsi_5m = inds['5m']['rsi']
                
                grid_upper = price_current * 1.01
                grid_lower = price_current * 0.99
                buffer = max(atr * 0.3, 0.0025)
                threshold = grid_lower + buffer
                
                boll_upper = inds['5m'].get('boll_upper', 0)
                boll_mid = inds['5m'].get('boll_mid', 0)
                boll_lower = inds['5m'].get('boll_lower', 0)
                
                volume_1m = inds['1m'].get('volume', 0)
                
                # ä»·æ ¼åŠ¨é‡ç‰¹å¾
                if len(window_5m) > 1 and 'close' in window_5m.columns:
                    price_change_1 = (price_current - window_5m['close'].iloc[-2]) / window_5m['close'].iloc[-2] * 100
                    price_change_5 = (price_current - window_5m['close'].iloc[-6]) / window_5m['close'].iloc[-6] * 100 if len(window_5m) > 5 else 0
                else:
                    price_change_1 = 0
                    price_change_5 = 0
                
                # æ³¢åŠ¨ç‡ç‰¹å¾
                volatility = window_5m['close'].std() / window_5m['close'].mean() * 100 if (len(window_5m) > 1 and 'close' in window_5m.columns) else 0
                
                # å¸ƒæ—å¸¦ä½ç½®
                boll_position = (price_current - boll_lower) / (boll_upper - boll_lower) if (boll_upper - boll_lower) > 0 else 0.5
                
                features = {
                    'timestamp': timestamp_5m,
                    'price_current': price_current,
                    'grid_lower': grid_lower,
                    'grid_upper': grid_upper,
                    'atr': atr,
                    'rsi_1m': rsi_1m,
                    'rsi_5m': rsi_5m,
                    'buffer': buffer,
                    'threshold': threshold,
                    'near_lower': price_current <= threshold,
                    'rsi_ok': rsi_1m < 30 or (rsi_5m > 45 and rsi_5m < 55),
                    'boll_upper': boll_upper,
                    'boll_mid': boll_mid,
                    'boll_lower': boll_lower,
                    'boll_position': boll_position,
                    'volume_1m': volume_1m,
                    'price_change_1': price_change_1,
                    'price_change_5': price_change_5,
                    'volatility': volatility,
                }
                
                features_list.append(features)
                
            except Exception as e:
                if i % 1000 == 0:
                    self._log(f"  âš ï¸ è®¡ç®—ç‰¹å¾æ—¶å‡ºé”™ (ç´¢å¼• {i}): {e}")
                    import traceback
                    self._log(f"  è¯¦ç»†é”™è¯¯:\n{traceback.format_exc()}")
                continue
        
        df_features = pd.DataFrame(features_list)
        self._log(f"âœ… ç‰¹å¾è®¡ç®—å®Œæˆ: {len(df_features)} æ¡è®°å½•")
        
        return df_features
    
    def generate_labels(self, df):
        """ç”Ÿæˆæ ‡ç­¾ï¼ˆä½¿ç”¨é…ç½®çš„ç­–ç•¥ï¼‰"""
        self._log("\n" + "=" * 80)
        self._log("ç”Ÿæˆè®­ç»ƒæ ‡ç­¾...")
        self._log(f"ä½¿ç”¨ç­–ç•¥: {LabelConfig.STRATEGY}")
        self._log(f"å‘å‰çœ‹å‘¨æœŸ: {LabelConfig.LOOK_AHEAD}")
        
        df = df.copy()
        look_ahead = LabelConfig.LOOK_AHEAD
        
        # è®¡ç®—æœªæ¥ä»·æ ¼å˜åŒ–
        price_changes = []
        for i in range(len(df)):
            if i + look_ahead < len(df):
                current = df.iloc[i]['price_current']
                future = df.iloc[i + look_ahead]['price_current']
                pct_change = (future - current) / current * 100
            else:
                pct_change = 0
            price_changes.append(pct_change)
        
        df['future_price_change'] = price_changes
        
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆæ ‡ç­¾
        if LabelConfig.STRATEGY == 'percentile':
            df = self._label_percentile(df, look_ahead)
        elif LabelConfig.STRATEGY == 'std':
            df = self._label_std(df, look_ahead)
        elif LabelConfig.STRATEGY == 'hybrid':
            df = self._label_percentile(df, look_ahead)
            df = self._label_std(df, look_ahead)
            df = self._label_hybrid(df)
        else:
            self._log(f"âš ï¸ æœªçŸ¥ç­–ç•¥ {LabelConfig.STRATEGY}ï¼Œä½¿ç”¨ç™¾åˆ†ä½æ•°")
            df = self._label_percentile(df, look_ahead)
        
        # ä½¿ç”¨ä¸»æ ‡ç­¾åˆ—
        if 'label_' + LabelConfig.STRATEGY in df.columns:
            df['label'] = df['label_' + LabelConfig.STRATEGY]
        elif 'label_percentile' in df.columns:
            df['label'] = df['label_percentile']
        
        # æ‰“å°æ ‡ç­¾åˆ†å¸ƒ
        if 'label' in df.columns:
            counts = df['label'].value_counts().sort_index()
            self._log(f"\næ ‡ç­¾åˆ†å¸ƒ:")
            for label, count in counts.items():
                label_name = {0: "æŒæœ‰", 1: "ä¹°å…¥", 2: "å–å‡º"}.get(label, "æœªçŸ¥")
                self._log(f"  {label_name} ({label}): {count} ({count/len(df)*100:.1f}%)")
        
        return df
    
    def _label_percentile(self, df, look_ahead):
        """ç™¾åˆ†ä½æ•°æ ‡æ³¨"""
        changes = df['future_price_change'].values[:-look_ahead]
        buy_threshold = np.percentile(changes, LabelConfig.PERCENTILE_BUY)
        sell_threshold = np.percentile(changes, LabelConfig.PERCENTILE_SELL)
        
        self._log(f"  ç™¾åˆ†ä½æ•°é˜ˆå€¼: ä¹°å…¥>{buy_threshold:.6f}%, å–å‡º<{sell_threshold:.6f}%")
        
        df['label_percentile'] = 0
        for i in range(len(df) - look_ahead):
            change = df.iloc[i]['future_price_change']
            if change > buy_threshold:
                df.iloc[i, df.columns.get_loc('label_percentile')] = 1
            elif change < sell_threshold:
                df.iloc[i, df.columns.get_loc('label_percentile')] = 2
        
        return df
    
    def _label_std(self, df, look_ahead):
        """æ ‡å‡†å·®æ ‡æ³¨"""
        changes = df['future_price_change'].values[:-look_ahead]
        mean = changes.mean()
        std = changes.std()
        
        buy_threshold = mean + std * LabelConfig.STD_MULTIPLIER
        sell_threshold = mean - std * LabelConfig.STD_MULTIPLIER
        
        self._log(f"  æ ‡å‡†å·®é˜ˆå€¼: ä¹°å…¥>{buy_threshold:.6f}%, å–å‡º<{sell_threshold:.6f}%")
        
        df['label_std'] = 0
        for i in range(len(df) - look_ahead):
            change = df.iloc[i]['future_price_change']
            if change > buy_threshold:
                df.iloc[i, df.columns.get_loc('label_std')] = 1
            elif change < sell_threshold:
                df.iloc[i, df.columns.get_loc('label_std')] = 2
        
        return df
    
    def _label_hybrid(self, df):
        """æ··åˆæ ‡æ³¨"""
        df['label_hybrid'] = 0
        
        for i in range(len(df)):
            votes = []
            if 'label_percentile' in df.columns:
                votes.append(df.iloc[i]['label_percentile'])
            if 'label_std' in df.columns:
                votes.append(df.iloc[i]['label_std'])
            
            if len(votes) > 0:
                buy_votes = sum(1 for v in votes if v == 1)
                sell_votes = sum(1 for v in votes if v == 2)
                
                if buy_votes >= LabelConfig.VOTE_THRESHOLD:
                    df.iloc[i, df.columns.get_loc('label_hybrid')] = 1
                elif sell_votes >= LabelConfig.VOTE_THRESHOLD:
                    df.iloc[i, df.columns.get_loc('label_hybrid')] = 2
        
        return df
    
    def split_dataset(self, df):
        """åˆ’åˆ†æ•°æ®é›†"""
        self._log("\n" + "=" * 80)
        self._log("åˆ’åˆ†æ•°æ®é›†...")
        
        if DataSplitConfig.RANDOM_SPLIT:
            # åˆ†å±‚éšæœºåˆ’åˆ†ï¼ˆç¡®ä¿å„é›†æ ‡ç­¾åˆ†å¸ƒç›¸ä¼¼ï¼‰
            self._log("ä½¿ç”¨åˆ†å±‚éšæœºåˆ’åˆ†ï¼ˆä¿æŒæ ‡ç­¾åˆ†å¸ƒå‡è¡¡ï¼‰")
            from sklearn.model_selection import train_test_split
            
            # ä½¿ç”¨stratifyå‚æ•°è¿›è¡Œåˆ†å±‚é‡‡æ ·
            train_val, test = train_test_split(
                df, 
                test_size=DataSplitConfig.TEST_RATIO,
                random_state=DataSplitConfig.RANDOM_SEED,
                stratify=df['label'] if 'label' in df.columns else None
            )
            train, val = train_test_split(
                train_val,
                test_size=DataSplitConfig.VAL_RATIO / (1 - DataSplitConfig.TEST_RATIO),
                random_state=DataSplitConfig.RANDOM_SEED,
                stratify=train_val['label'] if 'label' in train_val.columns else None
            )
        else:
            # æ—¶é—´é¡ºåºåˆ’åˆ†
            self._log("ä½¿ç”¨æ—¶é—´é¡ºåºåˆ’åˆ†")
            n = len(df)
            train_end = int(n * DataSplitConfig.TRAIN_RATIO)
            val_end = int(n * (DataSplitConfig.TRAIN_RATIO + DataSplitConfig.VAL_RATIO))
            
            train = df.iloc[:train_end].copy()
            val = df.iloc[train_end:val_end].copy()
            test = df.iloc[val_end:].copy()
        
        self._log(f"  è®­ç»ƒé›†: {len(train)} æ¡ ({len(train)/len(df)*100:.1f}%)")
        self._log(f"  éªŒè¯é›†: {len(val)} æ¡ ({len(val)/len(df)*100:.1f}%)")
        self._log(f"  æµ‹è¯•é›†: {len(test)} æ¡ ({len(test)/len(df)*100:.1f}%)")
        
        # æ‰“å°å„é›†æ ‡ç­¾åˆ†å¸ƒ
        if 'label' in df.columns:
            for name, data in [('è®­ç»ƒé›†', train), ('éªŒè¯é›†', val), ('æµ‹è¯•é›†', test)]:
                counts = data['label'].value_counts().sort_index()
                self._log(f"\n{name}æ ‡ç­¾åˆ†å¸ƒ:")
                for label, count in counts.items():
                    label_name = {0: "æŒæœ‰", 1: "ä¹°å…¥", 2: "å–å‡º"}.get(label, "æœªçŸ¥")
                    self._log(f"  {label_name}: {count} ({count/len(data)*100:.1f}%)")
        
        return train, val, test
    
    def save_datasets(self, train, val, test, full):
        """ä¿å­˜æ•°æ®é›†"""
        self._log("\n" + "=" * 80)
        self._log("ä¿å­˜æ•°æ®é›†...")
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        files = {}
        for name, data in [('train', train), ('val', val), ('test', test), ('full', full)]:
            filepath = os.path.join(self.output_dir, f'{name}_{timestamp}.csv')
            data.to_csv(filepath, index=True)
            files[name] = filepath
            self._log(f"  âœ… {name}: {filepath}")
        
        # ä¿å­˜æ•°æ®é›†ä¿¡æ¯
        info_file = os.path.join(self.output_dir, f'dataset_info_{timestamp}.txt')
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write(f"æ•°æ®é›†ä¿¡æ¯\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now()}\n\n")
            f.write(f"æ€»è®°å½•æ•°: {len(full)}\n")
            f.write(f"è®­ç»ƒé›†: {len(train)} æ¡\n")
            f.write(f"éªŒè¯é›†: {len(val)} æ¡\n")
            f.write(f"æµ‹è¯•é›†: {len(test)} æ¡\n\n")
            f.write(f"ç‰¹å¾æ•°é‡: {len(full.columns)}\n")
            f.write(f"ç‰¹å¾åˆ—è¡¨:\n")
            for col in full.columns:
                f.write(f"  - {col}\n")
        
        files['info'] = info_file
        self._log(f"  âœ… ä¿¡æ¯æ–‡ä»¶: {info_file}")
        
        return files
    
    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        try:
            start_time = time.time()
            
            # 1. è·å–Kçº¿æ•°æ®
            self._log("\n" + "=" * 80)
            self._log("æ­¥éª¤1: è·å–Kçº¿æ•°æ®")
            self._log("=" * 80)
            
            df_1m = self.fetch_kline_data_with_retry('1min', DataConfig.COUNT_1MIN)
            df_5m = self.fetch_kline_data_with_retry('5min', DataConfig.COUNT_5MIN)
            
            if df_1m.empty or df_5m.empty:
                self._log("âŒ Kçº¿æ•°æ®è·å–å¤±è´¥")
                return None
            
            # 2. è®¡ç®—ç‰¹å¾
            self._log("\n" + "=" * 80)
            self._log("æ­¥éª¤2: è®¡ç®—ç‰¹å¾")
            self._log("=" * 80)
            
            df_features = self.calculate_features_optimized(df_5m, df_1m)
            
            if df_features.empty:
                self._log("âŒ ç‰¹å¾è®¡ç®—å¤±è´¥")
                return None
            
            # 3. ç”Ÿæˆæ ‡ç­¾
            df_labeled = self.generate_labels(df_features)
            
            # 4. åˆ’åˆ†æ•°æ®é›†
            train, val, test = self.split_dataset(df_labeled)
            
            # 5. ä¿å­˜æ•°æ®é›†
            files = self.save_datasets(train, val, test, df_labeled)
            
            # å®Œæˆ
            elapsed = time.time() - start_time
            self._log("\n" + "=" * 80)
            self._log("âœ… æ•°æ®é‡‡é›†å®Œæˆï¼")
            self._log("=" * 80)
            self._log(f"æ€»è€—æ—¶: {elapsed:.2f}ç§’")
            self._log(f"æ—¥å¿—æ–‡ä»¶: {self.log_file}")
            
            return files
            
        except Exception as e:
            self._log(f"\nâŒ é‡‡é›†è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            self._log(traceback.format_exc())
            return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤§è§„æ¨¡æ•°æ®é‡‡é›†')
    parser.add_argument('--real-api', action='store_true', help='ä½¿ç”¨çœŸå®API')
    parser.add_argument('--days', type=int, default=30, help='è·å–å¤©æ•°')
    parser.add_argument('--max-records', type=int, default=100000, help='æœ€å¤§è®°å½•æ•°')
    
    args = parser.parse_args()
    
    print("\n" + "=" * 80)
    print("ğŸ“¥ å¤§è§„æ¨¡æ•°æ®é‡‡é›†å·¥å…·")
    print("=" * 80)
    print(f"ä½¿ç”¨çœŸå®API: {args.real_api}")
    print(f"ç›®æ ‡å¤©æ•°: {args.days}")
    print(f"æœ€å¤§è®°å½•æ•°: {args.max_records}")
    print("=" * 80)
    
    # åˆ›å»ºé‡‡é›†å™¨å¹¶è¿è¡Œ
    collector = LargeDatasetCollector(
        use_real_api=args.real_api,
        days=args.days,
        max_records=args.max_records
    )
    
    files = collector.run()
    
    if files:
        print("\n" + "=" * 80)
        print("âœ… æ•°æ®å·²å‡†å¤‡å°±ç»ªï¼")
        print("=" * 80)
        print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
        for key, path in files.items():
            print(f"  - {key}: {path}")


if __name__ == "__main__":
    main()
