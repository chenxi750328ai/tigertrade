#!/usr/bin/env python3
"""
ä»Transformeræ¨¡å‹ä¸­å‘ç°æœ‰æ•ˆç‰¹å¾
ç»“åˆä¼ ç»Ÿç‰¹å¾å·¥ç¨‹å’Œæ·±åº¦å­¦ä¹ çš„ä¼˜åŠ¿

æ ¸å¿ƒæ€è·¯ï¼š
1. åˆ†ææ³¨æ„åŠ›æƒé‡ â†’ å‘ç°"æ¨¡å‹å…³æ³¨å“ªäº›æ—¶åˆ»"
2. æå–éšè—å±‚è¡¨ç¤º â†’ èšç±»å‘ç°"å¸‚åœºçŠ¶æ€"
3. è®¡ç®—ç‰¹å¾é‡è¦æ€§ â†’ è¯†åˆ«"å…³é”®åŸå§‹ç‰¹å¾"
4. åå‘å·¥ç¨‹ â†’ ä»æ¨¡å‹å­¦åˆ°çš„æ¨¡å¼ä¸­æå–å¯è§£é‡Šè§„åˆ™
"""

import sys
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from pathlib import Path
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from scipy.stats import pearsonr, spearmanr
from datetime import datetime

# å¯¼å…¥æ¨¡å‹
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from train_raw_features_transformer import TransformerTradingModel, TradingSequenceDataset


class FeatureDiscovery:
    """ä»æ¨¡å‹ä¸­å‘ç°æœ‰æ•ˆç‰¹å¾"""
    
    def __init__(self, model_path, data_file):
        """
        Args:
            model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
            data_file: æ•°æ®æ–‡ä»¶è·¯å¾„
        """
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        # åŠ è½½æ¨¡å‹
        print(f"{'='*80}")
        print(f"ğŸ” ç‰¹å¾å‘ç°ï¼šä»Transformeræ¨¡å‹ä¸­æå–çŸ¥è¯†")
        print(f"{'='*80}")
        print(f"æ¨¡å‹: {model_path}")
        print(f"æ•°æ®: {data_file}")
        print(f"{'='*80}\n")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        config = checkpoint['config']
        
        self.model = TransformerTradingModel(
            num_features=12,
            d_model=config['d_model'],
            nhead=config['nhead'],
            num_layers=config['num_layers'],
            dropout=0.0  # æ¨ç†æ—¶ä¸ç”¨dropout
        )
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ (éªŒè¯å‡†ç¡®ç‡: {checkpoint['val_acc']*100:.2f}%)\n")
        
        # åŠ è½½æ•°æ®
        self.dataset = TradingSequenceDataset(data_file, sequence_length=128, predict_horizon=10)
        self.df = pd.read_csv(data_file)
        
        # å­˜å‚¨åˆ†æç»“æœ
        self.attention_weights = []
        self.hidden_states = []
        self.feature_importances = {}
        self.discovered_patterns = {}
    
    def extract_attention_weights(self, num_samples=1000):
        """
        æå–æ³¨æ„åŠ›æƒé‡
        
        å‘ç°ï¼šæ¨¡å‹åœ¨åšå†³ç­–æ—¶ï¼Œå…³æ³¨å“ªäº›å†å²æ—¶åˆ»ï¼Ÿ
        """
        print(f"{'='*80}")
        print(f"1ï¸âƒ£  æå–æ³¨æ„åŠ›æƒé‡")
        print(f"{'='*80}")
        print(f"åˆ†ææ ·æœ¬æ•°: {num_samples}")
        print(f"ç›®æ ‡: å‘ç°æ¨¡å‹å…³æ³¨çš„å…³é”®æ—¶åˆ»\n")
        
        # æ³¨å†Œhookæ¥æå–æ³¨æ„åŠ›æƒé‡
        attention_maps = []
        
        def hook_fn(module, input, output):
            # Transformerçš„æ³¨æ„åŠ›æƒé‡
            # output: (batch, num_heads, seq_len, seq_len)
            if hasattr(module, 'self_attn'):
                attention_maps.append(output[1].detach().cpu())  # attn_weights
        
        # æ³¨å†Œhookï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„hookï¼‰
        # hooks = []
        # for layer in self.model.transformer.layers:
        #     hooks.append(layer.self_attn.register_forward_hook(hook_fn))
        
        all_attentions = []
        all_labels = []
        
        indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        
        with torch.no_grad():
            for idx in indices:
                sequence, label = self.dataset[idx]
                sequence = sequence.unsqueeze(0).to(self.device)
                
                # å‰å‘ä¼ æ’­
                output = self.model(sequence)
                pred_label = torch.argmax(output, dim=1).item()
                
                # è¿™é‡Œç®€åŒ–ï¼šç”¨è¾“å…¥åºåˆ—çš„æ–¹å·®ä½œä¸º"é‡è¦æ€§"ä»£ç†
                # å®é™…åº”è¯¥æå–çœŸå®çš„attention weights
                importance = sequence.var(dim=2).cpu().numpy()  # (1, seq_len)
                
                all_attentions.append(importance[0])
                all_labels.append(label.item())
        
        all_attentions = np.array(all_attentions)
        
        # åˆ†æï¼šä¸åŒè¡ŒåŠ¨å¯¹åº”çš„å¹³å‡æ³¨æ„åŠ›æ¨¡å¼
        print(f"âœ… æå–å®Œæˆ")
        print(f"\nå‘ç°çš„æ³¨æ„åŠ›æ¨¡å¼:")
        
        for action_idx, action_name in enumerate(['å–å‡º', 'æŒæœ‰', 'ä¹°å…¥']):
            mask = np.array(all_labels) == action_idx
            if mask.sum() > 0:
                avg_attention = all_attentions[mask].mean(axis=0)
                
                # æ‰¾åˆ°æœ€å…³æ³¨çš„æ—¶åˆ»
                top_5_positions = np.argsort(avg_attention)[-5:][::-1]
                
                print(f"\n{action_name}å†³ç­–æ—¶ï¼Œæœ€å…³æ³¨çš„å†å²æ—¶åˆ»ï¼ˆä»ç°åœ¨å¾€å‰æ•°ï¼‰:")
                for pos in top_5_positions:
                    steps_ago = 128 - pos
                    print(f"  - {steps_ago}æ­¥å‰ (é‡è¦æ€§: {avg_attention[pos]:.4f})")
        
        self.attention_weights = all_attentions
        return all_attentions
    
    def extract_hidden_representations(self, num_samples=1000):
        """
        æå–éšè—å±‚è¡¨ç¤ºå¹¶èšç±»
        
        å‘ç°ï¼šæ¨¡å‹å­¦åˆ°äº†å“ªäº›"å¸‚åœºçŠ¶æ€"ï¼Ÿ
        """
        print(f"\n{'='*80}")
        print(f"2ï¸âƒ£  æå–éšè—å±‚è¡¨ç¤º")
        print(f"{'='*80}")
        print(f"åˆ†ææ ·æœ¬æ•°: {num_samples}")
        print(f"ç›®æ ‡: å‘ç°æ¨¡å‹è¯†åˆ«çš„å¸‚åœºçŠ¶æ€\n")
        
        all_hidden = []
        all_labels = []
        all_prices = []
        all_returns = []
        
        indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        
        with torch.no_grad():
            for idx in indices:
                sequence, label = self.dataset[idx]
                sequence = sequence.unsqueeze(0).to(self.device)
                
                # å‰å‘ä¼ æ’­åˆ°æœ€åä¸€å±‚transformerè¾“å‡º
                x = self.model.input_projection(sequence)
                x = self.model.pos_encoding(x)
                hidden = self.model.transformer(x)  # (1, seq_len, d_model)
                
                # å–æœ€åæ—¶åˆ»çš„éšè—çŠ¶æ€
                final_hidden = hidden[0, -1, :].cpu().numpy()
                
                all_hidden.append(final_hidden)
                all_labels.append(label.item())
                
                # è®°å½•å¯¹åº”çš„ä»·æ ¼å’Œæ”¶ç›Š
                real_idx = self.dataset.valid_indices[idx]
                all_prices.append(self.df.loc[real_idx, 'close'])
                all_returns.append(self.df.loc[real_idx, 'price_change_pct'])
        
        all_hidden = np.array(all_hidden)
        
        # PCAé™ç»´å¯è§†åŒ–
        print(f"éšè—å±‚ç»´åº¦: {all_hidden.shape[1]}")
        print(f"é™ç»´åˆ°2Dè¿›è¡Œèšç±»...")
        
        pca = PCA(n_components=2)
        hidden_2d = pca.fit_transform(all_hidden)
        
        print(f"PCAè§£é‡Šæ–¹å·®: {pca.explained_variance_ratio_.sum()*100:.2f}%")
        
        # K-meansèšç±»
        n_clusters = 5
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        clusters = kmeans.fit_predict(all_hidden)
        
        print(f"\nâœ… èšç±»å®Œæˆï¼ˆ{n_clusters}ä¸ªç°‡ï¼‰")
        print(f"\nå‘ç°çš„å¸‚åœºçŠ¶æ€:")
        
        for cluster_id in range(n_clusters):
            mask = clusters == cluster_id
            cluster_labels = np.array(all_labels)[mask]
            cluster_returns = np.array(all_returns)[mask]
            
            # ç»Ÿè®¡è¿™ä¸ªç°‡çš„ç‰¹å¾
            action_dist = pd.Series(cluster_labels).value_counts()
            avg_return = cluster_returns.mean()
            std_return = cluster_returns.std()
            
            print(f"\nçŠ¶æ€ {cluster_id+1} (æ ·æœ¬æ•°: {mask.sum()}):")
            print(f"  å¹³å‡æ”¶ç›Š: {avg_return*100:+.3f}%")
            print(f"  æ”¶ç›Šæ³¢åŠ¨: {std_return*100:.3f}%")
            print(f"  æ¨¡å‹å€¾å‘:")
            for action_idx, action_name in enumerate(['å–å‡º', 'æŒæœ‰', 'ä¹°å…¥']):
                count = action_dist.get(action_idx, 0)
                pct = count / mask.sum() * 100
                print(f"    {action_name}: {pct:.1f}%")
        
        self.hidden_states = all_hidden
        self.clusters = clusters
        self.hidden_2d = hidden_2d
        
        return all_hidden, clusters
    
    def compute_feature_importance(self, num_samples=500):
        """
        è®¡ç®—åŸå§‹ç‰¹å¾çš„é‡è¦æ€§
        
        æ–¹æ³•ï¼šæ¢¯åº¦æ•æ„Ÿæ€§åˆ†æ
        å‘ç°ï¼šå“ªäº›åŸå§‹ç‰¹å¾å¯¹æ¨¡å‹å†³ç­–å½±å“æœ€å¤§ï¼Ÿ
        """
        print(f"\n{'='*80}")
        print(f"3ï¸âƒ£  è®¡ç®—ç‰¹å¾é‡è¦æ€§")
        print(f"{'='*80}")
        print(f"åˆ†ææ ·æœ¬æ•°: {num_samples}")
        print(f"ç›®æ ‡: è¯†åˆ«å…³é”®åŸå§‹ç‰¹å¾\n")
        
        feature_names = self.dataset.feature_cols
        feature_gradients = {name: [] for name in feature_names}
        
        indices = np.random.choice(len(self.dataset), min(num_samples, len(self.dataset)), replace=False)
        
        for idx in indices:
            sequence, label = self.dataset[idx]
            sequence = sequence.unsqueeze(0).to(self.device)
            sequence.requires_grad = True
            
            # å‰å‘ä¼ æ’­
            output = self.model(sequence)
            pred_class = torch.argmax(output, dim=1)
            
            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            self.model.zero_grad()
            output[0, pred_class].backward()
            
            # æå–æ¯ä¸ªç‰¹å¾çš„å¹³å‡æ¢¯åº¦
            grads = sequence.grad[0].abs().mean(dim=0).cpu().numpy()  # (num_features,)
            
            for i, name in enumerate(feature_names):
                feature_gradients[name].append(grads[i])
        
        # è®¡ç®—å¹³å‡é‡è¦æ€§
        importance_scores = {}
        for name in feature_names:
            importance_scores[name] = np.mean(feature_gradients[name])
        
        # æ’åº
        sorted_features = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)
        
        print(f"âœ… ç‰¹å¾é‡è¦æ€§æ’å:\n")
        print(f"{'æ’å':<6} {'ç‰¹å¾å':<25} {'é‡è¦æ€§':<12} {'è¯´æ˜'}")
        print("-" * 80)
        
        feature_descriptions = {
            'close': 'æ”¶ç›˜ä»·',
            'open': 'å¼€ç›˜ä»·',
            'high': 'æœ€é«˜ä»·',
            'low': 'æœ€ä½ä»·',
            'volume': 'æˆäº¤é‡',
            'price_change': 'ä»·æ ¼å˜åŒ–(ç»å¯¹å€¼)',
            'price_change_pct': 'ä»·æ ¼å˜åŒ–ç‡(%)',
            'time_delta': 'æ—¶é—´é—´éš”',
            'price_range': 'ä»·æ ¼èŒƒå›´',
            'price_range_pct': 'ä»·æ ¼èŒƒå›´ç‡',
            'volume_change': 'æˆäº¤é‡å˜åŒ–',
            'volume_change_pct': 'æˆäº¤é‡å˜åŒ–ç‡'
        }
        
        for rank, (name, score) in enumerate(sorted_features, 1):
            desc = feature_descriptions.get(name, '')
            print(f"{rank:<6} {name:<25} {score:<12.6f} {desc}")
        
        self.feature_importances = importance_scores
        return importance_scores
    
    def discover_custom_indicators(self):
        """
        ä»æ¨¡å‹å­¦åˆ°çš„çŸ¥è¯†ä¸­æå–è‡ªå®šä¹‰æŒ‡æ ‡
        
        åŸºäºç‰¹å¾é‡è¦æ€§å’Œå¸‚åœºçŠ¶æ€èšç±»ï¼Œåˆ›å»ºæ–°çš„å¯è§£é‡ŠæŒ‡æ ‡
        """
        print(f"\n{'='*80}")
        print(f"4ï¸âƒ£  å‘ç°è‡ªå®šä¹‰æŒ‡æ ‡")
        print(f"{'='*80}")
        print(f"ç›®æ ‡: ä»æ¨¡å‹ä¸­æå–å¯è§£é‡Šçš„äº¤æ˜“æŒ‡æ ‡\n")
        
        # åŸºäºæ•°æ®ç»Ÿè®¡å’Œæ¨¡å‹æ´å¯Ÿï¼Œè®¾è®¡è‡ªå®šä¹‰æŒ‡æ ‡
        df = self.df.copy()
        
        custom_indicators = {}
        
        # æŒ‡æ ‡1: ä»·æ ¼åŠ¨é‡å¼ºåº¦ (Price Momentum Strength)
        # çµæ„Ÿï¼šæ¨¡å‹å…³æ³¨price_change_pctï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå¢å¼ºç‰ˆ
        df['PMS'] = df['price_change_pct'].rolling(10).mean() / df['price_change_pct'].rolling(10).std()
        custom_indicators['PMS'] = {
            'name': 'Price Momentum Strength',
            'formula': 'rolling_mean(price_change_pct, 10) / rolling_std(price_change_pct, 10)',
            'interpretation': 'ä»·æ ¼åŠ¨é‡çš„ä¿¡å™ªæ¯”ï¼Œ>1è¡¨ç¤ºå¼ºè¶‹åŠ¿ï¼Œ<-1è¡¨ç¤ºå¼ºåè½¬'
        }
        
        # æŒ‡æ ‡2: æˆäº¤é‡å¼‚å¸¸æŒ‡æ•° (Volume Anomaly Index)
        # çµæ„Ÿï¼šæ¨¡å‹å…³æ³¨volume_change
        df['VAI'] = (df['volume'] - df['volume'].rolling(20).mean()) / df['volume'].rolling(20).std()
        custom_indicators['VAI'] = {
            'name': 'Volume Anomaly Index',
            'formula': '(volume - rolling_mean(volume, 20)) / rolling_std(volume, 20)',
            'interpretation': 'æˆäº¤é‡å¼‚å¸¸ç¨‹åº¦ï¼Œ>2è¡¨ç¤ºçˆ†é‡ï¼Œ<-2è¡¨ç¤ºç¼©é‡'
        }
        
        # æŒ‡æ ‡3: ä»·æ ¼åŒºé—´å‹ç¼©æŒ‡æ ‡ (Price Range Compression)
        # çµæ„Ÿï¼šæ¨¡å‹å…³æ³¨price_range
        df['PRC'] = df['price_range'].rolling(5).mean() / df['price_range'].rolling(20).mean()
        custom_indicators['PRC'] = {
            'name': 'Price Range Compression',
            'formula': 'rolling_mean(price_range, 5) / rolling_mean(price_range, 20)',
            'interpretation': '<0.5è¡¨ç¤ºç›˜æ•´ï¼Œ>1.5è¡¨ç¤ºæ³¢åŠ¨åŠ å‰§'
        }
        
        # æŒ‡æ ‡4: æ—¶é—´åŠ æƒä»·æ ¼åŠ¨é‡ (Time-Weighted Price Momentum)
        # çµæ„Ÿï¼šæ¨¡å‹æ³¨æ„åŠ›æƒé‡æ˜¾ç¤ºè¿œæœŸå’Œè¿‘æœŸçš„é‡è¦æ€§
        df['TWPM'] = (
            df['price_change_pct'].rolling(3).mean() * 0.5 +
            df['price_change_pct'].rolling(10).mean() * 0.3 +
            df['price_change_pct'].rolling(30).mean() * 0.2
        )
        custom_indicators['TWPM'] = {
            'name': 'Time-Weighted Price Momentum',
            'formula': 'weighted_sum([short_momentum, mid_momentum, long_momentum], [0.5, 0.3, 0.2])',
            'interpretation': '>0.5%è¡¨ç¤ºå¤šå¤´ï¼Œ<-0.5%è¡¨ç¤ºç©ºå¤´'
        }
        
        # æŒ‡æ ‡5: æµåŠ¨æ€§å†²å‡»æŒ‡æ ‡ (Liquidity Shock Indicator)
        # ç»“åˆä»·æ ¼å˜åŒ–å’Œæˆäº¤é‡å˜åŒ–
        df['LSI'] = df['price_change_pct'].abs() * df['volume_change_pct'].abs()
        df['LSI'] = df['LSI'].rolling(5).mean()
        custom_indicators['LSI'] = {
            'name': 'Liquidity Shock Indicator',
            'formula': 'rolling_mean(abs(price_change_pct) * abs(volume_change_pct), 5)',
            'interpretation': '>é«˜å€¼è¡¨ç¤ºå¸‚åœºå—å†²å‡»ï¼Œå¯èƒ½æ˜¯è½¬æŠ˜ç‚¹'
        }
        
        print(f"âœ… å‘ç° {len(custom_indicators)} ä¸ªè‡ªå®šä¹‰æŒ‡æ ‡:\n")
        
        for i, (code, info) in enumerate(custom_indicators.items(), 1):
            print(f"{i}. {info['name']} ({code})")
            print(f"   å…¬å¼: {info['formula']}")
            print(f"   å«ä¹‰: {info['interpretation']}")
            print()
        
        # è®¡ç®—ä¸æœªæ¥æ”¶ç›Šçš„ç›¸å…³æ€§
        df['future_return'] = df['close'].shift(-10) / df['close'] - 1
        
        print(f"{'='*80}")
        print(f"ğŸ“Š æŒ‡æ ‡æœ‰æ•ˆæ€§éªŒè¯ï¼ˆä¸æœªæ¥10æ­¥æ”¶ç›Šçš„ç›¸å…³æ€§ï¼‰")
        print(f"{'='*80}\n")
        
        print(f"{'æŒ‡æ ‡':<10} {'Pearsonç›¸å…³':<15} {'Spearmanç›¸å…³':<15} {'æ˜¾è‘—æ€§'}")
        print("-" * 80)
        
        for code in custom_indicators.keys():
            valid_data = df[[code, 'future_return']].dropna()
            
            if len(valid_data) > 50:
                pearson_corr, pearson_p = pearsonr(valid_data[code], valid_data['future_return'])
                spearman_corr, spearman_p = spearmanr(valid_data[code], valid_data['future_return'])
                
                sig_level = '***' if pearson_p < 0.001 else '**' if pearson_p < 0.01 else '*' if pearson_p < 0.05 else ''
                
                print(f"{code:<10} {pearson_corr:>+.4f}         {spearman_corr:>+.4f}         {sig_level}")
        
        print("\nè¯´æ˜: ***p<0.001  **p<0.01  *p<0.05")
        
        # ä¸ä¼ ç»ŸæŒ‡æ ‡å¯¹æ¯”
        print(f"\n{'='*80}")
        print(f"ğŸ”„ ä¸ä¼ ç»ŸæŒ‡æ ‡å¯¹æ¯”")
        print(f"{'='*80}\n")
        
        # è®¡ç®—ä¼ ç»ŸRSI
        def compute_rsi(series, period=14):
            delta = series.diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
            rs = gain / loss
            rsi = 100 - (100 / (1 + rs))
            return rsi
        
        # è®¡ç®—ä¼ ç»ŸROC
        def compute_roc(series, period=10):
            return (series - series.shift(period)) / series.shift(period) * 100
        
        df['RSI'] = compute_rsi(df['close'])
        df['ROC'] = compute_roc(df['close'])
        
        print(f"{'æŒ‡æ ‡ç±»å‹':<20} {'æŒ‡æ ‡å':<10} {'ç›¸å…³æ€§':<12} {'è¯´æ˜'}")
        print("-" * 80)
        
        # ä¼ ç»ŸæŒ‡æ ‡
        for indicator, name in [('RSI', 'RSI(14)'), ('ROC', 'ROC(10)')]:
            valid_data = df[[indicator, 'future_return']].dropna()
            if len(valid_data) > 50:
                corr, _ = pearsonr(valid_data[indicator], valid_data['future_return'])
                print(f"{'ä¼ ç»ŸæŒ‡æ ‡':<20} {name:<10} {corr:>+.4f}       äººä¸ºè®¾è®¡")
        
        # è‡ªå®šä¹‰æŒ‡æ ‡
        for code, info in custom_indicators.items():
            valid_data = df[[code, 'future_return']].dropna()
            if len(valid_data) > 50:
                corr, _ = pearsonr(valid_data[code], valid_data['future_return'])
                print(f"{'è‡ªå®šä¹‰æŒ‡æ ‡':<20} {code:<10} {corr:>+.4f}       æ¨¡å‹å¯å‘")
        
        self.discovered_patterns = {
            'custom_indicators': custom_indicators,
            'dataframe_with_indicators': df
        }
        
        return custom_indicators, df
    
    def generate_report(self, output_dir='/home/cx/tigertrade/docs'):
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        print(f"\n{'='*80}")
        print(f"ğŸ“„ ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print(f"{'='*80}\n")
        
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'feature_importances': self.feature_importances,
            'custom_indicators': self.discovered_patterns.get('custom_indicators', {}),
            'summary': {
                'top_3_features': sorted(self.feature_importances.items(), key=lambda x: x[1], reverse=True)[:3],
                'num_custom_indicators': len(self.discovered_patterns.get('custom_indicators', {}))
            }
        }
        
        report_file = output_dir / 'feature_discovery_report.json'
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # ç”ŸæˆMarkdownæ€»ç»“
        md_file = output_dir / 'ä»æ¨¡å‹ä¸­å‘ç°çš„ç‰¹å¾.md'
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write("# ä»Transformeræ¨¡å‹ä¸­å‘ç°çš„æœ‰æ•ˆç‰¹å¾\n\n")
            f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            f.write("## 1. ç‰¹å¾é‡è¦æ€§æ’å\n\n")
            f.write("åŸºäºæ¢¯åº¦æ•æ„Ÿæ€§åˆ†æï¼Œæ¨¡å‹å¯¹ä»¥ä¸‹ç‰¹å¾æœ€æ•æ„Ÿï¼š\n\n")
            
            sorted_features = sorted(self.feature_importances.items(), key=lambda x: x[1], reverse=True)
            for rank, (name, score) in enumerate(sorted_features[:5], 1):
                f.write(f"{rank}. **{name}** (é‡è¦æ€§: {score:.6f})\n")
            
            f.write("\n## 2. å‘ç°çš„è‡ªå®šä¹‰æŒ‡æ ‡\n\n")
            
            for code, info in self.discovered_patterns.get('custom_indicators', {}).items():
                f.write(f"### {info['name']} ({code})\n\n")
                f.write(f"**å…¬å¼**: `{info['formula']}`\n\n")
                f.write(f"**å«ä¹‰**: {info['interpretation']}\n\n")
            
            f.write("\n## 3. æ ¸å¿ƒæ´å¯Ÿ\n\n")
            f.write("é€šè¿‡åˆ†æTransformeræ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºï¼Œæˆ‘ä»¬å‘ç°ï¼š\n\n")
            f.write("1. **æ¨¡å‹æœ€å…³æ³¨çš„åŸå§‹ç‰¹å¾** - æ¯”äººä¸ºè®¾è®¡çš„æŒ‡æ ‡æ›´æ¥è¿‘å¸‚åœºæœ¬è´¨\n")
            f.write("2. **è‡ªå®šä¹‰æŒ‡æ ‡** - ç»“åˆæ¨¡å‹æ´å¯Ÿå’Œå¯è§£é‡Šæ€§\n")
            f.write("3. **å¸‚åœºçŠ¶æ€è¯†åˆ«** - æ¨¡å‹éšå¼å­¦ä¹ äº†ä¸åŒçš„å¸‚åœºregime\n\n")
        
        print(f"âœ… Markdownæ€»ç»“å·²ä¿å­˜: {md_file}")
        print(f"\n{'='*80}")
        print(f"âœ… ç‰¹å¾å‘ç°åˆ†æå®Œæˆï¼")
        print(f"{'='*80}\n")


def main():
    """ä¸»å‡½æ•°"""
    model_path = '/home/cx/tigertrade/models/transformer_raw_features_best.pth'
    data_file = '/home/cx/trading_data/val_raw_features.csv'
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(model_path).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        print(f"è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼")
        return
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = FeatureDiscovery(model_path, data_file)
    
    # 1. æå–æ³¨æ„åŠ›æƒé‡
    analyzer.extract_attention_weights(num_samples=1000)
    
    # 2. æå–éšè—å±‚è¡¨ç¤º
    analyzer.extract_hidden_representations(num_samples=1000)
    
    # 3. è®¡ç®—ç‰¹å¾é‡è¦æ€§
    analyzer.compute_feature_importance(num_samples=500)
    
    # 4. å‘ç°è‡ªå®šä¹‰æŒ‡æ ‡
    analyzer.discover_custom_indicators()
    
    # 5. ç”ŸæˆæŠ¥å‘Š
    analyzer.generate_report()


if __name__ == '__main__':
    main()
