import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
from datetime import datetime
import threading
import time
import glob
import argparse
from typing import Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class TradingLSTM(nn.Module):
    """ç”¨äºäº¤æ˜“å†³ç­–çš„LSTMæ¨¡å‹"""
    def __init__(self, input_size=12, hidden_size=64, num_layers=2, output_size=3):
        super(TradingLSTM, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(hidden_size, output_size)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.dropout(out[:, -1, :])
        out = self.fc(out)
        out = self.softmax(out)
        return out


class TradingTransformer(nn.Module):
    """ç”¨äºäº¤æ˜“å†³ç­–çš„Transformeræ¨¡å‹"""
    def __init__(self, input_size=12, nhead=2, num_layers=2, output_size=3, d_model=64):
        super(TradingTransformer, self).__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        self.pos_encoding = nn.Parameter(torch.zeros(1, 1, d_model))
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.dropout = nn.Dropout(0.2)
        self.fc = nn.Linear(d_model, output_size)
        self.softmax = nn.Softmax(dim=1)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        x = self.input_projection(x)  # (batch_size, seq_len, d_model)
        x = x + self.pos_encoding  # Add positional encoding
        
        out = self.transformer(x)  # (batch_size, seq_len, d_model)
        out = self.dropout(out[:, -1, :])  # Take the last sequence element
        out = self.fc(out)  # (batch_size, output_size)
        out = self.softmax(out)
        return out


class ModelComparisonStrategy:
    """æ¨¡å‹æ¯”è¾ƒç­–ç•¥ - åŒæ—¶ä½¿ç”¨LSTMå’ŒTransformeræ¨¡å‹"""
    def __init__(self, data_dir='/home/cx/trading_data', model_path=None):
        # å¼ºåˆ¶ä½¿ç”¨GPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ­¤ç­–ç•¥éœ€è¦GPUè¿è¡Œ")
        
        self.data_dir = data_dir
        
        # åˆå§‹åŒ–ä¸¤ä¸ªæ¨¡å‹
        self.lstm_model = TradingLSTM(input_size=12).to(self.device)
        self.transformer_model = TradingTransformer(input_size=12).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.lstm_optimizer = torch.optim.Adam(self.lstm_model.parameters(), lr=0.001)
        self.transformer_optimizer = torch.optim.Adam(self.transformer_model.parameters(), lr=0.001)
        self.criterion = nn.CrossEntropyLoss()
        
        # æ§åˆ¶è®­ç»ƒå’Œæ¨ç†çš„æ ‡å¿—
        self.should_train = True
        self.model_lock = threading.Lock()
        
        # è®°å½•æ¨¡å‹æ€§èƒ½
        self.performance_log = {
            'lstm_correct': 0,
            'lstm_total': 0,
            'transformer_correct': 0,
            'transformer_total': 0
        }
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if model_path and os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                if 'lstm_state_dict' in checkpoint:
                    self.lstm_model.load_state_dict(checkpoint['lstm_state_dict'])
                if 'transformer_state_dict' in checkpoint:
                    self.transformer_model.load_state_dict(checkpoint['transformer_state_dict'])
                print(f"âœ… ä» {model_path} åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åˆå§‹æ¨¡å‹")
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        self.training_thread = threading.Thread(target=self.train_continuously, daemon=True)
        self.training_thread.start()
    
    def prepare_features(self, row):
        """ä»æ•°æ®è¡Œä¸­å‡†å¤‡ç‰¹å¾å‘é‡"""
        try:
            features = [
                row['price_current'],
                row['atr'],
                row['rsi_1m'] if pd.notna(row['rsi_1m']) else 50,
                row['rsi_5m'] if pd.notna(row['rsi_5m']) else 50,
                row.get('boll_upper', 0),
                row.get('boll_mid', 0),
                row.get('boll_lower', 0),
                row.get('boll_position', 0.5),
                row.get('price_change_1', 0),
                row.get('price_change_5', 0),
                row.get('volatility', 0),
                row.get('volume_1m', 0)
            ]
            # å½’ä¸€åŒ–ç‰¹å¾
            features_np = np.array(features)
            mean_val = np.mean(features_np)
            std_val = np.std(features_np) + 1e-8
            normalized_features = (features_np - mean_val) / std_val
            return normalized_features.tolist()
        except Exception as e:
            print(f"prepare_featuresé”™è¯¯: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾å€¼
            return [0.0] * 12
    
    def predict_both_models(self, current_data):
        """ä½¿ç”¨ä¸¤ä¸ªæ¨¡å‹é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        with self.model_lock:
            try:
                # å‡†å¤‡è¾“å…¥æ•°æ®
                features = self.prepare_features(current_data)
                input_tensor = torch.tensor([features], dtype=torch.float32).unsqueeze(1).to(self.device)
                
                # LSTMé¢„æµ‹
                with torch.no_grad():
                    self.lstm_model.eval()
                    lstm_prediction = self.lstm_model(input_tensor)
                    lstm_probabilities = lstm_prediction.cpu().numpy()
                    lstm_action = np.argmax(lstm_probabilities[0])
                    lstm_confidence = lstm_probabilities[0][lstm_action]
                
                # Transformeré¢„æµ‹ (éœ€è¦åºåˆ—é•¿åº¦ä¸º1)
                transformer_input = input_tensor  # Already shaped as (1, 1, 10)
                with torch.no_grad():
                    self.transformer_model.eval()
                    transformer_prediction = self.transformer_model(transformer_input)
                    transformer_probabilities = transformer_prediction.cpu().numpy()
                    transformer_action = np.argmax(transformer_probabilities[0])
                    transformer_confidence = transformer_probabilities[0][transformer_action]
                
                return {
                    'lstm': {'action': int(lstm_action), 'confidence': float(lstm_confidence)},
                    'transformer': {'action': int(transformer_action), 'confidence': float(transformer_confidence)}
                }
            except Exception as e:
                print(f"é¢„æµ‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return {
                    'lstm': {'action': 0, 'confidence': 0.0},
                    'transformer': {'action': 0, 'confidence': 0.0}
                }
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        all_data_files = []
        
        # è·å–æ‰€æœ‰æ•°æ®ç›®å½•
        all_data_dirs = glob.glob(os.path.join(self.data_dir, '202*-*-*'))
        if all_data_dirs:
            # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶
            sorted_dirs = sorted(all_data_dirs, reverse=True)
            for data_dir in sorted_dirs[:7]:  # ä½¿ç”¨æœ€è¿‘7å¤©çš„æ•°æ®
                # åŒ…å«åŸå§‹æ•°æ®å’Œæ‰©å±•æ•°æ®
                data_files = glob.glob(os.path.join(data_dir, 'trading_data_*.csv'))
                data_files.extend(glob.glob(os.path.join(data_dir, 'extended_trading_data_*.csv')))
                data_files.extend(glob.glob(os.path.join(data_dir, 'prepared_features_*.csv')))  # ä¹ŸåŒ…å«å‡†å¤‡å¥½çš„ç‰¹å¾æ•°æ®
                all_data_files.extend(data_files)
        
        if not all_data_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        data_file = sorted(all_data_files, key=os.path.getmtime, reverse=True)[0]
        
        try:
            df = pd.read_csv(data_file)
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=['price_current', 'grid_lower', 'grid_upper', 'atr', 'rsi_1m', 'rsi_5m'])
            
            if len(df) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return None
                
            return df
        except Exception as e:
            print(f"åŠ è½½è®­ç»ƒæ•°æ®é”™è¯¯: {e}")
            return None
    
    def train_lstm(self, df):
        """è®­ç»ƒLSTMæ¨¡å‹"""
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸºäºç›ˆåˆ©çš„æ ‡ç­¾
            X, y = [], []
            look_ahead = 10  # å‘å‰çœ‹10ä¸ªæ—¶é—´æ­¥é•¿æ¥è®¡ç®—ç›ˆåˆ©
            
            for i in range(len(df) - look_ahead):
                row = df.iloc[i]
                features = self.prepare_features(row)
                X.append(features)
                
                # è®¡ç®—æœªæ¥look_aheadæ­¥çš„ç›ˆåˆ©
                current_price = row['price_current']
                future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
                
                if len(future_prices) == 0:
                    continue
                    
                # è®¡ç®—æœ€å¤§ç›ˆåˆ©å’Œæœ€å¤§äºæŸ
                max_future_price = max(future_prices)
                min_future_price = min(future_prices)
                
                buy_profit = (max_future_price - current_price) / current_price
                sell_profit = (current_price - min_future_price) / current_price
                
                # åˆ›å»ºæ ‡ç­¾: 0=ä¸æ“ä½œ, 1=ä¹°å…¥, 2=å–å‡º
                # åªæœ‰å½“é¢„æœŸç›ˆåˆ©è¶…è¿‡é˜ˆå€¼æ—¶æ‰å»ºè®®æ“ä½œ
                profit_threshold = 0.002  # 0.2%çš„é˜ˆå€¼
                
                if buy_profit > profit_threshold and buy_profit > sell_profit:
                    label = 1  # ä¹°å…¥
                elif sell_profit > profit_threshold and sell_profit > buy_profit:
                    label = 2  # å–å‡º
                else:
                    label = 0  # ä¸æ“ä½œ
                
                y.append(label)
            
            if len(X) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
            split_idx = int(len(X) * 0.8)
            
            X_train = X[:split_idx]
            y_train = y[:split_idx]
            X_val = X[split_idx:]
            y_val = y[split_idx:]
            
            X_train = np.array(X_train)
            y_train = np.array(y_train)
            X_val = np.array(X_val)
            y_val = np.array(y_val)
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(self.device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)
            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1).to(self.device)
            y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
            
            # è®­ç»ƒLSTMæ¨¡å‹
            self.lstm_model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            best_val_acc = 0.0
            
            for epoch in range(5):  # å¢åŠ åˆ°5è½®è®­ç»ƒ
                # è®­ç»ƒé˜¶æ®µ
                total_loss = 0
                num_batches = 0
                correct_predictions = 0
                total_predictions = 0
                
                for batch_x, batch_y in train_loader:
                    self.lstm_optimizer.zero_grad()
                    outputs = self.lstm_model(batch_x)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    self.lstm_optimizer.step()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    predictions = torch.argmax(outputs, dim=1)
                    correct_predictions += (predictions == batch_y).sum().item()
                    total_predictions += batch_y.size(0)
                    
                    total_loss += loss.item()
                    num_batches += 1
                
                train_avg_loss = total_loss / num_batches
                train_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
                
                # éªŒè¯é˜¶æ®µ
                self.lstm_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                val_correct = 0
                val_total = 0
                val_loss = 0
                with torch.no_grad():
                    for batch_x, batch_y in val_loader:
                        outputs = self.lstm_model(batch_x)
                        loss = self.criterion(outputs, batch_y)
                        val_loss += loss.item()
                        
                        predictions = torch.argmax(outputs, dim=1)
                        val_correct += (predictions == batch_y).sum().item()
                        val_total += batch_y.size(0)
                
                val_avg_loss = val_loss / len(val_loader)
                val_accuracy = val_correct / val_total if val_total > 0 else 0
                
                print(f"LSTMè®­ç»ƒè½®æ¬¡ {epoch+1}/{5}")
                print(f"  è®­ç»ƒ - æŸå¤±: {train_avg_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.3f}")
                print(f"  éªŒè¯ - æŸå¤±: {val_avg_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.3f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_accuracy > best_val_acc:
                    best_val_acc = val_accuracy
                    print(f"  ğŸ† LSTMæ–°æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3f}")
                
                self.lstm_model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ
        
        except Exception as e:
            print(f"LSTMè®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    def train_transformer(self, df):
        """è®­ç»ƒTransformeræ¨¡å‹"""
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸºäºç›ˆåˆ©çš„æ ‡ç­¾
            X, y = [], []
            look_ahead = 10  # å‘å‰çœ‹10ä¸ªæ—¶é—´æ­¥é•¿æ¥è®¡ç®—ç›ˆåˆ©
            
            for i in range(len(df) - look_ahead):
                row = df.iloc[i]
                features = self.prepare_features(row)
                X.append(features)
                
                # è®¡ç®—æœªæ¥look_aheadæ­¥çš„ç›ˆåˆ©
                current_price = row['price_current']
                future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
                
                if len(future_prices) == 0:
                    continue
                    
                # è®¡ç®—æœ€å¤§ç›ˆåˆ©å’Œæœ€å¤§äºæŸ
                max_future_price = max(future_prices)
                min_future_price = min(future_prices)
                
                buy_profit = (max_future_price - current_price) / current_price
                sell_profit = (current_price - min_future_price) / current_price
                
                # åˆ›å»ºæ ‡ç­¾: 0=ä¸æ“ä½œ, 1=ä¹°å…¥, 2=å–å‡º
                # åªæœ‰å½“é¢„æœŸç›ˆåˆ©è¶…è¿‡é˜ˆå€¼æ—¶æ‰å»ºè®®æ“ä½œ
                profit_threshold = 0.002  # 0.2%çš„é˜ˆå€¼
                
                if buy_profit > profit_threshold and buy_profit > sell_profit:
                    label = 1  # ä¹°å…¥
                elif sell_profit > profit_threshold and sell_profit > buy_profit:
                    label = 2  # å–å‡º
                else:
                    label = 0  # ä¸æ“ä½œ
                
                y.append(label)
            
            if len(X) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
            split_idx = int(len(X) * 0.8)
            
            X_train = X[:split_idx]
            y_train = y[:split_idx]
            X_val = X[split_idx:]
            y_val = y[split_idx:]
            
            X_train = np.array(X_train)
            y_train = np.array(y_train)
            X_val = np.array(X_val)
            y_val = np.array(y_val)
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(self.device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)
            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1).to(self.device)
            y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
            
            # è®­ç»ƒTransformeræ¨¡å‹
            self.transformer_model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            best_val_acc = 0.0
            
            for epoch in range(5):  # å¢åŠ åˆ°5è½®è®­ç»ƒ
                # è®­ç»ƒé˜¶æ®µ
                total_loss = 0
                num_batches = 0
                correct_predictions = 0
                total_predictions = 0
                
                for batch_x, batch_y in train_loader:
                    self.transformer_optimizer.zero_grad()
                    outputs = self.transformer_model(batch_x)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    self.transformer_optimizer.step()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    predictions = torch.argmax(outputs, dim=1)
                    correct_predictions += (predictions == batch_y).sum().item()
                    total_predictions += batch_y.size(0)
                    
                    total_loss += loss.item()
                    num_batches += 1
                
                train_avg_loss = total_loss / num_batches
                train_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
                
                # éªŒè¯é˜¶æ®µ
                self.transformer_model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                val_correct = 0
                val_total = 0
                val_loss = 0
                with torch.no_grad():
                    for batch_x, batch_y in val_loader:
                        outputs = self.transformer_model(batch_x)
                        loss = self.criterion(outputs, batch_y)
                        val_loss += loss.item()
                        
                        predictions = torch.argmax(outputs, dim=1)
                        val_correct += (predictions == batch_y).sum().item()
                        val_total += batch_y.size(0)
                
                val_avg_loss = val_loss / len(val_loader)
                val_accuracy = val_correct / val_total if val_total > 0 else 0
                
                print(f"Transformerè®­ç»ƒè½®æ¬¡ {epoch+1}/{5}")
                print(f"  è®­ç»ƒ - æŸå¤±: {train_avg_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.3f}")
                print(f"  éªŒè¯ - æŸå¤±: {val_avg_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.3f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_accuracy > best_val_acc:
                    best_val_acc = val_accuracy
                    print(f"  ğŸ† Transformeræ–°æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3f}")
                
                self.transformer_model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ
        
        except Exception as e:
            print(f"Transformerè®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    def train_continuously(self):
        """è¿ç»­è®­ç»ƒä¸¤ä¸ªæ¨¡å‹çš„åå°çº¿ç¨‹"""
        while self.should_train:
            try:
                # åŠ è½½æ•°æ®
                df = self.load_training_data()
                if df is not None and len(df) > 0:
                    print(f"å¼€å§‹è®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼Œæ•°æ®é‡: {len(df)}")
                    with self.model_lock:
                        self.train_lstm(df)
                        self.train_transformer(df)
                    print("ä¸¤ä¸ªæ¨¡å‹è®­ç»ƒå®Œæˆ")
                
                # è®­ç»ƒè¾ƒæ…¢ï¼Œæ¯30åˆ†é’Ÿè®­ç»ƒä¸€æ¬¡
                time.sleep(1800)
                
            except Exception as e:
                print(f"è®­ç»ƒçº¿ç¨‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿåç»§ç»­
    
    def save_model(self, path):
        """ä¿å­˜ä¸¤ä¸ªæ¨¡å‹"""
        with self.model_lock:
            torch.save({
                'lstm_state_dict': self.lstm_model.state_dict(),
                'lstm_optimizer_state_dict': self.lstm_optimizer.state_dict(),
                'transformer_state_dict': self.transformer_model.state_dict(),
                'transformer_optimizer_state_dict': self.transformer_optimizer.state_dict(),
            }, path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def log_performance(self, actual_action, lstm_pred, transformer_pred):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        with self.model_lock:
            if actual_action == lstm_pred['action']:
                self.performance_log['lstm_correct'] += 1
            self.performance_log['lstm_total'] += 1
            
            if actual_action == transformer_pred['action']:
                self.performance_log['transformer_correct'] += 1
            self.performance_log['transformer_total'] += 1
            
            # æ‰“å°æ€§èƒ½æ‘˜è¦
            if self.performance_log['lstm_total'] > 0:
                lstm_acc = self.performance_log['lstm_correct'] / self.performance_log['lstm_total']
                trans_acc = self.performance_log['transformer_correct'] / self.performance_log['transformer_total']
                print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½ - LSTMå‡†ç¡®ç‡: {lstm_acc:.3f}, Transformerå‡†ç¡®ç‡: {trans_acc:.3f}")


def main():
    parser = argparse.ArgumentParser(description='Model Comparison Trading Strategy')
    parser.add_argument('--mode', choices=['train', 'predict', 'compare'], default='compare',
                        help='è¿è¡Œæ¨¡å¼: train(ä»…è®­ç»ƒ), predict(ä»…é¢„æµ‹), compare(æ¯”è¾ƒ)')
    parser.add_argument('--model_path', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜æˆ–åŠ è½½è·¯å¾„')
    args = parser.parse_args()
    
    strategy = ModelComparisonStrategy(model_path=args.model_path)
    
    if args.mode == 'train':
        print("ä»…è¿è¡Œè®­ç»ƒæ¨¡å¼...")
        # è®­ç»ƒæ¨¡å¼ï¼Œä¸é€€å‡º
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("è®­ç»ƒå·²åœæ­¢")
    elif args.mode == 'predict' or args.mode == 'compare':
        print("è¿è¡Œæ¯”è¾ƒæ¨¡å¼...")
        # æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
        sample_data = {
            'price_current': 93.5,
            'grid_lower': 93.0,
            'grid_upper': 94.0,
            'atr': 0.2,
            'rsi_1m': 30.0,
            'rsi_5m': 40.0,
            'buffer': 0.05,
            'threshold': 93.05,
            'near_lower': True,
            'rsi_ok': True
        }
        
        predictions = strategy.predict_both_models(sample_data)
        action_map = {0: "ä¸æ“ä½œ", 1: "ä¹°å…¥", 2: "å–å‡º"}
        
        print(f"ğŸ§  LSTMé¢„æµ‹: {action_map[predictions['lstm']['action']]}, ç½®ä¿¡åº¦: {predictions['lstm']['confidence']:.3f}")
        print(f"ğŸ§  Transformeré¢„æµ‹: {action_map[predictions['transformer']['action']]}, ç½®ä¿¡åº¦: {predictions['transformer']['confidence']:.3f}")
        
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("ç¨‹åºå·²åœæ­¢")
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜æ¨¡å‹
    if args.model_path:
        strategy.save_model(args.model_path)


if __name__ == "__main__":
    main()