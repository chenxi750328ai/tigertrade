import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
from datetime import datetime
import threading
import time
import glob
import argparse
from typing import Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class EnhancedTradingTransformer(nn.Module):
    """ç”¨äºäº¤æ˜“å†³ç­–çš„å¢å¼ºå‹Transformeræ¨¡å‹"""
    def __init__(self, input_size=12, nhead=8, num_layers=8, output_size=3, d_model=512):
        super(EnhancedTradingTransformer, self).__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        # ä½ç½®ç¼–ç  - ä½¿ç”¨æ­£å¼¦ä½™å¼¦ç¼–ç 
        self.register_buffer('pos_encoding', self._create_positional_encoding(1000, d_model))
        
        # å¤šå±‚Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            dim_feedforward=d_model*4,  # FFNéšè—å±‚æ˜¯d_modelçš„4å€
            dropout=0.1,
            batch_first=True,
            activation='gelu'  # ä½¿ç”¨GELUæ¿€æ´»å‡½æ•°
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # æ³¨æ„åŠ›æ± åŒ–å±‚ - ç”¨äºæ•´åˆåºåˆ—ä¿¡æ¯
        self.attention_pool = nn.MultiheadAttention(embed_dim=d_model, num_heads=nhead, batch_first=True)
        
        # å¤šå±‚åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.GELU(),
            nn.LayerNorm(d_model//2),
            nn.Dropout(0.3),
            nn.Linear(d_model//2, d_model//4),
            nn.GELU(),
            nn.LayerNorm(d_model//4),
            nn.Dropout(0.2),
            nn.Linear(d_model//4, output_size)
        )
        self.softmax = nn.Softmax(dim=1)
    
    def _create_positional_encoding(self, max_len, d_model):
        """åˆ›å»ºæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            -(np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)  # (1, max_len, d_model)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        batch_size, seq_len = x.size(0), x.size(1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # (batch_size, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if seq_len <= self.pos_encoding.size(1):
            pos_enc = self.pos_encoding[:, :seq_len, :]
        else:
            # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œæ‰©å±•ä½ç½®ç¼–ç 
            extended_pe = self._create_positional_encoding(seq_len, self.d_model).to(x.device)
            pos_enc = extended_pe[:, :seq_len, :]
        
        x = x + pos_enc
        
        # Transformerç¼–ç 
        out = self.transformer(x)  # (batch_size, seq_len, d_model)
        
        # æ³¨æ„åŠ›æ± åŒ– - ä½¿ç”¨æœ€åä¸€ä¸ªtokenä½œä¸ºæŸ¥è¯¢
        attn_out, _ = self.attention_pool(
            query=out[:, -1:, :],  # (batch, 1, d_model)
            key=out,               # (batch, seq_len, d_model)
            value=out              # (batch, seq_len, d_model)
        )
        
        # å±•å¹³æ³¨æ„åŠ›è¾“å‡º
        pooled_out = attn_out.squeeze(1)  # (batch, d_model)
        
        # åˆ†ç±»
        out = self.classifier(pooled_out)  # (batch, output_size)
        out = self.softmax(out)
        return out


class EnhancedTransformerStrategy:
    """å¢å¼ºå‹Transformerç­–ç•¥"""
    def __init__(self, data_dir='/home/cx/trading_data', model_path=None):
        # å¼ºåˆ¶ä½¿ç”¨GPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ­¤ç­–ç•¥éœ€è¦GPUè¿è¡Œ")
        
        self.data_dir = data_dir
        
        # åˆå§‹åŒ–å¢å¼ºå‹æ¨¡å‹
        self.model = EnhancedTradingTransformer(input_size=12).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0003, weight_decay=0.01)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, patience=5, factor=0.5)
        # åˆå§‹æŸå¤±å‡½æ•°ä¸å¸¦æƒé‡ï¼Œåç»­ä¼šæ ¹æ®æ•°æ®åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
        self.criterion = nn.CrossEntropyLoss()
        
        # æ§åˆ¶è®­ç»ƒå’Œæ¨ç†çš„æ ‡å¿—
        self.should_train = True
        self.model_lock = threading.Lock()
        
        # è®°å½•æ¨¡å‹æ€§èƒ½
        self.performance_log = {
            'correct': 0,
            'total': 0
        }
        
        # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"å¢å¼ºå‹Transformeræ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if model_path and os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                if 'scheduler_state_dict' in checkpoint:
                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
                print(f"âœ… ä» {model_path} åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åˆå§‹æ¨¡å‹")
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        self.training_thread = threading.Thread(target=self.train_continuously, daemon=True)
        self.training_thread.start()
    
    def prepare_features(self, row):
        """ä»æ•°æ®è¡Œä¸­å‡†å¤‡ç‰¹å¾å‘é‡"""
        try:
            features = [
                row['price_current'],
                row['atr'],
                row['rsi_1m'] if pd.notna(row['rsi_1m']) else 50,
                row['rsi_5m'] if pd.notna(row['rsi_5m']) else 50,
                row.get('boll_upper', 0),
                row.get('boll_mid', 0),
                row.get('boll_lower', 0),
                row.get('boll_position', 0.5),
                row.get('price_change_1', 0),
                row.get('price_change_5', 0),
                row.get('volatility', 0),
                row.get('volume_1m', 0)
            ]
            # å½’ä¸€åŒ–ç‰¹å¾
            features_np = np.array(features)
            mean_val = np.mean(features_np)
            std_val = np.std(features_np) + 1e-8
            normalized_features = (features_np - mean_val) / std_val
            return normalized_features.tolist()
        except Exception as e:
            print(f"prepare_featuresé”™è¯¯: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾å€¼
            return [0.0] * 12
    
    
    
    def predict_action(self, current_data):
        """ä½¿ç”¨æ¨¡å‹é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        with self.model_lock:
            try:
                # å‡†å¤‡è¾“å…¥æ•°æ®
                features = self.prepare_features(current_data)
                input_tensor = torch.tensor([features], dtype=torch.float32).unsqueeze(1).to(self.device)  # (1, 1, 10)
                
                # æ¨¡å‹é¢„æµ‹
                with torch.no_grad():
                    self.model.eval()
                    prediction = self.model(input_tensor)
                    probabilities = prediction.cpu().numpy()
                    action = np.argmax(probabilities[0])
                    confidence = probabilities[0][action]
                
                return int(action), float(confidence)
            except Exception as e:
                print(f"é¢„æµ‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return 0, 0.0
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        all_data_files = []
        
        # è·å–æ‰€æœ‰æ•°æ®ç›®å½•
        all_data_dirs = glob.glob(os.path.join(self.data_dir, '202*-*-*'))
        if all_data_dirs:
            # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶
            sorted_dirs = sorted(all_data_dirs, reverse=True)
            for data_dir in sorted_dirs[:7]:  # ä½¿ç”¨æœ€è¿‘7å¤©çš„æ•°æ®
                # åŒ…å«åŸå§‹æ•°æ®å’Œæ‰©å±•æ•°æ®
                data_files = glob.glob(os.path.join(data_dir, 'trading_data_*.csv'))
                data_files.extend(glob.glob(os.path.join(data_dir, 'extended_trading_data_*.csv')))
                data_files.extend(glob.glob(os.path.join(data_dir, 'prepared_features_*.csv')))  # ä¹ŸåŒ…å«å‡†å¤‡å¥½çš„ç‰¹å¾æ•°æ®
                all_data_files.extend(data_files)
        
        if not all_data_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        data_file = sorted(all_data_files, key=os.path.getmtime, reverse=True)[0]
        
        try:
            df = pd.read_csv(data_file)
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=['price_current', 'grid_lower', 'grid_upper', 'atr', 'rsi_1m', 'rsi_5m'])
            
            if len(df) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return None
                
            return df
        except Exception as e:
            print(f"åŠ è½½è®­ç»ƒæ•°æ®é”™è¯¯: {e}")
            return None
    
    def calculate_class_weights(self, y):
        """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®"""
        import numpy as np
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        classes, counts = np.unique(y, return_counts=True)
        total_samples = len(y)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ (æ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°))
        weights = []
        for count in counts:
            weight = total_samples / (len(classes) * count)
            weights.append(weight)
        
        # è½¬æ¢ä¸ºtensor
        class_weights = torch.FloatTensor(weights).to(self.device)
        return class_weights

    def train_model(self, df):
        """è®­ç»ƒæ¨¡å‹"""
        try:
            # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ä»¥é¿å…çŠ¶æ€é—®é¢˜
            if hasattr(self, 'optimizer') and self.optimizer is not None:
                # æ¸…ç©ºä¼˜åŒ–å™¨çŠ¶æ€
                self.optimizer.state.clear()
            else:
                self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-5)
                self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    self.optimizer, mode='min', factor=0.5, patience=3
                )
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸºäºç›ˆåˆ©çš„æ ‡ç­¾
            X, y = [], []
            look_ahead = 10  # å‘å‰çœ‹10ä¸ªæ—¶é—´æ­¥é•¿æ¥è®¡ç®—ç›ˆåˆ©
            
            for i in range(len(df) - look_ahead):
                row = df.iloc[i]
                features = self.prepare_features(row)
                X.append(features)
                
                # è®¡ç®—æœªæ¥look_aheadæ­¥çš„ç›ˆåˆ©
                current_price = row['price_current']
                future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
                
                if len(future_prices) == 0:
                    continue
                    
                # è®¡ç®—æœ€å¤§ç›ˆåˆ©å’Œæœ€å¤§äºæŸ
                max_future_price = max(future_prices)
                min_future_price = min(future_prices)
                
                buy_profit = (max_future_price - current_price) / current_price
                sell_profit = (current_price - min_future_price) / current_price
                
                # åˆ›å»ºæ ‡ç­¾: 0=ä¸æ“ä½œ, 1=ä¹°å…¥, 2=å–å‡º
                # åªæœ‰å½“é¢„æœŸç›ˆåˆ©è¶…è¿‡é˜ˆå€¼æ—¶æ‰å»ºè®®æ“ä½œ
                profit_threshold = 0.005  # æé«˜é˜ˆå€¼åˆ°0.5%ï¼Œå‡å°‘äº¤æ˜“é¢‘ç‡ä½†æé«˜è´¨é‡
                min_diff = 0.003  # æœ€å°å·®å€¼ï¼Œç¡®ä¿ä¹°å–ä¹‹é—´æœ‰è¶³å¤Ÿå·®è·
                
                # åªæœ‰å½“ä¹°å–ç›ˆåˆ©å·®å€¼è¶…è¿‡æœ€å°å·®å€¼ä¸”è¶…è¿‡é˜ˆå€¼æ—¶æ‰äº¤æ˜“
                if abs(buy_profit - sell_profit) >= min_diff:
                    if buy_profit > sell_profit and buy_profit > profit_threshold:
                        label = 1  # ä¹°å…¥
                    elif sell_profit > buy_profit and sell_profit > profit_threshold:
                        label = 2  # å–å‡º
                    else:
                        label = 0  # ä¸æ“ä½œ
                else:
                    label = 0  # ä¸æ“ä½œ - å·®å€¼å¤ªå°ï¼Œä¸ç¡®å®šæ€§é«˜
                    
                y.append(label)
            
            if len(X) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return
            
            # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
            unique, counts = np.unique(y, return_counts=True)
            label_distribution = dict(zip(unique, counts))
            print(f"æ ‡ç­¾åˆ†å¸ƒ: {label_distribution}")
            
            # å¦‚æœæŸä¸ªç±»åˆ«å æ¯”è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            total_samples = len(y)
            for label, count in label_distribution.items():
                percentage = count / total_samples * 100
                print(f"æ ‡ç­¾ {label} å æ¯”: {percentage:.2f}%")
            
            # è®¡ç®—ç±»åˆ«æƒé‡
            class_weights = self.calculate_class_weights(y)
            print(f"ç±»åˆ«æƒé‡: {class_weights}")
            
            # æ›´æ–°æŸå¤±å‡½æ•°
            self.criterion = nn.CrossEntropyLoss(weight=class_weights)
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
            split_idx = int(len(X) * 0.8)
            
            X_train = X[:split_idx]
            y_train = y[:split_idx]
            X_val = X[split_idx:]
            y_val = y[split_idx:]
            
            X_train = np.array(X_train)
            y_train = np.array(y_train)
            X_val = np.array(X_val)
            y_val = np.array(y_val)
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1).to(self.device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)
            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1).to(self.device)
            y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
            
            # è®­ç»ƒæ¨¡å‹
            self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            best_val_loss = float('inf')
            
            for epoch in range(20):  # å¢åŠ åˆ°20è½®è®­ç»ƒ
                # è®­ç»ƒé˜¶æ®µ
                total_loss = 0
                num_batches = 0
                correct_predictions = 0
                total_predictions = 0
                
                for batch_x, batch_y in train_loader:
                    self.optimizer.zero_grad()
                    outputs = self.model(batch_x)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()
                    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    predictions = torch.argmax(outputs, dim=1)
                    correct_predictions += (predictions == batch_y).sum().item()
                    total_predictions += batch_y.size(0)
                    
                    total_loss += loss.item()
                    num_batches += 1
                
                train_avg_loss = total_loss / num_batches
                train_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
                
                # éªŒè¯é˜¶æ®µ
                self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                val_correct = 0
                val_total = 0
                val_loss = 0
                with torch.no_grad():
                    for batch_x, batch_y in val_loader:
                        outputs = self.model(batch_x)
                        loss = self.criterion(outputs, batch_y)
                        val_loss += loss.item()
                        
                        predictions = torch.argmax(outputs, dim=1)
                        val_correct += (predictions == batch_y).sum().item()
                        val_total += batch_y.size(0)
                
                val_avg_loss = val_loss / len(val_loader)
                val_accuracy = val_correct / val_total if val_total > 0 else 0
                
                print(f"å¢å¼ºå‹Transformerè®­ç»ƒè½®æ¬¡ {epoch+1}/20")
                print(f"  è®­ç»ƒ - æŸå¤±: {train_avg_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.3f}")
                print(f"  éªŒè¯ - æŸå¤±: {val_avg_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.3f}")
                
                # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                self.scheduler.step(val_avg_loss)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_avg_loss < best_val_loss:
                    best_val_loss = val_avg_loss
                    print(f"  ğŸ† æ–°çš„æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
                
                self.model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ
        
        except Exception as e:
            print(f"å¢å¼ºå‹Transformerè®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    def train_continuously(self):
        """è¿ç»­è®­ç»ƒæ¨¡å‹çš„åå°çº¿ç¨‹"""
        while self.should_train:
            try:
                # åŠ è½½æ•°æ®
                df = self.load_training_data()
                if df is not None and len(df) > 0:
                    print(f"å¼€å§‹è®­ç»ƒå¢å¼ºå‹æ¨¡å‹ï¼Œæ•°æ®é‡: {len(df)}")
                    with self.model_lock:
                        self.train_model(df)
                    print("å¢å¼ºå‹æ¨¡å‹è®­ç»ƒå®Œæˆ")
                
                # è®­ç»ƒè¾ƒæ…¢ï¼Œæ¯30åˆ†é’Ÿè®­ç»ƒä¸€æ¬¡
                time.sleep(1800)
                
            except Exception as e:
                print(f"è®­ç»ƒçº¿ç¨‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿåç»§ç»­
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        with self.model_lock:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict()
            }, path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def log_performance(self, actual_action, predicted_action):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        with self.model_lock:
            if actual_action == predicted_action:
                self.performance_log['correct'] += 1
            self.performance_log['total'] += 1
            
            # æ‰“å°æ€§èƒ½æ‘˜è¦
            if self.performance_log['total'] > 0:
                acc = self.performance_log['correct'] / self.performance_log['total']
                print(f"ğŸ“Š å¢å¼ºå‹æ¨¡å‹æ€§èƒ½ - å‡†ç¡®ç‡: {acc:.3f}")


def main():
    parser = argparse.ArgumentParser(description='Enhanced Transformer Trading Strategy')
    parser.add_argument('--mode', choices=['train', 'predict'], default='predict',
                        help='è¿è¡Œæ¨¡å¼: train(ä»…è®­ç»ƒ), predict(ä»…é¢„æµ‹)')
    parser.add_argument('--model_path', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜æˆ–åŠ è½½è·¯å¾„')
    args = parser.parse_args()
    
    strategy = EnhancedTransformerStrategy(model_path=args.model_path)
    
    if args.mode == 'train':
        print("ä»…è¿è¡Œè®­ç»ƒæ¨¡å¼...")
        # è®­ç»ƒæ¨¡å¼ï¼Œä¸é€€å‡º
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("è®­ç»ƒå·²åœæ­¢")
    elif args.mode == 'predict':
        print("è¿è¡Œé¢„æµ‹æ¨¡å¼...")
        # æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
        sample_data = {
            'price_current': 93.5,
            'grid_lower': 93.0,
            'grid_upper': 94.0,
            'atr': 0.2,
            'rsi_1m': 30.0,
            'rsi_5m': 40.0,
            'buffer': 0.05,
            'threshold': 93.05,
            'near_lower': True,
            'rsi_ok': True
        }
        
        action, confidence = strategy.predict_action(sample_data)
        action_map = {0: "ä¸æ“ä½œ", 1: "ä¹°å…¥", 2: "å–å‡º"}
        
        print(f"ğŸ§  å¢å¼ºå‹Transformeré¢„æµ‹: {action_map[action]}, ç½®ä¿¡åº¦: {confidence:.3f}")
        
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("ç¨‹åºå·²åœæ­¢")
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜æ¨¡å‹
    if args.model_path:
        strategy.save_model(args.model_path)


if __name__ == "__main__":
    main()