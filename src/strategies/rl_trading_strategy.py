import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pandas as pd
import os
from datetime import datetime
import threading
import time
import glob
import argparse
from typing import Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class RLTradingNetwork(nn.Module):
    """ç”¨äºäº¤æ˜“å†³ç­–çš„å¼ºåŒ–å­¦ä¹ ç½‘ç»œ"""
    def __init__(self, input_size=12, action_size=3, hidden_size=512, num_layers=4):
        """
        è¾“å…¥: æŠ€æœ¯æŒ‡æ ‡å’Œå¸‚åœºçŠ¶æ€
        è¾“å‡º: ä¸‰ç§æ“ä½œçš„æ¦‚ç‡åˆ†å¸ƒ (ä¹°å…¥, å–å‡º, æŒæœ‰)
        """
        super(RLTradingNetwork, self).__init__()
        
        # è¾“å…¥å±‚
        self.input_layer = nn.Linear(input_size, hidden_size)
        
        # å¤šå±‚LSTMç”¨äºåºåˆ—å»ºæ¨¡
        self.lstm = nn.LSTM(
            input_size=hidden_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1
        )
        
        # Qå€¼ç½‘ç»œ - ä¼°è®¡æ¯ä¸ªåŠ¨ä½œçš„ä»·å€¼
        self.q_network = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.LayerNorm(hidden_size//2),
            nn.Dropout(0.2),
            nn.Linear(hidden_size//2, hidden_size//4),
            nn.ReLU(),
            nn.LayerNorm(hidden_size//4),
            nn.Linear(hidden_size//4, action_size)
        )
        
        # ä»·å€¼ç½‘ç»œ - ä¼°è®¡å½“å‰çŠ¶æ€çš„ä»·å€¼
        self.value_network = nn.Sequential(
            nn.Linear(hidden_size, hidden_size//2),
            nn.ReLU(),
            nn.LayerNorm(hidden_size//2),
            nn.Dropout(0.2),
            nn.Linear(hidden_size//2, hidden_size//4),
            nn.ReLU(),
            nn.LayerNorm(hidden_size//4),
            nn.Linear(hidden_size//4, 1)
        )
        
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.3)
        
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        batch_size = x.size(0)
        
        # è¾“å…¥å˜æ¢
        x = self.relu(self.input_layer(x))  # (batch, seq, hidden)
        
        # LSTMå¤„ç†
        lstm_out, _ = self.lstm(x)  # (batch, seq, hidden)
        
        # ä½¿ç”¨æœ€åä¸€æ­¥çš„è¾“å‡º
        final_hidden = lstm_out[:, -1, :]  # (batch, hidden)
        
        # è®¡ç®—Qå€¼å’ŒçŠ¶æ€ä»·å€¼
        q_values = self.q_network(final_hidden)  # (batch, action_size)
        state_value = self.value_network(final_hidden)  # (batch, 1)
        
        # è®¡ç®—åŠ¨ä½œæ¦‚ç‡ï¼ˆä½¿ç”¨ä¼˜åŠ¿å‡½æ•°ï¼‰
        advantages = q_values - state_value
        action_probs = torch.softmax(advantages, dim=-1)
        
        return action_probs, q_values


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""
    def __init__(self, capacity=10000):
        self.capacity = capacity
        self.buffer = []
        self.position = 0
    
    def push(self, state, action, reward, next_state, done):
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        batch = np.random.choice(len(self.buffer), batch_size, replace=False)
        state, action, reward, next_state, done = map(np.stack, zip(*[self.buffer[i] for i in batch]))
        return state, action, reward, next_state, done
    
    def __len__(self):
        return len(self.buffer)


class RLTradingStrategy:
    """åŸºäºå¼ºåŒ–å­¦ä¹ çš„äº¤æ˜“ç­–ç•¥"""
    def __init__(self, data_dir='/home/cx/trading_data', model_path=None, learning_rate=1e-4):
        # å¼ºåˆ¶ä½¿ç”¨GPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print(f"Using GPU: {torch.cuda.get_device_name()}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ­¤ç­–ç•¥éœ€è¦GPUè¿è¡Œ")
        
        self.data_dir = data_dir
        
        # åˆå§‹åŒ–ç½‘ç»œ
        self.network = RLTradingNetwork().to(self.device)
        self.target_network = RLTradingNetwork().to(self.device)
        self.optimizer = optim.AdamW(self.network.parameters(), lr=learning_rate, weight_decay=0.01)
        
        # å¤åˆ¶å‚æ•°åˆ°ç›®æ ‡ç½‘ç»œ
        self.target_network.load_state_dict(self.network.state_dict())
        
        # ç»éªŒå›æ”¾ç¼“å†²åŒº
        self.memory = ReplayBuffer(capacity=10000)
        
        # å¼ºåŒ–å­¦ä¹ å‚æ•°
        self.gamma = 0.95  # æŠ˜æ‰£å› å­
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.update_target_freq = 1000  # æ›´æ–°ç›®æ ‡ç½‘ç»œé¢‘ç‡
        self.step_count = 0
        
        # æ§åˆ¶è®­ç»ƒå’Œæ¨ç†çš„æ ‡å¿—
        self.should_train = True
        self.model_lock = threading.Lock()
        
        # è®°å½•äº¤æ˜“å†å²å’Œæ€§èƒ½
        self.performance_log = {
            'total_reward': 0,
            'total_steps': 0,
            'win_count': 0,
            'loss_count': 0,
            'total_trades': 0
        }
        
        # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.network.parameters())
        trainable_params = sum(p.numel() for p in self.network.parameters() if p.requires_grad)
        print(f"RL Trading Networkå‚æ•°æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if model_path and os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.network.load_state_dict(checkpoint['network_state_dict'])
                self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                self.epsilon = checkpoint.get('epsilon', self.epsilon)
                print(f"âœ… ä» {model_path} åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åˆå§‹æ¨¡å‹")
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        self.training_thread = threading.Thread(target=self.train_continuously, daemon=True)
        self.training_thread.start()
    
    def prepare_features(self, row):
        """ä»æ•°æ®è¡Œä¸­å‡†å¤‡ç‰¹å¾å‘é‡"""
        try:
            features = [
                row['price_current'],
                row['atr'],
                row['rsi_1m'] if pd.notna(row['rsi_1m']) else 50,
                row['rsi_5m'] if pd.notna(row['rsi_5m']) else 50,
                row.get('boll_upper', 0),
                row.get('boll_mid', 0),
                row.get('boll_lower', 0),
                row.get('boll_position', 0.5),
                row.get('price_change_1', 0),
                row.get('price_change_5', 0),
                row.get('volatility', 0),
                row.get('volume_1m', 0)
            ]
            # å½’ä¸€åŒ–ç‰¹å¾
            features_np = np.array(features)
            mean_val = np.mean(features_np)
            std_val = np.std(features_np) + 1e-8
            normalized_features = (features_np - mean_val) / std_val
            return normalized_features.tolist()
        except Exception as e:
            print(f"prepare_featuresé”™è¯¯: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾å€¼
            return [0.0] * 12
    
    def predict_action(self, current_data):
        """ä½¿ç”¨æ¨¡å‹é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        with self.model_lock:
            try:
                # å‡†å¤‡è¾“å…¥æ•°æ®
                features = self.prepare_features(current_data)
                
                # ä»¥ä¸€å®šæ¦‚ç‡éšæœºæ¢ç´¢
                if np.random.random() <= self.epsilon:
                    action = np.random.choice([0, 1, 2])  # éšæœºé€‰æ‹©åŠ¨ä½œ
                    confidence = 0.33  # éšæœºåŠ¨ä½œçš„ç½®ä¿¡åº¦è¾ƒä½
                    return int(action), float(confidence)
                
                # ä½¿ç”¨æ¨¡å‹é¢„æµ‹
                input_tensor = torch.tensor([features], dtype=torch.float32).unsqueeze(1).to(self.device)  # (1, 1, 10)
                
                # æ¨¡å‹é¢„æµ‹
                with torch.no_grad():
                    self.network.eval()
                    action_probs, _ = self.network(input_tensor)
                    action_probs = action_probs.cpu().numpy()[0]
                    
                    # æ ¹æ®æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ
                    action = np.random.choice(len(action_probs), p=action_probs)
                    confidence = action_probs[action]
                
                return int(action), float(confidence)
            except Exception as e:
                print(f"é¢„æµ‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return 0, 0.0
    
    def compute_reward(self, action, current_data, prev_data=None):
        """è®¡ç®—å¼ºåŒ–å­¦ä¹ å¥–åŠ±å‡½æ•°"""
        # åŸºç¡€å¥–åŠ±è®¡ç®—
        # action: 0=æŒæœ‰, 1=ä¹°å…¥, 2=å–å‡º
        # rewardåº”è¯¥åæ˜ äº¤æ˜“çš„ç›ˆåˆ©èƒ½åŠ›å’Œé£é™©æ§åˆ¶
        
        price_current = current_data['price_current']
        if prev_data is not None:
            prev_price = prev_data['price_current']
            if prev_price > 0:
                price_return = (price_current - prev_price) / prev_price  # è®¡ç®—æ”¶ç›Šç‡
            else:
                return 0.0
        else:
            return 0.0  # æ²¡æœ‰å‰ä¸€ä¸ªçŠ¶æ€æ—¶è¿”å›0å¥–åŠ±
        
        # æ ¹æ®åŠ¨ä½œå’Œæ”¶ç›Šç‡è®¡ç®—å¥–åŠ±
        if action == 1:  # ä¹°å…¥
            # ä¹°å…¥åä»·æ ¼ä¸Šæ¶¨è·å¾—æ­£å¥–åŠ±ï¼Œä¸‹è·Œè·å¾—è´Ÿå¥–åŠ±
            reward = price_return
        elif action == 2:  # å–å‡º
            # å–å‡ºåä»·æ ¼ä¸‹è·Œè·å¾—æ­£å¥–åŠ±ï¼Œä¸Šæ¶¨è·å¾—è´Ÿå¥–åŠ±
            reward = -price_return
        else:  # æŒæœ‰
            # æŒæœ‰æ—¶æ ¹æ®å¸‚åœºè¶‹åŠ¿è·å¾—è¾ƒå°å¥–åŠ±
            reward = abs(price_return) * 0.1
        
        # æ”¾å¤§å¥–åŠ±ä¿¡å·ä»¥ä¾¿æ›´å¥½åœ°è®­ç»ƒ
        reward *= 100
        
        # æ·»åŠ ä¸€äº›åŸºäºæŠ€æœ¯æŒ‡æ ‡çš„å¥–åŠ±ä¿®æ­£
        if current_data['rsi_1m'] is not None and current_data['rsi_5m'] is not None:
            # å¦‚æœRSIæ˜¾ç¤ºè¶…ä¹°è¶…å–ï¼Œä¸”é‡‡å–äº†ç›¸åº”çš„åå‘æ“ä½œï¼Œç»™äºˆé¢å¤–å¥–åŠ±
            if action == 2 and current_data['rsi_1m'] > 70:  # å–å‡ºä¸”è¶…ä¹°
                reward += 0.5
            elif action == 1 and current_data['rsi_1m'] < 30:  # ä¹°å…¥ä¸”è¶…å–
                reward += 0.5
        
        return float(reward)
    
    def remember(self, state, action, reward, next_state, done):
        """å°†ç»éªŒå­˜å‚¨åˆ°å›æ”¾ç¼“å†²åŒº"""
        self.memory.push(state, action, reward, next_state, done)
    
    def replay(self, batch_size=32):
        """ä»ç»éªŒå›æ”¾ç¼“å†²åŒºä¸­é‡‡æ ·å¹¶è®­ç»ƒ"""
        if len(self.memory) < batch_size:
            return
        
        # ä»ç»éªŒæ± ä¸­é‡‡æ ·
        states, actions, rewards, next_states, dones = self.memory.sample(batch_size)
        
        # è½¬æ¢ä¸ºtensor
        states = torch.FloatTensor(states).unsqueeze(1).to(self.device)  # (batch, 1, input_size)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(next_states).unsqueeze(1).to(self.device)
        dones = torch.BoolTensor(dones).to(self.device)
        
        # è®¡ç®—å½“å‰Qå€¼
        current_q_values, _ = self.network(states)
        current_q_values = current_q_values.gather(1, actions.unsqueeze(1))
        
        # è®¡ç®—ç›®æ ‡Qå€¼
        next_q_values, _ = self.target_network(next_states)
        next_q_values = next_q_values.max(1)[0].detach()
        target_q_values = rewards + (self.gamma * next_q_values * ~dones)
        
        # è®¡ç®—æŸå¤±
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        # æ¢¯åº¦è£å‰ªä»¥ç¨³å®šè®­ç»ƒ
        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)
        self.optimizer.step()
        
        # æ›´æ–°æ¢ç´¢ç‡
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        all_data_files = []
        
        # è·å–æ‰€æœ‰æ•°æ®ç›®å½•
        all_data_dirs = glob.glob(os.path.join(self.data_dir, '202*-*-*'))
        if all_data_dirs:
            # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶
            sorted_dirs = sorted(all_data_dirs, reverse=True)
            for data_dir in sorted_dirs[:7]:  # ä½¿ç”¨æœ€è¿‘7å¤©çš„æ•°æ®
                # åŒ…å«åŸå§‹æ•°æ®å’Œæ‰©å±•æ•°æ®
                data_files = glob.glob(os.path.join(data_dir, 'trading_data_*.csv'))
                data_files.extend(glob.glob(os.path.join(data_dir, 'extended_trading_data_*.csv')))
                data_files.extend(glob.glob(os.path.join(data_dir, 'prepared_features_*.csv')))  # ä¹ŸåŒ…å«å‡†å¤‡å¥½çš„ç‰¹å¾æ•°æ®
                all_data_files.extend(data_files)
        
        if not all_data_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        data_file = sorted(all_data_files, key=os.path.getmtime, reverse=True)[0]
        
        try:
            df = pd.read_csv(data_file)
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=['price_current', 'grid_lower', 'grid_upper', 'atr', 'rsi_1m', 'rsi_5m'])
            
            if len(df) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return None
                
            return df
        except Exception as e:
            print(f"åŠ è½½è®­ç»ƒæ•°æ®é”™è¯¯: {e}")
            return None
    
    def train_model(self, df):
        """ä½¿ç”¨å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¨¡å‹"""
        try:
            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸºäºç›ˆåˆ©çš„æ ‡ç­¾
            states = []
            actions = []
            rewards = []
            next_states = []
            dones = []
            
            look_ahead = 10  # å‘å‰çœ‹10ä¸ªæ—¶é—´æ­¥é•¿æ¥è®¡ç®—ç›ˆåˆ©
            
            for i in range(1, len(df) - look_ahead):
                prev_row = df.iloc[i-1]
                curr_row = df.iloc[i]
                
                # è®¡ç®—æœªæ¥look_aheadæ­¥çš„ç›ˆåˆ©æ¥ç¡®å®šæœ€ä½³åŠ¨ä½œ
                current_price = curr_row['price_current']
                future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
                
                if len(future_prices) == 0:
                    continue
                
                # è®¡ç®—æœ€å¤§ç›ˆåˆ©å’Œæœ€å¤§äºæŸ
                max_future_price = max(future_prices)
                min_future_price = min(future_prices)
                
                buy_profit = (max_future_price - current_price) / current_price
                sell_profit = (current_price - min_future_price) / current_price
                
                # ç¡®å®šæœ€ä½³åŠ¨ä½œ
                profit_threshold = 0.002  # 0.2%çš„é˜ˆå€¼
                if buy_profit > profit_threshold and buy_profit > sell_profit:
                    optimal_action = 1  # ä¹°å…¥
                elif sell_profit > profit_threshold and sell_profit > buy_profit:
                    optimal_action = 2  # å–å‡º
                else:
                    optimal_action = 0  # ä¸æ“ä½œ
                
                # å‡†å¤‡çŠ¶æ€è¡¨ç¤º
                prev_state = self.prepare_features(prev_row)
                curr_state = self.prepare_features(curr_row)
                
                # ä½¿ç”¨æœ€ä¼˜åŠ¨ä½œè®¡ç®—å¥–åŠ±
                reward = self.compute_reward(optimal_action, curr_row, prev_row)
                
                # å­˜å‚¨åˆ°ç»éªŒæ± 
                self.remember(prev_state, optimal_action, reward, curr_state, False)
            
            # ä»ç»éªŒæ± ä¸­è®­ç»ƒ
            if len(self.memory) >= 32:
                for epoch in range(5):  # è®­ç»ƒ5æ¬¡
                    batch_losses = []
                    for _ in range(10):  # æ¯æ¬¡è®­ç»ƒæŠ½10ä¸ªæ‰¹æ¬¡
                        if len(self.memory) >= 32:
                            try:
                                # ä»ç»éªŒæ± ä¸­æŠ½æ ·
                                states, actions, rewards, next_states, dones = self.memory.sample(32)
                                
                                # è½¬æ¢ä¸ºtensor
                                states = torch.FloatTensor(states).unsqueeze(1).to(self.device)  # (batch, 1, input_size)
                                actions = torch.LongTensor(actions).to(self.device)
                                rewards = torch.FloatTensor(rewards).to(self.device)
                                next_states = torch.FloatTensor(next_states).unsqueeze(1).to(self.device)
                                dones = torch.BoolTensor(dones).to(self.device)
                                
                                # è®¡ç®—å½“å‰Qå€¼
                                current_q_values, _ = self.network(states)
                                current_q_values = current_q_values.gather(1, actions.unsqueeze(1))
                                
                                # è®¡ç®—ç›®æ ‡Qå€¼
                                next_q_values, _ = self.target_network(next_states)
                                next_q_values = next_q_values.max(1)[0].detach()
                                target_q_values = rewards + (self.gamma * next_q_values * ~dones)
                                
                                # è®¡ç®—æŸå¤±
                                loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
                                
                                # åå‘ä¼ æ’­
                                self.optimizer.zero_grad()
                                loss.backward()
                                # æ¢¯åº¦è£å‰ªä»¥ç¨³å®šè®­ç»ƒ
                                torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)
                                self.optimizer.step()
                                
                                batch_losses.append(loss.item())
                            except:
                                continue
                    
                    if batch_losses:
                        avg_loss = sum(batch_losses) / len(batch_losses)
                        print(f"RLè®­ç»ƒè½®æ¬¡ {epoch+1}/5, å¹³å‡æŸå¤±: {avg_loss:.4f}")
            
            # æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.step_count += 1
            if self.step_count % self.update_target_freq == 0:
                self.target_network.load_state_dict(self.network.state_dict())
                print(f"âœ… ç›®æ ‡ç½‘ç»œå·²æ›´æ–° (step: {self.step_count})")
        
        except Exception as e:
            print(f"RLæ¨¡å‹è®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    def train_continuously(self):
        """è¿ç»­è®­ç»ƒæ¨¡å‹çš„åå°çº¿ç¨‹"""
        while self.should_train:
            try:
                # åŠ è½½æ•°æ®
                df = self.load_training_data()
                if df is not None and len(df) > 0:
                    print(f"å¼€å§‹RLè®­ç»ƒï¼Œæ•°æ®é‡: {len(df)}")
                    with self.model_lock:
                        self.train_model(df)
                    print("RLæ¨¡å‹è®­ç»ƒå®Œæˆ")
                
                # è®­ç»ƒè¾ƒæ…¢ï¼Œæ¯30åˆ†é’Ÿè®­ç»ƒä¸€æ¬¡
                time.sleep(1800)
                
            except Exception as e:
                print(f"RLè®­ç»ƒçº¿ç¨‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿåç»§ç»­
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        with self.model_lock:
            torch.save({
                'network_state_dict': self.network.state_dict(),
                'target_network_state_dict': self.target_network.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'epsilon': self.epsilon
            }, path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def log_performance(self, actual_action, predicted_action, reward):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        with self.model_lock:
            self.performance_log['total_reward'] += reward
            self.performance_log['total_steps'] += 1
            
            if reward > 0:
                self.performance_log['win_count'] += 1
            elif reward < 0:
                self.performance_log['loss_count'] += 1
                
            self.performance_log['total_trades'] += 1
            
            # æ‰“å°æ€§èƒ½æ‘˜è¦
            if self.performance_log['total_steps'] > 0:
                avg_reward = self.performance_log['total_reward'] / self.performance_log['total_steps']
                win_rate = self.performance_log['win_count'] / max(self.performance_log['total_trades'], 1)
                print(f"ğŸ“Š RLæ¨¡å‹æ€§èƒ½ - å¹³å‡å¥–åŠ±: {avg_reward:.3f}, èƒœç‡: {win_rate:.3f}, Îµ: {self.epsilon:.3f}")


def main():
    parser = argparse.ArgumentParser(description='Reinforcement Learning Trading Strategy')
    parser.add_argument('--mode', choices=['train', 'predict'], default='predict',
                        help='è¿è¡Œæ¨¡å¼: train(ä»…è®­ç»ƒ), predict(ä»…é¢„æµ‹)')
    parser.add_argument('--model_path', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜æˆ–åŠ è½½è·¯å¾„')
    args = parser.parse_args()
    
    strategy = RLTradingStrategy(model_path=args.model_path)
    
    if args.mode == 'train':
        print("ä»…è¿è¡ŒRLè®­ç»ƒæ¨¡å¼...")
        # è®­ç»ƒæ¨¡å¼ï¼Œä¸é€€å‡º
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("RLè®­ç»ƒå·²åœæ­¢")
    elif args.mode == 'predict':
        print("è¿è¡ŒRLé¢„æµ‹æ¨¡å¼...")
        # æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
        sample_data = {
            'price_current': 93.5,
            'grid_lower': 93.0,
            'grid_upper': 94.0,
            'atr': 0.2,
            'rsi_1m': 30.0,
            'rsi_5m': 40.0,
            'buffer': 0.05,
            'threshold': 93.05,
            'near_lower': True,
            'rsi_ok': True
        }
        
        action, confidence = strategy.predict_action(sample_data)
        action_map = {0: "æŒæœ‰", 1: "ä¹°å…¥", 2: "å–å‡º"}
        
        print(f"ğŸ§  RLæ¨¡å‹é¢„æµ‹: {action_map[action]}, ç½®ä¿¡åº¦: {confidence:.3f}")
        
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("ç¨‹åºå·²åœæ­¢")
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜æ¨¡å‹
    if args.model_path:
        strategy.save_model(args.model_path)


if __name__ == "__main__":
    main()