import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
from datetime import datetime
import threading
import time
import glob
import argparse
from typing import Tuple, Optional
import warnings
warnings.filterwarnings("ignore")

class LargeTradingTransformer(nn.Module):
    """ç”¨äºäº¤æ˜“å†³ç­–çš„å¤§å‹Transformeræ¨¡å‹"""
    def __init__(self, input_size=12, nhead=8, num_layers=6, output_size=3, d_model=256):
        super(LargeTradingTransformer, self).__init__()
        self.d_model = d_model
        self.input_projection = nn.Linear(input_size, d_model)
        # ä½ç½®ç¼–ç  - ä½¿ç”¨æ­£å¼¦ä½™å¼¦ç¼–ç 
        self.register_buffer('pos_encoding', self._create_positional_encoding(1000, d_model))
        
        # å¤šå±‚Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead, 
            dim_feedforward=d_model*4,  # FFNéšè—å±‚æ˜¯d_modelçš„4å€
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # å¤šå±‚åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(d_model//2, d_model//4),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model//4, output_size)
        )
        self.softmax = nn.Softmax(dim=1)
    
    def _create_positional_encoding(self, max_len, d_model):
        """åˆ›å»ºæ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç """
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                            -(np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        return pe.unsqueeze(0)  # (1, max_len, d_model)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        batch_size, seq_len = x.size(0), x.size(1)
        
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # (batch_size, seq_len, d_model)
        
        # æ·»åŠ ä½ç½®ç¼–ç 
        if seq_len <= self.pos_encoding.size(1):
            pos_enc = self.pos_encoding[:, :seq_len, :]
        else:
            # å¦‚æœåºåˆ—å¤ªé•¿ï¼Œæ‰©å±•ä½ç½®ç¼–ç 
            extended_pe = self._create_positional_encoding(seq_len, self.d_model).to(x.device)
            pos_enc = extended_pe[:, :seq_len, :]
        
        x = x + pos_enc
        
        # Transformerç¼–ç 
        out = self.transformer(x)  # (batch_size, seq_len, d_model)
        
        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = out[:, -1, :]  # (batch_size, d_model)
        
        # åˆ†ç±»
        out = self.classifier(out)  # (batch_size, output_size)
        out = self.softmax(out)
        return out


class LargeTransformerStrategy:
    """å¤§å‹Transformerç­–ç•¥"""
    def __init__(self, data_dir='/home/cx/trading_data', model_path=None):
        # å¼ºåˆ¶ä½¿ç”¨GPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print(f"Using GPU: {torch.cuda.get_device_name()}")
            print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ­¤ç­–ç•¥éœ€è¦GPUè¿è¡Œ")
        
        self.data_dir = data_dir
        
        # åˆå§‹åŒ–å¤§å‹æ¨¡å‹
        self.model = LargeTradingTransformer(input_size=12).to(self.device)
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0005, weight_decay=0.01)
        self.criterion = nn.CrossEntropyLoss()
        
        # æ§åˆ¶è®­ç»ƒå’Œæ¨ç†çš„æ ‡å¿—
        self.should_train = True
        self.model_lock = threading.Lock()
        
        # è®°å½•æ¨¡å‹æ€§èƒ½
        self.performance_log = {
            'correct': 0,
            'total': 0
        }
        
        # æ‰“å°æ¨¡å‹å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Transformeræ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if model_path and os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print(f"âœ… ä» {model_path} åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åˆå§‹æ¨¡å‹")
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        self.training_thread = threading.Thread(target=self.train_continuously, daemon=True)
        self.training_thread.start()
    
    def prepare_features(self, row):
        """ä»æ•°æ®è¡Œä¸­å‡†å¤‡ç‰¹å¾å‘é‡"""
        try:
            features = [
                row['price_current'],
                row['atr'],
                row['rsi_1m'] if pd.notna(row['rsi_1m']) else 50,
                row['rsi_5m'] if pd.notna(row['rsi_5m']) else 50,
                row.get('boll_upper', 0),
                row.get('boll_mid', 0),
                row.get('boll_lower', 0),
                row.get('boll_position', 0.5),
                row.get('price_change_1', 0),
                row.get('price_change_5', 0),
                row.get('volatility', 0),
                row.get('volume_1m', 0)
            ]
            # å½’ä¸€åŒ–ç‰¹å¾
            features_np = np.array(features)
            mean_val = np.mean(features_np)
            std_val = np.std(features_np) + 1e-8
            normalized_features = (features_np - mean_val) / std_val
            return normalized_features.tolist()
        except Exception as e:
            print(f"prepare_featuresé”™è¯¯: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾å€¼
            return [0.0] * 12
    
    
    
    def predict_action(self, current_data):
        """ä½¿ç”¨æ¨¡å‹é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        with self.model_lock:
            try:
                # å‡†å¤‡è¾“å…¥æ•°æ®
                features = self.prepare_features(current_data)
                input_tensor = torch.tensor([features], dtype=torch.float32).unsqueeze(1).to(self.device)  # (1, 1, 10)
                
                # æ¨¡å‹é¢„æµ‹
                with torch.no_grad():
                    self.model.eval()
                    prediction = self.model(input_tensor)
                    probabilities = prediction.cpu().numpy()
                    action = np.argmax(probabilities[0])
                    confidence = probabilities[0][action]
                
                return int(action), float(confidence)
            except Exception as e:
                print(f"é¢„æµ‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return 0, 0.0
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        all_data_files = []
        
        # è·å–æ‰€æœ‰æ•°æ®ç›®å½•
        all_data_dirs = glob.glob(os.path.join(self.data_dir, '202*-*-*'))
        if all_data_dirs:
            # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶
            sorted_dirs = sorted(all_data_dirs, reverse=True)
            for data_dir in sorted_dirs[:7]:  # ä½¿ç”¨æœ€è¿‘7å¤©çš„æ•°æ®
                # åŒ…å«åŸå§‹æ•°æ®å’Œæ‰©å±•æ•°æ®
                data_files = glob.glob(os.path.join(data_dir, 'trading_data_*.csv'))
                data_files.extend(glob.glob(os.path.join(data_dir, 'extended_trading_data_*.csv')))
                data_files.extend(glob.glob(os.path.join(data_dir, 'prepared_features_*.csv')))  # ä¹ŸåŒ…å«å‡†å¤‡å¥½çš„ç‰¹å¾æ•°æ®
                all_data_files.extend(data_files)
        
        if not all_data_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        data_file = sorted(all_data_files, key=os.path.getmtime, reverse=True)[0]
        
        try:
            df = pd.read_csv(data_file)
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=['price_current', 'grid_lower', 'grid_upper', 'atr', 'rsi_1m', 'rsi_5m'])
            
            if len(df) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return None
                
            return df
        except Exception as e:
            print(f"åŠ è½½è®­ç»ƒæ•°æ®é”™è¯¯: {e}")
            return None
    
    def calculate_class_weights(self, y):
        """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®"""
        import numpy as np
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        classes, counts = np.unique(y, return_counts=True)
        total_samples = len(y)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ (æ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°))
        weights = []
        for count in counts:
            weight = total_samples / (len(classes) * count)
            weights.append(weight)
        
        # è½¬æ¢ä¸ºtensor
        class_weights = torch.FloatTensor(weights).to(self.device)
        return class_weights

    def train_model(self, df):
        """è®­ç»ƒæ¨¡å‹"""
        try:
            # é‡æ–°åˆå§‹åŒ–ä¼˜åŒ–å™¨ä»¥é¿å…çŠ¶æ€é—®é¢˜ï¼ˆä½¿ç”¨AdamWè€Œä¸æ˜¯Adamï¼‰
            self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.0005, weight_decay=0.01)
            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, mode='max', factor=0.5, patience=5
            )
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸºäºç›ˆåˆ©çš„æ ‡ç­¾ï¼ˆä¸LSTMä¿æŒä¸€è‡´ï¼‰
            X, y = [], []
            look_ahead = 10  # å‘å‰çœ‹10ä¸ªæ—¶é—´æ­¥é•¿æ¥è®¡ç®—ç›ˆåˆ©
            seq_length = 10  # ä½¿ç”¨åºåˆ—é•¿åº¦10ï¼ˆä¸LSTMä¸€è‡´ï¼‰
            min_required = seq_length + look_ahead  # éœ€è¦è‡³å°‘seq_length + look_aheadä¸ªæ•°æ®ç‚¹
            
            print(f"ğŸ“Š ä½¿ç”¨åºåˆ—é•¿åº¦: {seq_length}, éœ€è¦è‡³å°‘ {min_required} ä¸ªæ•°æ®ç‚¹")
            
            # å‡†å¤‡åºåˆ—ç‰¹å¾ï¼ˆä¸LSTMä¿æŒä¸€è‡´ï¼‰
            for i in range(min_required, len(df)):
                # æ£€æŸ¥æ•°æ®æ˜¯å¦è¶³å¤Ÿ
                if i + look_ahead >= len(df):
                    break
                
                # å‡†å¤‡åºåˆ—ç‰¹å¾ï¼ˆå†å²seq_lengthä¸ªæ—¶é—´æ­¥ï¼‰
                # æ„å»ºåºåˆ—ï¼šä½¿ç”¨æœ€è¿‘seq_lengthä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
                sequence_features = []
                for j in range(max(0, i - seq_length + 1), i + 1):
                    row = df.iloc[j]
                    features = self.prepare_features(row)
                    sequence_features.append(features)
                
                # å¦‚æœåºåˆ—ä¸è¶³seq_lengthï¼Œç”¨ç¬¬ä¸€ä¸ªå€¼å¡«å……
                while len(sequence_features) < seq_length:
                    if sequence_features:
                        sequence_features.insert(0, sequence_features[0])
                    else:
                        sequence_features.insert(0, [0.0] * 12)
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                sequence = np.array(sequence_features[-seq_length:], dtype=np.float32)
                X.append(sequence)
                
                # è®¡ç®—æœªæ¥look_aheadæ­¥çš„ç›ˆåˆ©
                current_price = row['price_current']
                future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
                
                if len(future_prices) == 0:
                    continue
                    
                # è®¡ç®—æœ€å¤§ç›ˆåˆ©å’Œæœ€å¤§äºæŸ
                max_future_price = max(future_prices)
                min_future_price = min(future_prices)
                
                buy_profit = (max_future_price - current_price) / current_price
                sell_profit = (current_price - min_future_price) / current_price
                
                # åˆ›å»ºæ ‡ç­¾: 0=ä¸æ“ä½œ, 1=ä¹°å…¥, 2=å–å‡º
                # åªæœ‰å½“é¢„æœŸç›ˆåˆ©è¶…è¿‡é˜ˆå€¼æ—¶æ‰å»ºè®®æ“ä½œ
                profit_threshold = 0.005  # æé«˜é˜ˆå€¼åˆ°0.5%ï¼Œå‡å°‘äº¤æ˜“é¢‘ç‡ä½†æé«˜è´¨é‡
                min_diff = 0.003  # æœ€å°å·®å€¼ï¼Œç¡®ä¿ä¹°å–ä¹‹é—´æœ‰è¶³å¤Ÿå·®è·
                
                # åªæœ‰å½“ä¹°å–ç›ˆåˆ©å·®å€¼è¶…è¿‡æœ€å°å·®å€¼ä¸”è¶…è¿‡é˜ˆå€¼æ—¶æ‰äº¤æ˜“
                if abs(buy_profit - sell_profit) >= min_diff:
                    if buy_profit > sell_profit and buy_profit > profit_threshold:
                        label = 1  # ä¹°å…¥
                    elif sell_profit > buy_profit and sell_profit > profit_threshold:
                        label = 2  # å–å‡º
                    else:
                        label = 0  # ä¸æ“ä½œ
                else:
                    label = 0  # ä¸æ“ä½œ - å·®å€¼å¤ªå°ï¼Œä¸ç¡®å®šæ€§é«˜
                    
                y.append(label)
            
            if len(X) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return
            
            # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
            unique, counts = np.unique(y, return_counts=True)
            label_distribution = dict(zip(unique, counts))
            print(f"æ ‡ç­¾åˆ†å¸ƒ: {label_distribution}")
            
            # å¦‚æœæŸä¸ªç±»åˆ«å æ¯”è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            total_samples = len(y)
            for label, count in label_distribution.items():
                percentage = count / total_samples * 100
                print(f"æ ‡ç­¾ {label} å æ¯”: {percentage:.2f}%")
            
            # è®¡ç®—ç±»åˆ«æƒé‡ï¼ˆç¡®ä¿æœ‰3ä¸ªç±»åˆ«ï¼‰
            unique_labels = np.unique(y)
            if len(unique_labels) < 3:
                print(f"âš ï¸ è­¦å‘Š: åªæœ‰ {len(unique_labels)} ä¸ªç±»åˆ«ï¼Œéœ€è¦è‡³å°‘3ä¸ªç±»åˆ«")
                # å¦‚æœç±»åˆ«ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤æƒé‡
                class_weights = None
            else:
                class_weights = self.calculate_class_weights(y)
                print(f"ç±»åˆ«æƒé‡: {class_weights}")
            
            # æ›´æ–°æŸå¤±å‡½æ•°
            if class_weights is not None and len(class_weights) == 3:
                self.criterion = nn.CrossEntropyLoss(weight=class_weights)
            else:
                self.criterion = nn.CrossEntropyLoss()  # ä¸ä½¿ç”¨ç±»åˆ«æƒé‡
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
            split_idx = int(len(X) * 0.8)
            
            X_train = X[:split_idx]
            y_train = y[:split_idx]
            X_val = X[split_idx:]
            y_val = y[split_idx:]
            
            X_train = np.array(X_train)
            y_train = np.array(y_train)
            X_val = np.array(X_val)
            y_val = np.array(y_val)
            
            # è½¬æ¢ä¸ºå¼ é‡
            # X_trainå·²ç»æ˜¯3D (n_samples, seq_length, features)ï¼Œä¸éœ€è¦unsqueeze
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)
            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)
            y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)
            
            # ç¡®ä¿Xæ˜¯3D (batch, seq, features)
            if len(X_train_tensor.shape) == 2:
                # å¦‚æœæ˜¯2Dï¼Œéœ€è¦reshape
                feature_size = X_train_tensor.shape[1] // seq_length if X_train_tensor.shape[1] >= seq_length else 12
                X_train_tensor = X_train_tensor.view(-1, seq_length, feature_size)
                X_val_tensor = X_val_tensor.view(-1, seq_length, feature_size)
            elif len(X_train_tensor.shape) == 3:
                # å·²ç»æ˜¯3Dï¼Œç›´æ¥ä½¿ç”¨
                pass
            else:
                print(f"âš ï¸ æœªçŸ¥çš„æ•°æ®å½¢çŠ¶: {X_train_tensor.shape}ï¼Œå°è¯•reshape")
                X_train_tensor = X_train_tensor.view(X_train_tensor.size(0), -1, X_train_tensor.size(-1))
                X_val_tensor = X_val_tensor.view(X_val_tensor.size(0), -1, X_val_tensor.size(-1))
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
            
            # è®­ç»ƒæ¨¡å‹
            self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            best_val_acc = 0.0
            patience_counter = 0
            max_epochs = 50  # å¢åŠ åˆ°50è½®è®­ç»ƒ
            patience = 10  # æ—©åœè€å¿ƒå€¼
            
            # ç¡®ä¿max_epochsæ­£ç¡®
            actual_max_epochs = min(max_epochs, 50)  # é™åˆ¶æœ€å¤§è½®æ¬¡
            
            for epoch in range(actual_max_epochs):
                # è®­ç»ƒé˜¶æ®µ
                total_loss = 0
                num_batches = 0
                correct_predictions = 0
                total_predictions = 0
                
                for batch_x, batch_y in train_loader:
                    # ç¡®ä¿batch_xæ˜¯3D (batch, seq, features)
                    if len(batch_x.shape) == 4:
                        # å¦‚æœæ˜¯4Dï¼Œreshapeä¸º3D
                        batch_x = batch_x.squeeze(1)
                    elif len(batch_x.shape) == 2:
                        # å¦‚æœæ˜¯2Dï¼Œéœ€è¦reshape
                        batch_x = batch_x.view(batch_x.size(0), -1, batch_x.size(-1))
                    
                    self.optimizer.zero_grad()
                    outputs = self.model(batch_x)
                    loss = self.criterion(outputs, batch_y)
                    
                    # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnan
                    if torch.isnan(loss):
                        print(f"âš ï¸ è­¦å‘Š: æŸå¤±ä¸ºnanï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    
                    loss.backward()
                    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    predictions = torch.argmax(outputs, dim=1)
                    correct_predictions += (predictions == batch_y).sum().item()
                    total_predictions += batch_y.size(0)
                    
                    total_loss += loss.item()
                    num_batches += 1
                
                train_avg_loss = total_loss / num_batches
                train_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
                
                # éªŒè¯é˜¶æ®µ
                self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                val_correct = 0
                val_total = 0
                val_loss = 0
                with torch.no_grad():
                    for batch_x, batch_y in val_loader:
                        # ç¡®ä¿batch_xæ˜¯3D
                        if len(batch_x.shape) == 4:
                            batch_x = batch_x.squeeze(1)
                        elif len(batch_x.shape) == 2:
                            batch_x = batch_x.view(batch_x.size(0), -1, batch_x.size(-1))
                        
                        outputs = self.model(batch_x)
                        loss = self.criterion(outputs, batch_y)
                        
                        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºnan
                        if not torch.isnan(loss):
                            val_loss += loss.item()
                        
                        predictions = torch.argmax(outputs, dim=1)
                        val_correct += (predictions == batch_y).sum().item()
                        val_total += batch_y.size(0)
                
                val_avg_loss = val_loss / len(val_loader)
                val_accuracy = val_correct / val_total if val_total > 0 else 0
                
                # å­¦ä¹ ç‡è°ƒåº¦
                self.scheduler.step(val_avg_loss if not np.isnan(val_avg_loss) else float('inf'))
                
                print(f"Transformerè®­ç»ƒè½®æ¬¡ {epoch+1}/{actual_max_epochs}")
                print(f"  è®­ç»ƒ - æŸå¤±: {train_avg_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.3f}")
                print(f"  éªŒè¯ - æŸå¤±: {val_avg_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.3f}")
                print(f"  å­¦ä¹ ç‡: {self.optimizer.param_groups[0]['lr']:.6f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹å’Œæ—©åœ
                if val_accuracy > best_val_acc:
                    best_val_acc = val_accuracy
                    patience_counter = 0
                    print(f"  ğŸ† æ–°çš„æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3f}")
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_path = os.path.join(self.data_dir, 'best_transformer_model.pth')
                    torch.save({
                        'model_state_dict': self.model.state_dict(),
                        'optimizer_state_dict': self.optimizer.state_dict(),
                        'best_val_acc': best_val_acc,
                        'epoch': epoch
                    }, best_model_path)
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        print(f"  â¹ï¸ æ—©åœäºç¬¬ {epoch+1} è½®ï¼ˆéªŒè¯å‡†ç¡®ç‡æœªæå‡ {patience} è½®ï¼‰")
                        break
                
                self.model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ
        
        except Exception as e:
            print(f"Transformerè®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    def train_continuously(self):
        """è¿ç»­è®­ç»ƒæ¨¡å‹çš„åå°çº¿ç¨‹"""
        while self.should_train:
            try:
                # åŠ è½½æ•°æ®
                df = self.load_training_data()
                if df is not None and len(df) > 0:
                    print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œæ•°æ®é‡: {len(df)}")
                    with self.model_lock:
                        self.train_model(df)
                    print("æ¨¡å‹è®­ç»ƒå®Œæˆ")
                
                # è®­ç»ƒè¾ƒæ…¢ï¼Œæ¯30åˆ†é’Ÿè®­ç»ƒä¸€æ¬¡
                time.sleep(1800)
                
            except Exception as e:
                print(f"è®­ç»ƒçº¿ç¨‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿåç»§ç»­
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        with self.model_lock:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, path)
            print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {path}")
    
    def log_performance(self, actual_action, predicted_action):
        """è®°å½•æ¨¡å‹æ€§èƒ½"""
        with self.model_lock:
            if actual_action == predicted_action:
                self.performance_log['correct'] += 1
            self.performance_log['total'] += 1
            
            # æ‰“å°æ€§èƒ½æ‘˜è¦
            if self.performance_log['total'] > 0:
                acc = self.performance_log['correct'] / self.performance_log['total']
                print(f"ğŸ“Š æ¨¡å‹æ€§èƒ½ - å‡†ç¡®ç‡: {acc:.3f}")


def main():
    parser = argparse.ArgumentParser(description='Large Transformer Trading Strategy')
    parser.add_argument('--mode', choices=['train', 'predict'], default='predict',
                        help='è¿è¡Œæ¨¡å¼: train(ä»…è®­ç»ƒ), predict(ä»…é¢„æµ‹)')
    parser.add_argument('--model_path', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜æˆ–åŠ è½½è·¯å¾„')
    args = parser.parse_args()
    
    strategy = LargeTransformerStrategy(model_path=args.model_path)
    
    if args.mode == 'train':
        print("ä»…è¿è¡Œè®­ç»ƒæ¨¡å¼...")
        # è®­ç»ƒæ¨¡å¼ï¼Œä¸é€€å‡º
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("è®­ç»ƒå·²åœæ­¢")
    elif args.mode == 'predict':
        print("è¿è¡Œé¢„æµ‹æ¨¡å¼...")
        # æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
        sample_data = {
            'price_current': 93.5,
            'grid_lower': 93.0,
            'grid_upper': 94.0,
            'atr': 0.2,
            'rsi_1m': 30.0,
            'rsi_5m': 40.0,
            'buffer': 0.05,
            'threshold': 93.05,
            'near_lower': True,
            'rsi_ok': True
        }
        
        action, confidence = strategy.predict_action(sample_data)
        action_map = {0: "ä¸æ“ä½œ", 1: "ä¹°å…¥", 2: "å–å‡º"}
        
        print(f"ğŸ§  Transformeré¢„æµ‹: {action_map[action]}, ç½®ä¿¡åº¦: {confidence:.3f}")
        
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("ç¨‹åºå·²åœæ­¢")
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜æ¨¡å‹
    if args.model_path:
        strategy.save_model(args.model_path)


if __name__ == "__main__":
    main()