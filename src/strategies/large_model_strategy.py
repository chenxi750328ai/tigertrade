import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd
import os
from datetime import datetime
import threading
import time
import glob
import argparse
from typing import Tuple, Optional
import warnings
warnings.filterwarnings("ignore")


class LargeTradingNetwork(nn.Module):
    """å¤§å‹äº¤æ˜“ç½‘ç»œ - ä½¿ç”¨æ›´å¤šå‚æ•°å’Œæ›´æ·±çš„ç½‘ç»œç»“æ„"""
    def __init__(self, input_size=12, hidden_size=256, num_layers=4, output_size=3, dropout_rate=0.1):
        super(LargeTradingNetwork, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        # å¤šå±‚LSTMï¼Œæ›´å®½æ›´æ·±
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=dropout_rate if num_layers > 1 else 0
        )
        
        # å¤šå±‚å…¨è¿æ¥å±‚ï¼Œå¢åŠ éçº¿æ€§ï¼Œæ˜ç¡®è®¾ç½®inplace=Falseé¿å…inplaceæ“ä½œé—®é¢˜
        self.fc_layers = nn.ModuleList([
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(inplace=False),  # æ˜ç¡®è®¾ç½®inplace=False
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(inplace=False),  # æ˜ç¡®è®¾ç½®inplace=False
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(inplace=False),  # æ˜ç¡®è®¾ç½®inplace=False
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_size, output_size)
        ])
        
        # ç§»é™¤softmaxå±‚ï¼Œå› ä¸ºCrossEntropyLosså†…éƒ¨å¤„ç†
        # self.softmax = nn.Softmax(dim=1)
        
        # å‚æ•°åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)
    
    def forward(self, x):
        # ç¡®ä¿è¾“å…¥ç»´åº¦æ­£ç¡®
        if len(x.shape) == 2:
            x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦ (batch, seq, features)
        
        batch_size = x.size(0)
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = out[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        
        # é€šè¿‡å¤šå±‚å…¨è¿æ¥
        for layer in self.fc_layers:
            out = layer(out)
        
        return out


class LargeModelStrategy:
    """å¤§æ¨¡å‹äº¤æ˜“ç­–ç•¥"""
    def __init__(self, data_dir='/home/cx/trading_data', model_path=None):
        # å¼ºåˆ¶ä½¿ç”¨GPU
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
            print(f"Using GPU: {torch.cuda.get_device_name()}")
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"GPU Memory: {gpu_memory:.2f} GB")
        else:
            raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ­¤ç­–ç•¥éœ€è¦GPUè¿è¡Œ")
        
        self.data_dir = data_dir
        
        # åˆå§‹åŒ–å¤§å‹æ¨¡å‹
        self.model = LargeTradingNetwork(
            input_size=12,
            hidden_size=256,  # æ›´å®½çš„éšè—å±‚
            num_layers=4,     # æ›´æ·±çš„å±‚æ•°
            dropout_rate=0.1
        ).to(self.device)
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"Large Modelå‚æ•°æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°å°†åœ¨è®­ç»ƒæ—¶åŠ¨æ€è®¾ç½®
        self.optimizer = None
        self.criterion = None
        
        # æ§åˆ¶è®­ç»ƒå’Œæ¨ç†çš„æ ‡å¿—
        self.should_train = True
        self.model_lock = threading.Lock()
        
        # å¦‚æœæä¾›äº†æ¨¡å‹è·¯å¾„ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if model_path and os.path.exists(model_path):
            try:
                checkpoint = torch.load(model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                # åªæœ‰å½“ä¼˜åŒ–å™¨å­˜åœ¨æ—¶æ‰åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
                if 'optimizer_state_dict' in checkpoint and self.optimizer is not None:
                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print(f"âœ… ä» {model_path} åŠ è½½æ¨¡å‹æˆåŠŸ")
            except Exception as e:
                print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}ï¼Œä½¿ç”¨åˆå§‹æ¨¡å‹")
        
        # å¯åŠ¨è®­ç»ƒçº¿ç¨‹
        self.training_thread = threading.Thread(target=self.train_continuously, daemon=True)
        self.training_thread.start()
    
    def calculate_class_weights(self, y):
        """è®¡ç®—ç±»åˆ«æƒé‡ä»¥å¤„ç†ä¸å¹³è¡¡æ•°æ®"""
        import numpy as np
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        classes, counts = np.unique(y, return_counts=True)
        total_samples = len(y)
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æƒé‡ (æ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°))
        weights = []
        for count in counts:
            weight = total_samples / (len(classes) * count)
            weights.append(weight)
        
        # è½¬æ¢ä¸ºtensor
        class_weights = torch.FloatTensor(weights).to(self.device)
        return class_weights

    def train_model(self, df):
        """è®­ç»ƒå¤§å‹æ¨¡å‹"""
        try:
            # åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰
            if self.optimizer is None:
                self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=0.001, weight_decay=1e-5)
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨åŸºäºç›ˆåˆ©çš„æ ‡ç­¾
            X, y = [], []
            look_ahead = 10  # å‘å‰çœ‹10ä¸ªæ—¶é—´æ­¥é•¿æ¥è®¡ç®—ç›ˆåˆ©
            
            for i in range(len(df) - look_ahead):
                row = df.iloc[i]
                features = self.prepare_features(row)
                X.append(features)
                
                # è®¡ç®—æœªæ¥look_aheadæ­¥çš„ç›ˆåˆ©
                current_price = row['price_current']
                future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
                
                if len(future_prices) == 0:
                    continue
                    
                # è®¡ç®—æœ€å¤§ç›ˆåˆ©å’Œæœ€å¤§äºæŸ
                max_future_price = max(future_prices)
                min_future_price = min(future_prices)
                
                buy_profit = (max_future_price - current_price) / current_price
                sell_profit = (current_price - min_future_price) / current_price
                
                # åˆ›å»ºæ ‡ç­¾: 0=ä¸æ“ä½œ, 1=ä¹°å…¥, 2=å–å‡º
                # åªæœ‰å½“é¢„æœŸç›ˆåˆ©è¶…è¿‡é˜ˆå€¼æ—¶æ‰å»ºè®®æ“ä½œ
                profit_threshold = 0.005  # æé«˜é˜ˆå€¼åˆ°0.5%ï¼Œå‡å°‘äº¤æ˜“é¢‘ç‡ä½†æé«˜è´¨é‡
                min_diff = 0.003  # æœ€å°å·®å€¼ï¼Œç¡®ä¿ä¹°å–ä¹‹é—´æœ‰è¶³å¤Ÿå·®è·
                
                # åªæœ‰å½“ä¹°å–ç›ˆåˆ©å·®å€¼è¶…è¿‡æœ€å°å·®å€¼ä¸”è¶…è¿‡é˜ˆå€¼æ—¶æ‰äº¤æ˜“
                if abs(buy_profit - sell_profit) >= min_diff:
                    if buy_profit > sell_profit and buy_profit > profit_threshold:
                        label = 1  # ä¹°å…¥
                    elif sell_profit > buy_profit and sell_profit > profit_threshold:
                        label = 2  # å–å‡º
                    else:
                        label = 0  # ä¸æ“ä½œ
                else:
                    label = 0  # ä¸æ“ä½œ - å·®å€¼å¤ªå°ï¼Œä¸ç¡®å®šæ€§é«˜
                    
                y.append(label)
            
            if len(X) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return
            
            # åˆ†ææ ‡ç­¾åˆ†å¸ƒ
            unique, counts = np.unique(y, return_counts=True)
            label_distribution = dict(zip(unique, counts))
            print(f"æ ‡ç­¾åˆ†å¸ƒ: {label_distribution}")
            
            # å¦‚æœæŸä¸ªç±»åˆ«å æ¯”è¿‡é«˜ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®ä¸å¹³è¡¡é—®é¢˜
            total_samples = len(y)
            for label, count in label_distribution.items():
                percentage = count / total_samples * 100
                print(f"æ ‡ç­¾ {label} å æ¯”: {percentage:.2f}%")
            
            # è®¡ç®—ç±»åˆ«æƒé‡
            class_weights = self.calculate_class_weights(y)
            print(f"ç±»åˆ«æƒé‡: {class_weights}")
            
            # æ›´æ–°æŸå¤±å‡½æ•°
            self.criterion = nn.CrossEntropyLoss(weight=class_weights)
            
            # åˆ†å‰²è®­ç»ƒé›†å’ŒéªŒè¯é›† (80% è®­ç»ƒ, 20% éªŒè¯)
            split_idx = int(len(X) * 0.8)
            
            X_train = X[:split_idx]
            y_train = y[:split_idx]
            X_val = X[split_idx:]
            y_val = y[split_idx:]
            
            X_train = np.array(X_train)
            y_train = np.array(y_train)
            X_val = np.array(X_val)
            y_val = np.array(y_val)
            
            # è½¬æ¢ä¸ºå¼ é‡
            X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(self.device)
            y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(self.device)
            X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(self.device)
            y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(self.device)
            
            # åˆ›å»ºæ•°æ®é›†
            train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
            val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
            train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
            val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
            
            # è®­ç»ƒæ¨¡å‹
            self.model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            best_val_acc = 0.0
            
            for epoch in range(20):  # å¢åŠ åˆ°20è½®è®­ç»ƒ
                # è®­ç»ƒé˜¶æ®µ
                total_loss = 0
                num_batches = 0
                correct_predictions = 0
                total_predictions = 0
                
                for batch_x, batch_y in train_loader:
                    # åˆ›å»ºæ–°çš„å¼ é‡å‰¯æœ¬ä»¥é¿å…ç‰ˆæœ¬å†²çª
                    batch_x = batch_x.clone().detach().to(self.device)
                    batch_y = batch_y.clone().detach().to(self.device)
                    
                    # ç¡®ä¿æ¯æ¬¡å¤„ç†æ‰¹æ¬¡æ—¶éƒ½æ·»åŠ ç»´åº¦ä»¥åŒ¹é…æ¨¡å‹è¾“å…¥
                    batch_x = batch_x.unsqueeze(1).contiguous()  # æ·»åŠ åºåˆ—ç»´åº¦ (batch, seq, features)
                    
                    self.optimizer.zero_grad()
                    outputs = self.model(batch_x)
                    loss = self.criterion(outputs, batch_y)
                    loss.backward()  # è¿™é‡Œå¯èƒ½å‡ºç°Noneå¼ é‡é”™è¯¯
                    self.optimizer.step()
                    
                    # è®¡ç®—å‡†ç¡®ç‡
                    predictions = torch.argmax(outputs, dim=1)
                    correct_predictions += (predictions == batch_y).sum().item()
                    total_predictions += batch_y.size(0)
                    
                    total_loss += loss.item()
                    num_batches += 1
                
                train_avg_loss = total_loss / num_batches
                train_accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
                
                # éªŒè¯é˜¶æ®µ
                self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
                val_correct = 0
                val_total = 0
                val_loss = 0
                with torch.no_grad():
                    for batch_x, batch_y in val_loader:
                        # åˆ›å»ºæ–°çš„å¼ é‡å‰¯æœ¬ä»¥é¿å…ç‰ˆæœ¬å†²çª
                        batch_x = batch_x.clone().detach().to(self.device)
                        batch_y = batch_y.clone().detach().to(self.device)
                        
                        # ç¡®ä¿éªŒè¯æ‰¹æ¬¡ä¹Ÿæ·»åŠ ç»´åº¦
                        batch_x = batch_x.unsqueeze(1).contiguous()  # æ·»åŠ åºåˆ—ç»´åº¦ (batch, seq, features)
                        
                        outputs = self.model(batch_x)
                        loss = self.criterion(outputs, batch_y)
                        val_loss += loss.item()
                        
                        predictions = torch.argmax(outputs, dim=1)
                        val_correct += (predictions == batch_y).sum().item()
                        val_total += batch_y.size(0)
                
                val_avg_loss = val_loss / len(val_loader)
                val_accuracy = val_correct / val_total if val_total > 0 else 0
                
                print(f"å¤§æ¨¡å‹è®­ç»ƒè½®æ¬¡ {epoch+1}/20")
                print(f"  è®­ç»ƒ - æŸå¤±: {train_avg_loss:.4f}, å‡†ç¡®ç‡: {train_accuracy:.3f}")
                print(f"  éªŒè¯ - æŸå¤±: {val_avg_loss:.4f}, å‡†ç¡®ç‡: {val_accuracy:.3f}")
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                if val_accuracy > best_val_acc:
                    best_val_acc = val_accuracy
                    print(f"  ğŸ† å¤§æ¨¡å‹æ–°æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.3f}")
                
                self.model.train()  # é‡æ–°è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ä»¥è¿›è¡Œä¸‹ä¸€è½®è®­ç»ƒ
        
        except Exception as e:
            print(f"å¤§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
    
    def prepare_features(self, row):
        """ä»æ•°æ®è¡Œä¸­å‡†å¤‡ç‰¹å¾å‘é‡"""
        try:
            features = [
                row['price_current'],
                row['atr'],
                row['rsi_1m'] if pd.notna(row['rsi_1m']) else 50,
                row['rsi_5m'] if pd.notna(row['rsi_5m']) else 50,
                row.get('boll_upper', 0),
                row.get('boll_mid', 0),
                row.get('boll_lower', 0),
                row.get('boll_position', 0.5),
                row.get('price_change_1', 0),
                row.get('price_change_5', 0),
                row.get('volatility', 0),
                row.get('volume_1m', 0)
            ]
            # å½’ä¸€åŒ–ç‰¹å¾
            features_np = np.array(features)
            mean_val = np.mean(features_np)
            std_val = np.std(features_np) + 1e-8
            normalized_features = (features_np - mean_val) / std_val
            return normalized_features.tolist()
        except Exception as e:
            print(f"prepare_featuresé”™è¯¯: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾å€¼
            return [0.0] * 12
    
    
    
    def predict_action(self, current_data):
        """é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
        with self.model_lock:
            try:
                # å‡†å¤‡è¾“å…¥æ•°æ®
                features = self.prepare_features(current_data)
                input_tensor = torch.tensor([features], dtype=torch.float32).unsqueeze(1).to(self.device)
                
                # é¢„æµ‹
                with torch.no_grad():
                    self.model.eval()
                    prediction = self.model(input_tensor)
                    probabilities = prediction.cpu().numpy()[0]
                    
                    # è¿”å›æœ€å¯èƒ½çš„åŠ¨ä½œ: 0=ä¸æ“ä½œ, 1=ä¹°å…¥, 2=å–å‡º
                    action = np.argmax(probabilities)
                    confidence = probabilities[action]
                    
                    return int(action), float(confidence)
            except Exception as e:
                print(f"é¢„æµ‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                return 0, 0.0  # é»˜è®¤è¿”å›ä¸æ“ä½œï¼Œç½®ä¿¡åº¦0
    
    def load_training_data(self):
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        # è·å–æœ€æ–°çš„æ•°æ®æ–‡ä»¶
        all_data_files = []
        
        # è·å–æ‰€æœ‰æ•°æ®ç›®å½•
        all_data_dirs = glob.glob(os.path.join(self.data_dir, '202*-*-*'))
        if all_data_dirs:
            # æŒ‰æ—¥æœŸæ’åºï¼Œè·å–æœ€æ–°çš„å‡ ä¸ªæ–‡ä»¶
            sorted_dirs = sorted(all_data_dirs, reverse=True)
            for data_dir in sorted_dirs[:7]:  # ä½¿ç”¨æœ€è¿‘7å¤©çš„æ•°æ®
                # åŒ…å«åŸå§‹æ•°æ®å’Œæ‰©å±•æ•°æ®
                data_files = glob.glob(os.path.join(data_dir, 'trading_data_*.csv'))
                data_files.extend(glob.glob(os.path.join(data_dir, 'extended_trading_data_*.csv')))
                data_files.extend(glob.glob(os.path.join(data_dir, 'prepared_features_*.csv')))  # ä¹ŸåŒ…å«å‡†å¤‡å¥½çš„ç‰¹å¾æ•°æ®
                all_data_files.extend(data_files)
        
        if not all_data_files:
            return None
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        data_file = sorted(all_data_files, key=os.path.getmtime, reverse=True)[0]
        
        try:
            df = pd.read_csv(data_file)
            # æ¸…ç†æ•°æ®
            df = df.dropna(subset=['price_current', 'grid_lower', 'grid_upper', 'atr', 'rsi_1m', 'rsi_5m'])
            
            if len(df) < 10:  # éœ€è¦è‡³å°‘10ä¸ªæ ·æœ¬è¿›è¡Œè®­ç»ƒ
                return None
                
            return df
        except Exception as e:
            print(f"åŠ è½½è®­ç»ƒæ•°æ®é”™è¯¯: {e}")
            return None
    
    def train_continuously(self):
        """è¿ç»­è®­ç»ƒå¤§æ¨¡å‹çš„åå°çº¿ç¨‹"""
        while self.should_train:
            try:
                # åŠ è½½æ•°æ®
                df = self.load_training_data()
                if df is not None and len(df) > 0:
                    print(f"å¼€å§‹è®­ç»ƒå¤§æ¨¡å‹ï¼Œæ•°æ®é‡: {len(df)}")
                    with self.model_lock:
                        self.train_model(df)
                    print("å¤§æ¨¡å‹è®­ç»ƒå®Œæˆ")
                
                # è®­ç»ƒè¾ƒæ…¢ï¼Œæ¯30åˆ†é’Ÿè®­ç»ƒä¸€æ¬¡
                time.sleep(1800)
                
            except Exception as e:
                print(f"å¤§æ¨¡å‹è®­ç»ƒçº¿ç¨‹é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(60)  # å‡ºé”™åç­‰å¾…1åˆ†é’Ÿåç»§ç»­
    
    def save_model(self, path):
        """ä¿å­˜æ¨¡å‹"""
        with self.model_lock:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, path)
            print(f"å¤§æ¨¡å‹å·²ä¿å­˜åˆ° {path}")


def main():
    parser = argparse.ArgumentParser(description='Large Model Trading Strategy')
    parser.add_argument('--mode', choices=['train', 'predict'], default='predict',
                        help='è¿è¡Œæ¨¡å¼: train(ä»…è®­ç»ƒ), predict(ä»…é¢„æµ‹)')
    parser.add_argument('--model_path', type=str, default=None,
                        help='æ¨¡å‹ä¿å­˜æˆ–åŠ è½½è·¯å¾„')
    args = parser.parse_args()
    
    strategy = LargeModelStrategy(model_path=args.model_path)
    
    if args.mode == 'train':
        print("ä»…è¿è¡Œå¤§æ¨¡å‹è®­ç»ƒæ¨¡å¼...")
        # è®­ç»ƒæ¨¡å¼ï¼Œä¸é€€å‡º
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("å¤§æ¨¡å‹è®­ç»ƒå·²åœæ­¢")
    elif args.mode == 'predict':
        print("è¿è¡Œå¤§æ¨¡å‹é¢„æµ‹æ¨¡å¼...")
        # æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨
        sample_data = {
            'price_current': 93.5,
            'grid_lower': 93.0,
            'grid_upper': 94.0,
            'atr': 0.2,
            'rsi_1m': 30.0,
            'rsi_5m': 40.0,
            'buffer': 0.05,
            'threshold': 93.05,
            'near_lower': True,
            'rsi_ok': True
        }
        
        action, confidence = strategy.predict_action(sample_data)
        action_map = {0: "ä¸æ“ä½œ", 1: "ä¹°å…¥", 2: "å–å‡º"}
        
        print(f"ğŸ§  å¤§æ¨¡å‹é¢„æµ‹: {action_map[action]}, ç½®ä¿¡åº¦: {confidence:.3f}")
        
        try:
            while True:
                time.sleep(60)
        except KeyboardInterrupt:
            print("å¤§æ¨¡å‹é¢„æµ‹å·²åœæ­¢")
    
    # å¦‚æœæä¾›äº†ä¿å­˜è·¯å¾„ï¼Œä¿å­˜æ¨¡å‹
    if args.model_path:
        strategy.save_model(args.model_path)


if __name__ == "__main__":
    main()