#!/usr/bin/env python3
"""
ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒ - ä½¿ç”¨æ ‡å‡†çš„åˆ†ç±»ç½‘ç»œ
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from train_with_detailed_logging import TradingDataset
from config import TrainingConfig

def create_simple_logger(log_file):
    """åˆ›å»ºç®€å•æ—¥å¿—è®°å½•å™¨"""
    class SimpleLogger:
        def __init__(self, log_file):
            self.log_file = log_file
            
        def log(self, message):
            timestamp = datetime.now().strftime('[%Y-%m-%d %H:%M:%S.%f')[:-3] + ']'
            log_msg = f"{timestamp} [INFO] {message}"
            print(log_msg)
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_msg + '\n')
    
    return SimpleLogger(log_file)


class SimpleRLNetwork(nn.Module):
    """ç®€åŒ–çš„å¼ºåŒ–å­¦ä¹ ç½‘ç»œ - ç”¨äºç›‘ç£å­¦ä¹ """
    def __init__(self, input_size=12, num_classes=3):
        super(SimpleRLNetwork, self).__init__()
        
        self.network = nn.Sequential(
            nn.Linear(input_size, 256),
            nn.ReLU(),
            nn.BatchNorm1d(256),
            nn.Dropout(0.3),
            
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.2),
            
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            
            nn.Linear(64, num_classes)
        )
    
    def forward(self, x):
        return self.network(x)


def train_simple_rl(data_file, output_dir, logger):
    """ç®€åŒ–çš„RLè®­ç»ƒ"""
    
    logger.log("=" * 80)
    logger.log("ğŸš€ å¼ºåŒ–å­¦ä¹ ç­–ç•¥ - ç®€åŒ–ç›‘ç£å­¦ä¹ æ¨¡å¼")
    logger.log("=" * 80)
    
    # åŠ è½½æ•°æ®ï¼ˆä½¿ç”¨å·²ç»åˆ†å±‚åˆ†å‰²å¥½çš„æ•°æ®ï¼‰
    data_dir = os.path.dirname(data_file)
    # ä» full_20260120_192018.csv æå– 20260120_192018
    basename = os.path.basename(data_file)
    timestamp = '_'.join(basename.split('_')[1:]).replace('.csv', '')
    
    train_file = os.path.join(data_dir, f'train_{timestamp}.csv')
    val_file = os.path.join(data_dir, f'val_{timestamp}.csv')
    test_file = os.path.join(data_dir, f'test_{timestamp}.csv')
    
    logger.log(f"ğŸ“Š åŠ è½½åˆ†å±‚æ•°æ®:")
    logger.log(f"  è®­ç»ƒé›†: {train_file}")
    logger.log(f"  éªŒè¯é›†: {val_file}")
    logger.log(f"  æµ‹è¯•é›†: {test_file}")
    
    train_df = pd.read_csv(train_file)
    val_df = pd.read_csv(val_file)
    test_df = pd.read_csv(test_file)
    
    # ç‰¹å¾åˆ—
    feature_cols = [
        'rsi_1m', 'rsi_5m', 'atr', 'boll_position',
        'boll_upper', 'boll_lower', 'boll_mid',
        'price_change_1', 'price_change_5',
        'volatility', 'volume_1m', 'price_current'
    ]
    
    logger.log(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
    logger.log(f"éªŒè¯é›†: {len(val_df)} æ¡")
    logger.log(f"æµ‹è¯•é›†: {len(test_df)} æ¡")
    
    # æ ‡ç­¾åˆ†å¸ƒ
    logger.log(f"è®­ç»ƒé›†æ ‡ç­¾åˆ†å¸ƒ: {dict(train_df['label'].value_counts().sort_index())}")
    logger.log(f"éªŒè¯é›†æ ‡ç­¾åˆ†å¸ƒ: {dict(val_df['label'].value_counts().sort_index())}")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TradingDataset(train_df, feature_cols)
    val_dataset = TradingDataset(val_df, feature_cols)
    test_dataset = TradingDataset(test_df, feature_cols)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.log(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = SimpleRLNetwork(input_size=len(feature_cols)).to(device)
    
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.log(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    logger.log(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5
    )
    
    # è®­ç»ƒ
    best_val_acc = 0
    patience_counter = 0
    max_patience = 10
    
    logger.log("\nå¼€å§‹è®­ç»ƒ...")
    logger.log("=" * 80)
    
    for epoch in range(1, 51):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for features, labels in train_loader:
            features = features.to(device)
            labels = labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(features)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        train_acc = train_correct / train_total if train_total > 0 else 0
        train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(device)
                labels = labels.to(device)
                
                outputs = model(features)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
        
        val_acc = val_correct / val_total if val_total > 0 else 0
        val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_acc)
        current_lr = optimizer.param_groups[0]['lr']
        
        logger.log(f"Epoch {epoch:2d}/50 | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
                  f"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | LR: {current_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            model_path = os.path.join(output_dir, 'å¼ºåŒ–å­¦ä¹ ç­–ç•¥_best.pth')
            torch.save(model.state_dict(), model_path)
            logger.log(f"  ğŸ† æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}, æ¨¡å‹å·²ä¿å­˜")
            patience_counter = 0
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= max_patience:
            logger.log(f"â¹ï¸ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
            break
    
    # æµ‹è¯•é˜¶æ®µ
    logger.log("\n" + "=" * 80)
    logger.log("ğŸ“Š æµ‹è¯•é˜¶æ®µ")
    logger.log("=" * 80)
    
    model_path = os.path.join(output_dir, 'å¼ºåŒ–å­¦ä¹ ç­–ç•¥_best.pth')
    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()
    
    test_loss = 0
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for features, labels in test_loader:
            features = features.to(device)
            labels = labels.to(device)
            
            outputs = model(features)
            loss = criterion(outputs, labels)
            
            test_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()
    
    test_acc = test_correct / test_total if test_total > 0 else 0
    test_loss = test_loss / len(test_loader) if len(test_loader) > 0 else 0
    
    logger.log(f"\nâœ… å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒå®Œæˆ!")
    logger.log(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
    logger.log(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.log(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    
    return {
        'best_val_acc': best_val_acc,
        'test_acc': test_acc,
        'test_loss': test_loss
    }

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='ç®€åŒ–å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒ')
    parser.add_argument('--data-file', type=str, required=True)
    parser.add_argument('--output-dir', type=str, required=True)
    
    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    log_file = os.path.join(args.output_dir, 'rl_simple_training.log')
    logger = create_simple_logger(log_file)
    
    result = train_simple_rl(args.data_file, args.output_dir, logger)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result['best_val_acc']:.4f}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {result['test_acc']:.4f}")
    print("=" * 80)
