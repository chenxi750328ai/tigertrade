#!/usr/bin/env python3
"""
å¼ºåŒ–å­¦ä¹ ç­–ç•¥ä¸“ç”¨è®­ç»ƒè„šæœ¬
å°†å¼ºåŒ–å­¦ä¹ é—®é¢˜è½¬æ¢ä¸ºç›‘ç£å­¦ä¹ é—®é¢˜
"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import pandas as pd
import numpy as np
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from strategies.rl_trading_strategy import RLTradingNetwork
from train_with_detailed_logging import TradingDataset
from config import TrainingConfig

def create_simple_logger(log_file):
    """åˆ›å»ºç®€å•æ—¥å¿—è®°å½•å™¨"""
    class SimpleLogger:
        def __init__(self, log_file):
            self.log_file = log_file
            
        def log(self, message):
            timestamp = datetime.now().strftime('[%Y-%m-%d %H:%M:%S.%f')[:-3] + ']'
            log_msg = f"{timestamp} [INFO] {message}"
            print(log_msg)
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(log_msg + '\n')
    
    return SimpleLogger(log_file)

def train_rl_as_supervised(data_file, output_dir, logger):
    """å°†RLç½‘ç»œç”¨äºç›‘ç£å­¦ä¹ è®­ç»ƒ"""
    
    logger.log("=" * 80)
    logger.log("ğŸš€ å¼ºåŒ–å­¦ä¹ ç­–ç•¥ - ç›‘ç£å­¦ä¹ è®­ç»ƒæ¨¡å¼")
    logger.log("=" * 80)
    
    # åŠ è½½æ•°æ®
    logger.log(f"ğŸ“Š åŠ è½½æ•°æ®: {data_file}")
    df = pd.read_csv(data_file)
    logger.log(f"æ€»æ•°æ®é‡: {len(df)} æ¡")
    
    # ç‰¹å¾åˆ—ï¼ˆä¸æ•°æ®é‡‡é›†æ—¶çš„ç‰¹å¾åç§°åŒ¹é…ï¼‰
    feature_cols = [
        'rsi_1m', 'rsi_5m', 'atr', 'boll_position',
        'boll_upper', 'boll_lower', 'boll_mid',
        'price_change_1', 'price_change_5',
        'volatility', 'volume_1m', 'price_current'
    ]
    
    # æ•°æ®åˆ†å‰²
    train_size = int(len(df) * 0.7)
    val_size = int(len(df) * 0.15)
    
    train_df = df[:train_size]
    val_df = df[train_size:train_size + val_size]
    test_df = df[train_size + val_size:]
    
    logger.log(f"è®­ç»ƒé›†: {len(train_df)} æ¡")
    logger.log(f"éªŒè¯é›†: {len(val_df)} æ¡")
    logger.log(f"æµ‹è¯•é›†: {len(test_df)} æ¡")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TradingDataset(train_df, feature_cols)
    val_dataset = TradingDataset(val_df, feature_cols)
    test_dataset = TradingDataset(test_df, feature_cols)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)
    
    # åˆå§‹åŒ–æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.log(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    model = RLTradingNetwork().to(device)
    
    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    logger.log(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
    logger.log(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='max', factor=0.5, patience=5
    )
    
    # è®­ç»ƒ
    best_val_acc = 0
    patience_counter = 0
    max_patience = 10
    
    logger.log("\nå¼€å§‹è®­ç»ƒ...")
    logger.log("=" * 80)
    
    for epoch in range(1, 51):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        for batch_idx, (features, labels) in enumerate(train_loader):
            features = features.to(device)
            labels = labels.to(device)
            
            # RLTradingNetworkéœ€è¦3Dè¾“å…¥: (batch, seq_len, features)
            if len(features.shape) == 2:
                features = features.unsqueeze(1)  # (batch, features) -> (batch, 1, features)
            
            optimizer.zero_grad()
            action_probs, _ = model(features)  # RLTradingNetworkè¿”å›(action_probs, q_values)
            loss = criterion(action_probs, labels)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            _, predicted = torch.max(action_probs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
        
        train_acc = train_correct / train_total if train_total > 0 else 0
        train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        all_predictions = []
        
        with torch.no_grad():
            for features, labels in val_loader:
                features = features.to(device)
                labels = labels.to(device)
                
                # RLTradingNetworkéœ€è¦3Dè¾“å…¥
                if len(features.shape) == 2:
                    features = features.unsqueeze(1)
                
                action_probs, _ = model(features)
                loss = criterion(action_probs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(action_probs.data, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
                all_predictions.extend(predicted.cpu().tolist())
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ£€æŸ¥é¢„æµ‹åˆ†å¸ƒ
        if epoch == 1:
            from collections import Counter
            pred_dist = Counter(all_predictions)
            logger.log(f"  éªŒè¯é›†é¢„æµ‹åˆ†å¸ƒ: {dict(pred_dist)}")
        
        val_acc = val_correct / val_total if val_total > 0 else 0
        val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_acc)
        current_lr = optimizer.param_groups[0]['lr']
        
        logger.log(f"Epoch {epoch:2d}/50 | Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | "
                  f"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f} | LR: {current_lr:.6f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            model_path = os.path.join(output_dir, 'å¼ºåŒ–å­¦ä¹ ç­–ç•¥_best.pth')
            torch.save(model.state_dict(), model_path)
            logger.log(f"  ğŸ† æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}, æ¨¡å‹å·²ä¿å­˜")
            patience_counter = 0
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= max_patience:
            logger.log(f"â¹ï¸ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
            break
    
    # æµ‹è¯•é˜¶æ®µ
    logger.log("\n" + "=" * 80)
    logger.log("ğŸ“Š æµ‹è¯•é˜¶æ®µ")
    logger.log("=" * 80)
    
    model_path = os.path.join(output_dir, 'å¼ºåŒ–å­¦ä¹ ç­–ç•¥_best.pth')
    if not os.path.exists(model_path):
        logger.log(f"âš ï¸ æ²¡æœ‰ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆéªŒè¯å‡†ç¡®ç‡ä¸º0ï¼‰ï¼Œä½¿ç”¨æœ€åè®­ç»ƒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•")
    else:
        model.load_state_dict(torch.load(model_path, map_location=device))
    
    model.eval()
    
    test_loss = 0
    test_correct = 0
    test_total = 0
    
    with torch.no_grad():
        for features, labels in test_loader:
            features = features.to(device)
            labels = labels.to(device)
            
            # RLTradingNetworkéœ€è¦3Dè¾“å…¥
            if len(features.shape) == 2:
                features = features.unsqueeze(1)
            
            action_probs, _ = model(features)
            loss = criterion(action_probs, labels)
            
            test_loss += loss.item()
            _, predicted = torch.max(action_probs.data, 1)
            test_total += labels.size(0)
            test_correct += (predicted == labels).sum().item()
    
    test_acc = test_correct / test_total if test_total > 0 else 0
    test_loss = test_loss / len(test_loader) if len(test_loader) > 0 else 0
    
    logger.log(f"\nâœ… å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒå®Œæˆ!")
    logger.log(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
    logger.log(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    logger.log(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    
    return {
        'best_val_acc': best_val_acc,
        'test_acc': test_acc,
        'test_loss': test_loss
    }

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='å¼ºåŒ–å­¦ä¹ ç­–ç•¥è®­ç»ƒ')
    parser.add_argument('--data-file', type=str, required=True, help='è®­ç»ƒæ•°æ®CSVæ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, required=True, help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    log_file = os.path.join(args.output_dir, 'rl_training.log')
    logger = create_simple_logger(log_file)
    
    result = train_rl_as_supervised(args.data_file, args.output_dir, logger)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {result['best_val_acc']:.4f}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {result['test_acc']:.4f}")
    print("=" * 80)
