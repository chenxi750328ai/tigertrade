#!/usr/bin/env python3
"""
åŸºäºåŸç”Ÿç‰¹å¾çš„Transformeräº¤æ˜“æ¨¡å‹
ä¸ä½¿ç”¨äººä¸ºè®¾è®¡çš„æŒ‡æ ‡ï¼ˆRSI/BOLL/MACDï¼‰ï¼Œè®©æ¨¡å‹è‡ªå·±ä»åŸå§‹æ•°æ®ä¸­å­¦ä¹ 
"""

import sys
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
from datetime import datetime
import json

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)


class TradingSequenceDataset(Dataset):
    """äº¤æ˜“åºåˆ—æ•°æ®é›†"""
    
    def __init__(self, csv_file, sequence_length=128, predict_horizon=10):
        """
        Args:
            csv_file: æ•°æ®æ–‡ä»¶è·¯å¾„
            sequence_length: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆè¿‡å»Nä¸ªæ—¶é—´ç‚¹ï¼‰
            predict_horizon: é¢„æµ‹æœªæ¥Nä¸ªæ—¶é—´ç‚¹åçš„æ”¶ç›Š
        """
        self.df = pd.read_csv(csv_file)
        self.sequence_length = sequence_length
        self.predict_horizon = predict_horizon
        
        # ç‰¹å¾åˆ—ï¼ˆåŸç”Ÿç‰¹å¾ï¼‰
        self.feature_cols = [
            'open', 'high', 'low', 'close', 'volume',
            'price_change', 'price_change_pct',
            'time_delta', 'price_range', 'price_range_pct',
            'volume_change', 'volume_change_pct'
        ]
        
        # æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼‰
        self.features = self.df[self.feature_cols].values.astype(np.float32)
        
        # è®¡ç®—æœªæ¥æ”¶ç›Šï¼ˆæ ‡ç­¾ï¼‰
        self.df['future_return'] = self.df['close'].shift(-predict_horizon) / self.df['close'] - 1
        
        # ä¸‰åˆ†ç±»ï¼šä¹°å…¥(+1), æŒæœ‰(0), å–å‡º(-1)
        self.df['action'] = 0
        self.df.loc[self.df['future_return'] > 0.002, 'action'] = 1   # ä¸Šæ¶¨>0.2% â†’ ä¹°å…¥
        self.df.loc[self.df['future_return'] < -0.002, 'action'] = -1  # ä¸‹è·Œ>0.2% â†’ å–å‡º
        
        self.labels = self.df['action'].values
        
        # æœ‰æ•ˆçš„æ ·æœ¬ç´¢å¼•ï¼ˆéœ€è¦è¶³å¤Ÿçš„å†å²å’Œæœªæ¥æ•°æ®ï¼‰
        self.valid_indices = list(range(
            sequence_length, 
            len(self.df) - predict_horizon
        ))
        
        print(f"{'='*60}")
        print(f"æ•°æ®é›†: {Path(csv_file).name}")
        print(f"{'='*60}")
        print(f"æ€»æ•°æ®é‡: {len(self.df):,}æ¡")
        print(f"åºåˆ—é•¿åº¦: {sequence_length}")
        print(f"é¢„æµ‹é—´éš”: {predict_horizon}ä¸ªæ—¶é—´ç‚¹å")
        print(f"æœ‰æ•ˆæ ·æœ¬: {len(self.valid_indices):,}ä¸ª")
        
        # æ ‡ç­¾åˆ†å¸ƒ
        action_counts = pd.Series(self.labels[self.valid_indices]).value_counts()
        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        print(f"  ä¹°å…¥(+1):  {action_counts.get(1, 0):>6}ä¸ª ({action_counts.get(1, 0)/len(self.valid_indices)*100:>5.1f}%)")
        print(f"  æŒæœ‰(0):   {action_counts.get(0, 0):>6}ä¸ª ({action_counts.get(0, 0)/len(self.valid_indices)*100:>5.1f}%)")
        print(f"  å–å‡º(-1):  {action_counts.get(-1, 0):>6}ä¸ª ({action_counts.get(-1, 0)/len(self.valid_indices)*100:>5.1f}%)")
        print(f"{'='*60}\n")
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        real_idx = self.valid_indices[idx]
        
        # è¾“å…¥åºåˆ—ï¼šè¿‡å»sequence_lengthä¸ªæ—¶é—´ç‚¹çš„ç‰¹å¾
        seq_start = real_idx - self.sequence_length
        seq_end = real_idx
        
        sequence = self.features[seq_start:seq_end]  # (sequence_length, num_features)
        label = self.labels[real_idx]
        
        # è½¬ä¸ºtensor
        sequence = torch.from_numpy(sequence)
        label = torch.tensor(label, dtype=torch.long) + 1  # è½¬ä¸º 0, 1, 2ï¼ˆæ–¹ä¾¿äº¤å‰ç†µï¼‰
        
        return sequence, label


class TransformerTradingModel(nn.Module):
    """åŸºäºTransformerçš„äº¤æ˜“æ¨¡å‹"""
    
    def __init__(self, num_features, d_model=128, nhead=8, num_layers=4, dropout=0.1):
        """
        Args:
            num_features: è¾“å…¥ç‰¹å¾æ•°é‡
            d_model: Transformeréšè—ç»´åº¦
            nhead: å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
            num_layers: Transformerå±‚æ•°
            dropout: Dropoutæ¯”ä¾‹
        """
        super().__init__()
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Linear(num_features, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, dropout)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        # è¾“å‡ºå±‚
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model // 2, 3)  # ä¸‰åˆ†ç±»ï¼šå–å‡º/æŒæœ‰/ä¹°å…¥
        )
        
        print(f"{'='*60}")
        print(f"ğŸ¤– æ¨¡å‹æ¶æ„")
        print(f"{'='*60}")
        print(f"è¾“å…¥ç‰¹å¾æ•°: {num_features}")
        print(f"Transformerç»´åº¦: {d_model}")
        print(f"æ³¨æ„åŠ›å¤´æ•°: {nhead}")
        print(f"Transformerå±‚æ•°: {num_layers}")
        print(f"Dropout: {dropout}")
        print(f"è¾“å‡º: 3ç±»ï¼ˆå–å‡º/æŒæœ‰/ä¹°å…¥ï¼‰")
        print(f"{'='*60}\n")
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, sequence_length, num_features)
        Returns:
            (batch_size, 3) - ä¸‰ä¸ªç±»åˆ«çš„logits
        """
        # è¾“å…¥æŠ•å½±
        x = self.input_projection(x)  # (batch, seq_len, d_model)
        
        # ä½ç½®ç¼–ç 
        x = self.pos_encoding(x)
        
        # Transformerç¼–ç 
        x = self.transformer(x)  # (batch, seq_len, d_model)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¡¨ç¤º
        x = x[:, -1, :]  # (batch, d_model)
        
        # åˆ†ç±»
        x = self.fc(x)  # (batch, 3)
        
        return x


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç """
    
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        
        # è®¡ç®—ä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        pe = pe.unsqueeze(0)  # (1, max_len, d_model)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        """
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)


def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_idx, (sequences, labels) in enumerate(dataloader):
        sequences = sequences.to(device)
        labels = labels.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(sequences)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
        # æ¯50æ‰¹æ¬¡æ‰“å°ä¸€æ¬¡
        if (batch_idx + 1) % 50 == 0:
            print(f"  Batch {batch_idx+1}/{len(dataloader)}: Loss={loss.item():.4f}, Acc={correct/total*100:.2f}%")
    
    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total
    
    return avg_loss, accuracy


def validate(model, dataloader, criterion, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    all_preds = []
    all_labels = []
    
    with torch.no_grad():
        for sequences, labels in dataloader:
            sequences = sequences.to(device)
            labels = labels.to(device)
            
            outputs = model(sequences)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
            
            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    avg_loss = total_loss / len(dataloader)
    accuracy = correct / total
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    
    class_acc = {}
    for cls in [0, 1, 2]:
        mask = all_labels == cls
        if mask.sum() > 0:
            class_acc[cls] = (all_preds[mask] == all_labels[mask]).mean()
    
    return avg_loss, accuracy, class_acc


def main():
    print("\n" + "="*80)
    print("ğŸš€ åŸºäºåŸç”Ÿç‰¹å¾çš„Transformeräº¤æ˜“æ¨¡å‹è®­ç»ƒ")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ ¸å¿ƒç†å¿µ: ä¸ä½¿ç”¨RSI/BOLL/MACDç­‰äººä¸ºæŒ‡æ ‡ï¼Œè®©æ¨¡å‹è‡ªå·±å­¦ä¹ ")
    print("="*80 + "\n")
    
    # é…ç½®
    config = {
        'train_file': '/home/cx/trading_data/train_raw_features.csv',
        'val_file': '/home/cx/trading_data/val_raw_features.csv',
        'sequence_length': 128,      # ç”¨è¿‡å»128ä¸ªæ—¶é—´ç‚¹
        'predict_horizon': 10,       # é¢„æµ‹10ä¸ªæ—¶é—´ç‚¹åçš„èµ°åŠ¿
        'd_model': 128,              # Transformerç»´åº¦
        'nhead': 8,                  # æ³¨æ„åŠ›å¤´æ•°
        'num_layers': 4,             # Transformerå±‚æ•°
        'dropout': 0.2,              # Dropout
        'batch_size': 64,
        'learning_rate': 0.0001,
        'num_epochs': 50,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu'
    }
    
    print(f"é…ç½®:")
    for key, value in config.items():
        if not key.endswith('_file'):
            print(f"  {key}: {value}")
    print()
    
    # åŠ è½½æ•°æ®
    train_dataset = TradingSequenceDataset(
        config['train_file'],
        sequence_length=config['sequence_length'],
        predict_horizon=config['predict_horizon']
    )
    
    val_dataset = TradingSequenceDataset(
        config['val_file'],
        sequence_length=config['sequence_length'],
        predict_horizon=config['predict_horizon']
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=2
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=2
    )
    
    # åˆ›å»ºæ¨¡å‹
    num_features = len(train_dataset.feature_cols)
    model = TransformerTradingModel(
        num_features=num_features,
        d_model=config['d_model'],
        nhead=config['nhead'],
        num_layers=config['num_layers'],
        dropout=config['dropout']
    )
    
    model = model.to(config['device'])
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # è®­ç»ƒ
    print(f"\n{'='*80}")
    print(f"å¼€å§‹è®­ç»ƒ")
    print(f"{'='*80}\n")
    
    best_val_acc = 0
    best_epoch = 0
    training_history = []
    
    for epoch in range(config['num_epochs']):
        print(f"Epoch {epoch+1}/{config['num_epochs']}")
        print("-" * 60)
        
        # è®­ç»ƒ
        train_loss, train_acc = train_epoch(
            model, train_loader, criterion, optimizer, config['device']
        )
        
        # éªŒè¯
        val_loss, val_acc, class_acc = validate(
            model, val_loader, criterion, config['device']
        )
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(val_loss)
        
        # æ‰“å°ç»“æœ
        print(f"\nç»“æœ:")
        print(f"  è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc*100:.2f}%")
        print(f"  éªŒè¯ - Loss: {val_loss:.4f}, Acc: {val_acc*100:.2f}%")
        print(f"  å„ç±»åˆ«å‡†ç¡®ç‡:")
        print(f"    å–å‡º(-1): {class_acc.get(0, 0)*100:.2f}%")
        print(f"    æŒæœ‰(0):  {class_acc.get(1, 0)*100:.2f}%")
        print(f"    ä¹°å…¥(+1): {class_acc.get(2, 0)*100:.2f}%")
        print()
        
        # è®°å½•å†å²
        training_history.append({
            'epoch': epoch + 1,
            'train_loss': train_loss,
            'train_acc': train_acc,
            'val_loss': val_loss,
            'val_acc': val_acc,
            'class_acc': class_acc
        })
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            best_epoch = epoch + 1
            
            save_dir = Path('/home/cx/tigertrade/models')
            save_dir.mkdir(exist_ok=True)
            
            model_path = save_dir / 'transformer_raw_features_best.pth'
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_acc': val_acc,
                'config': config
            }, model_path)
            
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path} (éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.2f}%)")
        
        print("="*80 + "\n")
    
    # è®­ç»ƒå®Œæˆ
    print(f"\n{'='*80}")
    print(f"âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*80}")
    print(f"æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc*100:.2f}% (Epoch {best_epoch})")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_file = '/home/cx/tigertrade/results/transformer_raw_features_history.json'
    Path(history_file).parent.mkdir(exist_ok=True)
    
    with open(history_file, 'w') as f:
        json.dump(training_history, f, indent=2)
    
    print(f"\nè®­ç»ƒå†å²å·²ä¿å­˜: {history_file}")
    print(f"{'='*80}\n")


if __name__ == '__main__':
    main()
