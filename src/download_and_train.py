#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æœŸè´§æ•°æ®ä¸‹è½½å’Œæ¨¡å‹è®­ç»ƒç³»ç»Ÿ
æ”¯æŒä»»æ„æ ‡çš„ï¼šæ•°æ®é‡‡é›† + æ¨¡å‹è®­ç»ƒ + æµ‹è¯•è¯„ä¼°
"""

import sys
import os
import time
import argparse
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, '/home/cx/tigertrade')

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é€šç”¨æœŸè´§æ•°æ®ä¸‹è½½å’Œæ¨¡å‹è®­ç»ƒç³»ç»Ÿ')
    parser.add_argument('--symbol', type=str, default=None, 
                        help='æœŸè´§æ ‡çš„ä»£ç ï¼ˆå¦‚SIL2603, GC2603ç­‰ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„FUTURE_SYMBOL')
    parser.add_argument('--real-api', action='store_true', help='ä½¿ç”¨çœŸå®APIè·å–æ•°æ®')
    parser.add_argument('--days', type=int, default=60, help='è·å–å¤©æ•°ï¼ˆé»˜è®¤60å¤©ï¼‰')
    parser.add_argument('--min-records', type=int, default=20000, help='æœ€å°‘è®°å½•æ•°ï¼ˆé»˜è®¤20000ï¼‰')
    parser.add_argument('--max-records', type=int, default=50000, help='æœ€å¤§è®°å½•æ•°ï¼ˆé»˜è®¤50000ï¼‰')
    parser.add_argument('--output-dir', type=str, default=None, 
                        help='è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤æ ¹æ®æ ‡çš„è‡ªåŠ¨ç”Ÿæˆï¼‰')
    
    args = parser.parse_args()
    
    # è·å–æ ‡çš„ä»£ç 
    if args.symbol:
        symbol = args.symbol
        # ä¸´æ—¶è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['TRADING_SYMBOL'] = symbol
    else:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤æ ‡çš„
        from src import tiger1
        symbol = tiger1.FUTURE_SYMBOL
    
    # è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºç›®å½•ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
    if not args.output_dir:
        # ä»å®Œæ•´æ ‡çš„æå–ç®€çŸ­ä»£ç ï¼ˆå¦‚SIL.COMEX.202603 -> SIL2603ï¼‰
        symbol_short = symbol.replace('.', '_').replace('COMEX', '').replace('__', '_')
        args.output_dir = f'/home/cx/trading_data/{symbol_short}_dataset'
    
    print("\n" + "=" * 80)
    print("ğŸš€ é€šç”¨æœŸè´§æ•°æ®ä¸‹è½½å’Œæ¨¡å‹è®­ç»ƒå®Œæ•´æµç¨‹")
    print("=" * 80)
    print(f"æ ‡çš„ä»£ç : {symbol}")
    print(f"ä½¿ç”¨çœŸå®API: {args.real_api}")
    print(f"ç›®æ ‡å¤©æ•°: {args.days}")
    print(f"æœ€å°‘è®°å½•æ•°: {args.min_records}")
    print(f"æœ€å¤§è®°å½•æ•°: {args.max_records}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print("=" * 80)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # é˜¶æ®µ1: æ•°æ®é‡‡é›†
    print("\n" + "=" * 80)
    print("ğŸ“¥ é˜¶æ®µ1: æ•°æ®é‡‡é›†")
    print("=" * 80)
    
    from collect_large_dataset import LargeDatasetCollector
    
    collector = LargeDatasetCollector(
        use_real_api=args.real_api,
        days=args.days,
        max_records=args.max_records
    )
    
    # æ›´æ–°è¾“å‡ºç›®å½•
    collector.output_dir = args.output_dir
    
    # è¿è¡Œæ•°æ®é‡‡é›†
    files = collector.run()
    
    if not files or 'train' not in files or 'val' not in files or 'test' not in files:
        print("\nâŒ æ•°æ®é‡‡é›†å¤±è´¥ï¼")
        return 1
    
    # æ£€æŸ¥æ•°æ®é‡æ˜¯å¦æ»¡è¶³è¦æ±‚
    import pandas as pd
    train_df = pd.read_csv(files['train'])
    val_df = pd.read_csv(files['val'])
    test_df = pd.read_csv(files['test'])
    
    total_records = len(train_df) + len(val_df) + len(test_df)
    
    print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    print(f"  è®­ç»ƒé›†: {len(train_df)} æ¡")
    print(f"  éªŒè¯é›†: {len(val_df)} æ¡")
    print(f"  æµ‹è¯•é›†: {len(test_df)} æ¡")
    print(f"  æ€»è®¡: {total_records} æ¡")
    
    if total_records < args.min_records:
        print(f"\nâš ï¸ æ•°æ®é‡ä¸è¶³ï¼éœ€è¦è‡³å°‘ {args.min_records} æ¡ï¼Œå®é™…åªæœ‰ {total_records} æ¡")
        print("   å»ºè®®ï¼šå¢åŠ  --days å‚æ•°æˆ–ä½¿ç”¨ --real-api è·å–çœŸå®æ•°æ®")
        return 1
    
    print(f"\nâœ… æ•°æ®é‡æ»¡è¶³è¦æ±‚ï¼")
    
    # é˜¶æ®µ2: æ¨¡å‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("ğŸ¤– é˜¶æ®µ2: è®­ç»ƒæ‰€æœ‰æ¨¡å‹")
    print("=" * 80)
    
    # å¯¼å…¥è®­ç»ƒæ¨¡å—
    from train_all_models import get_all_models, train_single_model
    from train_with_detailed_logging import DetailedLogger, TradingDataset
    from config import TrainingConfig, FeatureConfig
    import torch
    from torch.utils.data import DataLoader
    
    # åˆ›å»ºæ—¥å¿—
    log_dir = os.path.join(args.output_dir, 'training_logs')
    os.makedirs(log_dir, exist_ok=True)
    logger = DetailedLogger(log_dir)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() and TrainingConfig.DEVICE == 'cuda' else 'cpu')
    logger.log(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # å‡†å¤‡æ•°æ®é›†
    feature_cols = FeatureConfig.get_all_features()
    train_dataset = TradingDataset(train_df, feature_cols)
    val_dataset = TradingDataset(val_df, feature_cols)
    test_dataset = TradingDataset(test_df, feature_cols)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=TrainingConfig.BATCH_SIZE,
        shuffle=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=TrainingConfig.BATCH_SIZE,
        shuffle=False
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=TrainingConfig.BATCH_SIZE,
        shuffle=False
    )
    
    # è·å–æ‰€æœ‰æ¨¡å‹
    all_models = get_all_models()
    logger.log(f"\næ‰¾åˆ° {len(all_models)} ä¸ªæ¨¡å‹:")
    for name in all_models.keys():
        logger.log(f"  - {name}")
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    all_results = []
    model_dir = os.path.join(args.output_dir, 'models')
    os.makedirs(model_dir, exist_ok=True)
    
    for model_name, model_class in all_models.items():
        result = train_single_model(
            model_name, 
            model_class, 
            train_loader, 
            val_loader, 
            device, 
            logger,
            model_dir
        )
        
        if result:
            all_results.append(result)
    
    # é˜¶æ®µ3: æ¨¡å‹æµ‹è¯•è¯„ä¼°
    print("\n" + "=" * 80)
    print("ğŸ“Š é˜¶æ®µ3: æµ‹è¯•è¯„ä¼°")
    print("=" * 80)
    
    import torch.nn as nn
    
    test_results = []
    criterion = nn.CrossEntropyLoss()
    
    for result in all_results:
        model_name = result['model_name']
        logger.log(f"\næµ‹è¯• {model_name}...")
        
        try:
            # åŠ è½½æœ€ä½³æ¨¡å‹
            model_path = os.path.join(model_dir, f'{model_name}_best.pth')
            if not os.path.exists(model_path):
                logger.log(f"  âš ï¸ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
                continue
            
            # é‡æ–°åˆ›å»ºæ¨¡å‹å®ä¾‹
            all_models_dict = get_all_models()
            model_class = all_models_dict[model_name]
            strategy = model_class()
            
            # è·å–å®é™…çš„æ¨¡å‹å¯¹è±¡
            if hasattr(strategy, 'model'):
                model = strategy.model
            elif hasattr(strategy, 'network'):
                model = strategy.network
            elif hasattr(strategy, 'lstm_model'):
                model = strategy.lstm_model
            elif hasattr(strategy, 'to'):
                model = strategy
            else:
                logger.log(f"  âš ï¸ {model_name} æ— æ³•è·å–æ¨¡å‹å¯¹è±¡ï¼Œè·³è¿‡æµ‹è¯•")
                continue
            
            model = model.to(device)
            model.load_state_dict(torch.load(model_path, map_location=device))
            model.eval()
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            test_loss = 0
            test_correct = 0
            test_total = 0
            
            with torch.no_grad():
                for batch_X, batch_y in test_loader:
                    batch_X = batch_X.to(device)
                    batch_y = batch_y.to(device)
                    
                    # ä¸ºTransformeræ¨¡å‹æ·»åŠ seq_lenç»´åº¦
                    if len(batch_X.shape) == 2:
                        batch_X = batch_X.unsqueeze(1)  # (batch, features) -> (batch, 1, features)
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    
                    test_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    test_total += batch_y.size(0)
                    test_correct += (predicted == batch_y).sum().item()
            
            test_acc = test_correct / test_total if test_total > 0 else 0
            test_loss = test_loss / len(test_loader) if len(test_loader) > 0 else 0
            
            logger.log(f"  âœ… æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}, æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            
            test_results.append({
                'model_name': model_name,
                'test_acc': test_acc,
                'test_loss': test_loss,
                'val_acc': result['best_val_acc']
            })
            
        except Exception as e:
            logger.log(f"  âŒ æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    import json
    
    results_file = os.path.join(args.output_dir, 'all_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'training_results': all_results,
            'test_results': test_results,
            'data_files': files,
            'config': {
                'real_api': args.real_api,
                'days': args.days,
                'total_records': total_records,
                'train_records': len(train_df),
                'val_records': len(val_df),
                'test_records': len(test_df)
            }
        }, f, indent=2, ensure_ascii=False)
    
    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“‹ æœ€ç»ˆæŠ¥å‘Š")
    print("=" * 80)
    
    # æŒ‰æµ‹è¯•å‡†ç¡®ç‡æ’åº
    test_results.sort(key=lambda x: x['test_acc'], reverse=True)
    
    print(f"\næ¨¡å‹æ’åï¼ˆæŒ‰æµ‹è¯•é›†å‡†ç¡®ç‡ï¼‰:")
    print("-" * 80)
    print(f"{'æ’å':<6} {'æ¨¡å‹åç§°':<30} {'éªŒè¯å‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15}")
    print("-" * 80)
    
    for i, result in enumerate(test_results, 1):
        print(f"{i:<6} {result['model_name']:<30} {result['val_acc']:<15.4f} {result['test_acc']:<15.4f}")
    
    print("\n" + "=" * 80)
    print("âœ… å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")
    print("=" * 80)
    print(f"\nç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  - æ•°æ®ç›®å½•: {args.output_dir}")
    print(f"  - æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"  - æ—¥å¿—ç›®å½•: {log_dir}")
    print(f"  - ç»“æœæ–‡ä»¶: {results_file}")
    
    # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
    report_file = os.path.join(args.output_dir, 'final_report.txt')
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("SIL2603æ•°æ®ä¸‹è½½å’Œæ¨¡å‹è®­ç»ƒæœ€ç»ˆæŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now()}\n\n")
        f.write(f"æ•°æ®ç»Ÿè®¡:\n")
        f.write(f"  - è®­ç»ƒé›†: {len(train_df)} æ¡\n")
        f.write(f"  - éªŒè¯é›†: {len(val_df)} æ¡\n")
        f.write(f"  - æµ‹è¯•é›†: {len(test_df)} æ¡\n")
        f.write(f"  - æ€»è®¡: {total_records} æ¡\n\n")
        f.write(f"æ¨¡å‹æ’åï¼ˆæŒ‰æµ‹è¯•é›†å‡†ç¡®ç‡ï¼‰:\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'æ’å':<6} {'æ¨¡å‹åç§°':<30} {'éªŒè¯å‡†ç¡®ç‡':<15} {'æµ‹è¯•å‡†ç¡®ç‡':<15}\n")
        f.write("-" * 80 + "\n")
        for i, result in enumerate(test_results, 1):
            f.write(f"{i:<6} {result['model_name']:<30} {result['val_acc']:<15.4f} {result['test_acc']:<15.4f}\n")
        f.write("\n" + "=" * 80 + "\n")
    
    print(f"  - æŠ¥å‘Šæ–‡ä»¶: {report_file}")
    print()
    
    return 0


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
