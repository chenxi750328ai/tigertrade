#!/usr/bin/env python3
"""
TigerTrade - Transformeræ¨¡å‹è®­ç»ƒ
ç›®æ ‡ï¼šé¢„æµ‹æœªæ¥æ”¶ç›Šç‡ï¼Œå®ç°æœˆç›ˆåˆ©20%
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from pathlib import Path
import json
from datetime import datetime
import sys

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

class TimeSeriesDataset(Dataset):
    """æ—¶é—´åºåˆ—æ•°æ®é›†"""
    
    def __init__(self, data_path, sequence_length=20):
        self.df = pd.read_csv(data_path)
        self.sequence_length = sequence_length
        
        # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡
        self.feature_cols = [c for c in self.df.columns 
                            if c not in ['timestamp', 'Unnamed: 0'] 
                            and not c.startswith('target_')]
        
        self.target_col = 'target_return_5'  # é¢„æµ‹5æœŸåçš„æ”¶ç›Š
        
        # å½’ä¸€åŒ–ç‰¹å¾
        self.features = self.df[self.feature_cols].fillna(0).astype(np.float32).values
        self.targets = self.df[self.target_col].fillna(0).astype(np.float32).values
        
        # æ ‡å‡†åŒ–
        self.feature_mean = np.mean(self.features, axis=0)
        self.feature_std = np.std(self.features, axis=0) + 1e-8
        self.features = (self.features - self.feature_mean) / self.feature_std
        
        print(f"æ•°æ®åŠ è½½: {len(self.df)} æ¡è®°å½•")
        print(f"ç‰¹å¾æ•°é‡: {len(self.feature_cols)}")
        print(f"åºåˆ—é•¿åº¦: {sequence_length}")
    
    def __len__(self):
        return len(self.df) - self.sequence_length
    
    def __getitem__(self, idx):
        # è·å–åºåˆ—
        x = self.features[idx:idx+self.sequence_length]
        y = self.targets[idx+self.sequence_length]
        
        return torch.FloatTensor(x), torch.FloatTensor([y])


class TransformerModel(nn.Module):
    """Transformeré¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, input_dim, d_model=128, nhead=4, num_layers=3, dropout=0.1):
        super().__init__()
        
        self.input_proj = nn.Linear(input_dim, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model*4,
            dropout=dropout,
            batch_first=True
        )
        
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model//2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(d_model//2, 1)
        )
    
    def forward(self, x):
        # x: (batch, seq_len, input_dim)
        x = self.input_proj(x)
        x = self.transformer(x)
        x = x[:, -1, :]  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
        x = self.fc(x)
        return x


def train_epoch(model, dataloader, optimizer, criterion):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    
    for batch_x, batch_y in dataloader:
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)
        
        optimizer.zero_grad()
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)


def evaluate(model, dataloader, criterion):
    """è¯„ä¼°æ¨¡å‹"""
    model.eval()
    total_loss = 0
    predictions = []
    targets = []
    
    with torch.no_grad():
        for batch_x, batch_y in dataloader:
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            
            total_loss += loss.item()
            predictions.extend(outputs.cpu().numpy())
            targets.extend(batch_y.cpu().numpy())
    
    predictions = np.array(predictions).flatten()
    targets = np.array(targets).flatten()
    
    # è®¡ç®—æ–¹å‘å‡†ç¡®ç‡ï¼ˆæ¶¨è·Œé¢„æµ‹å‡†ç¡®ç‡ï¼‰
    direction_acc = ((predictions > 0) == (targets > 0)).mean()
    
    return total_loss / len(dataloader), direction_acc, predictions, targets


def main():
    """ä¸»å‡½æ•°"""
    
    print("="*70)
    print("ğŸ¤– TigerTrade Transformeræ¨¡å‹è®­ç»ƒ")
    print("="*70)
    print(f"ç›®æ ‡: é¢„æµ‹æœªæ¥æ”¶ç›Šç‡ï¼Œå®ç°æœˆç›ˆåˆ©20%")
    print("="*70)
    
    # é…ç½®
    DATA_DIR = Path("/home/cx/tigertrade/data/processed")
    OUTPUT_DIR = Path("/home/cx/tigertrade/models")
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    SEQUENCE_LENGTH = 20
    BATCH_SIZE = 32
    EPOCHS = 50
    LEARNING_RATE = 0.001
    
    print(f"\nğŸ“ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸ’¾ æ¨¡å‹è¾“å‡º: {OUTPUT_DIR}")
    print(f"\nè¶…å‚æ•°:")
    print(f"  åºåˆ—é•¿åº¦: {SEQUENCE_LENGTH}")
    print(f"  æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"  è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"  å­¦ä¹ ç‡: {LEARNING_RATE}")
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®...")
    train_dataset = TimeSeriesDataset(
        DATA_DIR / "train.csv",
        sequence_length=SEQUENCE_LENGTH
    )
    val_dataset = TimeSeriesDataset(
        DATA_DIR / "val.csv",
        sequence_length=SEQUENCE_LENGTH
    )
    test_dataset = TimeSeriesDataset(
        DATA_DIR / "test.csv",
        sequence_length=SEQUENCE_LENGTH
    )
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"âœ… æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ”§ åˆ›å»ºTransformeræ¨¡å‹...")
    input_dim = len(train_dataset.feature_cols)
    model = TransformerModel(
        input_dim=input_dim,
        d_model=128,
        nhead=4,
        num_layers=3,
        dropout=0.1
    ).to(device)
    
    print(f"âœ… æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    best_val_loss = float('inf')
    patience_counter = 0
    training_history = {
        'train_loss': [],
        'val_loss': [],
        'val_direction_acc': []
    }
    
    for epoch in range(EPOCHS):
        # è®­ç»ƒ
        train_loss = train_epoch(model, train_loader, optimizer, criterion)
        
        # éªŒè¯
        val_loss, val_dir_acc, _, _ = evaluate(model, val_loader, criterion)
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(val_loss)
        
        # è®°å½•
        training_history['train_loss'].append(train_loss)
        training_history['val_loss'].append(val_loss)
        training_history['val_direction_acc'].append(val_dir_acc)
        
        # æ‰“å°è¿›åº¦
        print(f"Epoch {epoch+1:3d}/{EPOCHS} | "
              f"è®­ç»ƒæŸå¤±: {train_loss:.6f} | "
              f"éªŒè¯æŸå¤±: {val_loss:.6f} | "
              f"æ–¹å‘å‡†ç¡®ç‡: {val_dir_acc:.2%}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            
            model_path = OUTPUT_DIR / "transformer_best.pth"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'val_loss': val_loss,
                'val_direction_acc': val_dir_acc,
                'feature_cols': train_dataset.feature_cols,
                'feature_mean': train_dataset.feature_mean,
                'feature_std': train_dataset.feature_std
            }, model_path)
            
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯æŸå¤±: {val_loss:.6f})")
        else:
            patience_counter += 1
        
        # æ—©åœ
        if patience_counter >= 10:
            print(f"\nâš ï¸  æ—©åœï¼šéªŒè¯æŸå¤±10è½®æœªæ”¹å–„")
            break
    
    # æµ‹è¯•é›†è¯„ä¼°
    print(f"\nğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...")
    test_loss, test_dir_acc, test_preds, test_targets = evaluate(
        model, test_loader, criterion
    )
    
    print(f"âœ… æµ‹è¯•æŸå¤±: {test_loss:.6f}")
    print(f"âœ… æ–¹å‘å‡†ç¡®ç‡: {test_dir_acc:.2%}")
    
    # è®¡ç®—ç›ˆåˆ©æŒ‡æ ‡
    print(f"\nğŸ’° ç›ˆåˆ©èƒ½åŠ›è¯„ä¼°...")
    
    # ç®€å•ç­–ç•¥ï¼šé¢„æµ‹ä¸ºæ­£å°±åšå¤š
    strategy_returns = []
    for pred, actual in zip(test_preds, test_targets):
        if pred > 0:  # é¢„æµ‹ä¸Šæ¶¨ï¼Œåšå¤š
            strategy_returns.append(actual)
        else:  # é¢„æµ‹ä¸‹è·Œï¼Œä¸äº¤æ˜“æˆ–åšç©º
            strategy_returns.append(-actual)
    
    strategy_returns = np.array(strategy_returns)
    
    # è®¡ç®—æ”¶ç›Š
    total_return = strategy_returns.sum()
    mean_return = strategy_returns.mean()
    sharpe = strategy_returns.mean() / (strategy_returns.std() + 1e-8) * np.sqrt(252)
    max_drawdown = (np.maximum.accumulate(np.cumsum(strategy_returns)) - 
                    np.cumsum(strategy_returns)).max()
    
    win_rate = (strategy_returns > 0).mean()
    
    print(f"  æ€»æ”¶ç›Šç‡: {total_return:.2%}")
    print(f"  å¹³å‡æ”¶ç›Šç‡: {mean_return:.4%}")
    print(f"  å¤æ™®æ¯”ç‡: {sharpe:.2f}")
    print(f"  æœ€å¤§å›æ’¤: {max_drawdown:.2%}")
    print(f"  èƒœç‡: {win_rate:.2%}")
    
    # æœˆåº¦æ”¶ç›Šä¼°ç®—
    periods_per_month = 252 / 12 / 5  # å‡è®¾5æœŸ=1å¤©
    monthly_return = mean_return * periods_per_month
    print(f"\n  ğŸ“ˆ é¢„ä¼°æœˆæ”¶ç›Šç‡: {monthly_return:.2%}")
    
    if monthly_return >= 0.20:
        print(f"  âœ… è¾¾åˆ°ç›®æ ‡ï¼šæœˆç›ˆåˆ©ç‡ >= 20%ï¼")
    else:
        print(f"  âš ï¸  æœªè¾¾æ ‡ï¼šéœ€è¦ä¼˜åŒ–")
        print(f"     å·®è·: {(0.20 - monthly_return):.2%}")
    
    # ä¿å­˜ç»“æœ
    results = {
        'timestamp': datetime.now().isoformat(),
        'model': 'Transformer',
        'test_loss': float(test_loss),
        'direction_accuracy': float(test_dir_acc),
        'total_return': float(total_return),
        'mean_return': float(mean_return),
        'sharpe_ratio': float(sharpe),
        'max_drawdown': float(max_drawdown),
        'win_rate': float(win_rate),
        'estimated_monthly_return': float(monthly_return),
        'target_achieved': bool(monthly_return >= 0.20),
        'training_history': {
            k: [float(v) for v in vals] 
            for k, vals in training_history.items()
        }
    }
    
    result_path = OUTPUT_DIR / "training_results.json"
    with open(result_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {result_path}")
    
    print("\n" + "="*70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*70)
    print(f"\nä¸‹ä¸€æ­¥ï¼šç­–ç•¥å›æµ‹å’Œé£é™©ç®¡ç†")
    
    return results


if __name__ == "__main__":
    try:
        results = main()
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
