#!/usr/bin/env python3
"""
æ•°æ®å‡†å¤‡ä¸»è„šæœ¬ - Agent 1æ ¸å¿ƒä»»åŠ¡
æ•´åˆæ‰€æœ‰æ•°æ®å¤„ç†æ­¥éª¤ï¼Œç”Ÿæˆtrain/val/test.csv
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

import pandas as pd
from pathlib import Path
from datetime import datetime

from src.data_processor.cleaner import DataCleaner
from src.data_processor.normalizer import DataNormalizer
from src.data_processor.splitter import DataSplitter


def main():
    print("="*80)
    print("ğŸ“Š Agent 1: æ•°æ®å‡†å¤‡Pipeline")
    print("="*80)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # é…ç½®
    data_dir = Path('/home/cx/trading_data')
    output_dir = data_dir / 'processed'
    output_dir.mkdir(exist_ok=True)
    
    # 1. åŠ è½½æ‰€æœ‰æ•°æ®
    print(f"{'='*80}")
    print(f"æ­¥éª¤1: åŠ è½½æ‰€æœ‰åŸå§‹æ•°æ®")
    print(f"{'='*80}\n")
    
    data_files = [
        data_dir / 'ticks/SIL2603_ticks_20260121.csv',
        data_dir / 'SIL2603_1min_combined.csv',
        data_dir / 'SIL2603_5min_7days.csv',
        data_dir / 'SIL2603_1h_30days.csv',
        data_dir / 'SIL2603_daily_90days.csv'
    ]
    
    all_data = []
    total_size = 0
    
    for file in data_files:
        if file.exists():
            df = pd.read_csv(file)
            size_mb = file.stat().st_size / 1024 / 1024
            total_size += size_mb
            
            # æ ‡å‡†åŒ–æ—¶é—´åˆ—
            for time_col in ['time', 'datetime']:
                if time_col in df.columns:
                    if time_col == 'time':
                        df['datetime'] = pd.to_datetime(df['time'], unit='ms')
                    else:
                        df['datetime'] = pd.to_datetime(df['datetime'])
                    break
            
            # æ ‡å‡†åŒ–åˆ—åï¼ˆTickæ•°æ®ç”¨priceä½œä¸ºcloseï¼‰
            if 'price' in df.columns and 'close' not in df.columns:
                df['close'] = df['price']
            if 'open' not in df.columns:
                df['open'] = df['close']
            if 'high' not in df.columns:
                df['high'] = df['close']
            if 'low' not in df.columns:
                df['low'] = df['close']
            if 'volume' not in df.columns:
                df['volume'] = 0
            
            # ä¿ç•™æ ¸å¿ƒåˆ—
            core_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']
            df = df[[col for col in core_cols if col in df.columns]]
            
            all_data.append(df)
            print(f"  âœ… {file.name:<40} {len(df):>8}æ¡  {size_mb:>6.2f}MB")
    
    print(f"\n  æ€»æ–‡ä»¶å¤§å°: {total_size:.2f}MB")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    print(f"\n  åˆå¹¶æ‰€æœ‰æ•°æ®...")
    df_all = pd.concat(all_data, ignore_index=True)
    df_all = df_all.sort_values('datetime').reset_index(drop=True)
    
    print(f"  åˆå¹¶åæ€»è®¡: {len(df_all):,}æ¡")
    print(f"  æ—¶é—´èŒƒå›´: {df_all['datetime'].iloc[0]} ~ {df_all['datetime'].iloc[-1]}")
    print(f"  æ—¶é—´è·¨åº¦: {(df_all['datetime'].iloc[-1] - df_all['datetime'].iloc[0]).days}å¤©")
    
    # 2. æ•°æ®æ¸…æ´—
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤2: æ•°æ®æ¸…æ´—")
    print(f"{'='*80}\n")
    
    cleaner = DataCleaner(outlier_threshold=0.10)
    df_clean = cleaner.clean(df_all)
    
    # æ˜¾ç¤ºæ¸…æ´—ç»Ÿè®¡
    stats = cleaner.get_stats()
    print(f"\n  æ¸…æ´—ç»Ÿè®¡:")
    print(f"    åŸå§‹: {stats['original_count']:,}æ¡")
    print(f"    é‡å¤: {stats['duplicates']:,}æ¡")
    print(f"    å¼‚å¸¸: {stats['outliers']:,}æ¡")
    print(f"    ç¼ºå¤±: {stats['missing']:,}ä¸ªå€¼")
    print(f"    æœ€ç»ˆ: {stats['final_count']:,}æ¡")
    
    # 3. è®¡ç®—åŸºç¡€ç‰¹å¾
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤3: è®¡ç®—åŸºç¡€ç‰¹å¾")
    print(f"{'='*80}\n")
    
    # ä»·æ ¼å˜åŒ–
    df_clean['price_change'] = df_clean['close'].diff()
    df_clean['price_change_pct'] = df_clean['close'].pct_change()
    
    # æ—¶é—´é—´éš”
    df_clean['time_delta'] = df_clean['datetime'].diff().dt.total_seconds()
    
    # ä»·æ ¼èŒƒå›´
    df_clean['price_range'] = df_clean['high'] - df_clean['low']
    df_clean['price_range_pct'] = df_clean['price_range'] / df_clean['close']
    
    # æˆäº¤é‡å˜åŒ–
    df_clean['volume_change'] = df_clean['volume'].diff()
    df_clean['volume_change_pct'] = df_clean['volume'].pct_change()
    
    # å¡«å……ç¬¬ä¸€è¡Œçš„NaN
    df_clean = df_clean.fillna(0)
    
    print(f"  âœ… ç‰¹å¾æ•°é‡: {len(df_clean.columns)}åˆ—")
    print(f"  ç‰¹å¾åˆ—:")
    for col in df_clean.columns:
        if col not in ['datetime']:
            print(f"    - {col}")
    
    # 4. æ•°æ®æ ‡å‡†åŒ–
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤4: æ•°æ®æ ‡å‡†åŒ–")
    print(f"{'='*80}\n")
    
    normalizer = DataNormalizer(method='zscore')
    
    feature_cols = ['open', 'high', 'low', 'close', 'volume',
                    'price_change', 'price_change_pct', 'time_delta',
                    'price_range', 'price_range_pct',
                    'volume_change', 'volume_change_pct']
    
    df_norm = normalizer.fit_transform(df_clean, feature_cols)
    
    # ä¿å­˜æ ‡å‡†åŒ–å‚æ•°
    normalizer.save_scalers(output_dir / 'scaler_params.json')
    
    # 5. åˆ’åˆ†æ•°æ®é›†
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤5: åˆ’åˆ†æ•°æ®é›†")
    print(f"{'='*80}\n")
    
    splitter = DataSplitter(train_ratio=0.7, val_ratio=0.15)
    df_train, df_val, df_test = splitter.split(df_norm)
    
    # 6. ä¿å­˜æ•°æ®
    print(f"\n{'='*80}")
    print(f"æ­¥éª¤6: ä¿å­˜æ•°æ®")
    print(f"{'='*80}\n")
    
    df_train.to_csv(output_dir / 'train.csv', index=False)
    df_val.to_csv(output_dir / 'val.csv', index=False)
    df_test.to_csv(output_dir / 'test.csv', index=False)
    
    print(f"  âœ… {output_dir / 'train.csv'}")
    print(f"  âœ… {output_dir / 'val.csv'}")
    print(f"  âœ… {output_dir / 'test.csv'}")
    
    # 7. ç”Ÿæˆæ•°æ®æŠ¥å‘Š
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ æ•°æ®è´¨é‡æŠ¥å‘Š")
    print(f"{'='*80}\n")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'total_records': len(df_norm),
        'train_records': len(df_train),
        'val_records': len(df_val),
        'test_records': len(df_test),
        'time_range': {
            'start': str(df_norm['datetime'].iloc[0]),
            'end': str(df_norm['datetime'].iloc[-1]),
            'days': (df_norm['datetime'].iloc[-1] - df_norm['datetime'].iloc[0]).days
        },
        'price_range': {
            'min': float(df_clean['close'].min()),
            'max': float(df_clean['close'].max()),
            'change_pct': float((df_clean['close'].iloc[-1] / df_clean['close'].iloc[0] - 1) * 100)
        },
        'cleaning_stats': stats,
        'feature_count': len(feature_cols),
        'normalization_method': 'zscore'
    }
    
    import json
    report_file = output_dir / 'data_report.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"æ€»è®°å½•æ•°: {report['total_records']:,}æ¡")
    print(f"æ—¶é—´è·¨åº¦: {report['time_range']['days']}å¤©")
    print(f"ä»·æ ¼å˜åŒ–: {report['price_range']['change_pct']:+.2f}%")
    print(f"ç‰¹å¾æ•°é‡: {report['feature_count']}ä¸ª")
    print(f"\næŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    # å®Œæˆ
    print(f"\n{'='*80}")
    print(f"âœ… Agent 1 ä»»åŠ¡å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print(f"   - train.csv: {len(df_train):,}æ¡")
    print(f"   - val.csv: {len(df_val):,}æ¡")
    print(f"   - test.csv: {len(df_test):,}æ¡")
    print(f"   - scaler_params.json: æ ‡å‡†åŒ–å‚æ•°")
    print(f"   - data_report.json: æ•°æ®è´¨é‡æŠ¥å‘Š")
    print(f"\nğŸ¯ ä¸‹ä¸€æ­¥: Agent 2 ä½¿ç”¨è¿™äº›æ•°æ®è®­ç»ƒæ¨¡å‹")
    print(f"{'='*80}\n")

    return report


def prepare_training_data():
    """ä¾›æ¯æ—¥ä¾‹è¡Œè„šæœ¬è°ƒç”¨ï¼Œä¸ main() ä¸€è‡´ï¼Œè¿”å›æ•°æ®æŠ¥å‘Šæ‘˜è¦ã€‚"""
    return main()


if __name__ == '__main__':
    main()
