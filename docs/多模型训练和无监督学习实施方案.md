# 多模型训练和无监督学习实施方案

**生成时间**: 2026-01-26  
**目的**: 实施多模型对比训练、修复收益率预测头、无监督学习、获取更多数据

---

## 一、任务清单

1. ✅ **至少保证三个模型训练和对比**
2. ✅ **解决预测头没有学到的问题**
3. ✅ **增加无监督学习模式**
4. ✅ **获取更多数据**

---

## 二、实施方案

### 2.1 多模型对比训练

**文件**: `/home/cx/tigertrade/scripts/train_multiple_models_comparison.py`

**功能**:
- 训练至少3个模型（LSTM、Transformer、Enhanced Transformer）
- 对比模型性能（验证准确率、收益率MAE）
- 保存对比结果

**使用方法**:
```bash
cd /home/cx/tigertrade
python scripts/train_multiple_models_comparison.py
```

**输出**:
- `best_lstm_improved.pth`: 改进的LSTM模型
- `model_comparison_results.txt`: 模型对比结果

**改进点**:
- LSTM模型使用MSELoss替代HuberLoss
- 移除sigmoid限制，使用clamp限制范围
- 分离训练阶段（先训练收益率，再训练动作分类）

---

### 2.2 修复收益率预测头

**修改文件**: `/home/cx/tigertrade/src/strategies/llm_strategy.py`

**修改内容**:

1. **损失函数改进**:
```python
# 旧: self.profit_criterion = nn.HuberLoss(delta=0.01)
# 新: self.profit_criterion = nn.MSELoss()  # 更敏感
```

2. **移除sigmoid限制**:
```python
# 旧: profit = torch.sigmoid(profit) * 0.3
# 新: profit = torch.clamp(profit, min=0.0, max=0.3)  # 不使用sigmoid压缩
```

**原因**:
- MSELoss比HuberLoss更敏感，能更好地更新模型参数
- sigmoid函数会压缩输出差异，导致所有样本输出相似
- clamp限制范围但不压缩差异，允许模型学习不同的收益率

---

### 2.3 无监督学习模式

**文件**: `/home/cx/tigertrade/scripts/unsupervised_pretraining.py`

**功能**:
- BERT MASK式的无监督预训练
- 预测未来收益率（不需要动作标签）
- 使用大量未标注数据

**训练目标**:
- 预测未来1小时（120分钟）的最大收益率
- 使用MSELoss作为损失函数

**使用方法**:
```bash
cd /home/cx/tigertrade
python scripts/unsupervised_pretraining.py
```

**输出**:
- `pretrained_return_model.pth`: 预训练模型
- 可用于后续的有监督微调

**优势**:
- 不需要动作标签，可以使用更多数据
- 学习收益率的内在模式
- 可能解决收益率预测头不学习的问题

---

### 2.4 获取更多数据

**文件**: `/home/cx/tigertrade/scripts/fetch_more_training_data.py`

**功能**:
- 通过API获取更多历史数据
- 扩大数据时间范围（默认60天）
- 支持多时间尺度（1min, 5min, 1h, 1d, 1w, 1M）

**使用方法**:
```bash
cd /home/cx/tigertrade
python scripts/fetch_more_training_data.py --days 60
```

**参数**:
- `--days`: 回溯天数（默认60天）
- `--output`: 输出文件路径（可选）

**输出**:
- `training_data_multitimeframe_extended_YYYYMMDD_HHMMSS.csv`: 扩展的训练数据

---

## 三、执行顺序

### 步骤1: 获取更多数据
```bash
cd /home/cx/tigertrade
python scripts/fetch_more_training_data.py --days 60
```

### 步骤2: 无监督预训练（可选）
```bash
python scripts/unsupervised_pretraining.py
```

### 步骤3: 多模型对比训练
```bash
python scripts/train_multiple_models_comparison.py
```

---

## 四、预期效果

### 4.1 收益率预测头改进

**改进前**:
- 所有样本的收益率预测都在15.50%左右
- 标准差0.000003，几乎没有变化

**改进后**:
- 收益率预测应该根据输入特征变化
- 不同动作、不同价格区间，收益率预测应该不同
- 标准差应该明显增大

### 4.2 模型对比

**对比指标**:
- 验证准确率（动作分类）
- 收益率MAE（收益率预测）
- 训练时间
- 模型大小

**预期结果**:
- LSTM（改进版）: 收益率MAE应该降低
- Transformer: 可能在某些指标上更好
- Enhanced Transformer: 可能性能最好，但训练时间更长

### 4.3 无监督学习

**预期效果**:
- 预训练模型能够预测未来收益率
- 预训练后的模型在微调时应该学习更快
- 收益率预测头应该能够学习到有用的特征

---

## 五、注意事项

### 5.1 数据获取

- API可能有请求频率限制
- 需要确保网络连接稳定
- 大量数据可能需要较长时间

### 5.2 模型训练

- 多模型训练需要大量GPU内存
- 建议逐个训练，而不是同时训练
- 保存中间结果，避免训练中断

### 5.3 无监督学习

- 预训练可能需要较长时间
- 预训练模型需要与有监督模型架构兼容
- 可能需要调整学习率和训练策略

---

## 六、后续优化

1. **集成预训练模型**: 将预训练模型集成到有监督训练中
2. **模型集成**: 使用多个模型的预测结果进行集成
3. **超参数优化**: 根据对比结果优化超参数
4. **数据增强**: 使用数据增强技术增加训练数据

---

**报告生成时间**: 2026-01-26  
**状态**: 实施方案已完成，可以开始执行
