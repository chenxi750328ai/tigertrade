# 保持大容量增强正则化方案

**原则**: 不降低模型容量，使用更强的正则化技术

---

## 一、增强的正则化技术

### 1.1 Focal Loss

**原理**: 
- Focal Loss = -alpha * (1-p_t)^gamma * log(p_t)
- gamma=2.0：关注难分类样本
- alpha：类别权重，处理类别不平衡

**优势**:
- 专门处理类别不平衡问题
- 自动关注难分类样本
- 减少易分类样本的权重

### 1.2 增强的标签平滑

**改进**:
- 标签平滑: 0.1 → 0.2（增强）
- 结合类别权重
- 防止模型过度自信

### 1.3 组合损失函数

**策略**:
- 70% Focal Loss + 30% Label Smoothing
- 结合两种技术的优势

### 1.4 其他正则化（保持）

- Dropout: 0.3-0.5
- weight_decay: 5e-3
- 更好的初始化
- CosineAnnealingLR

---

## 二、技术对比

### 2.1 损失函数对比

| 技术 | 原始 | 第一版正则化 | 增强版正则化 |
|------|------|------------|------------|
| 损失函数 | CrossEntropy | Label Smoothing (0.1) | **Focal Loss + Label Smoothing (0.2)** |
| 类别权重 | ❌ | ✅ | ✅ |
| 标签平滑 | ❌ | 0.1 | **0.2** |
| Focal Loss | ❌ | ❌ | **✅ (gamma=2.0)** |

### 2.2 预期效果

**Focal Loss的优势**:
- 自动关注难分类样本
- 减少易分类样本的权重
- 更好地处理类别不平衡

**组合使用的优势**:
- Focal Loss处理类别不平衡
- Label Smoothing防止过度自信
- 两者结合，效果更好

---

## 三、为什么Focal Loss可能有效？

### 3.1 问题分析

**原始问题**:
- 模型总是预测类别2（占54.06%）
- 类别2是"易分类"样本（多数类）
- 模型过度关注易分类样本

### 3.2 Focal Loss的解决方案

**Focal Loss机制**:
- (1-p_t)^gamma：当p_t接近1时（易分类），权重接近0
- 当p_t接近0时（难分类），权重接近1
- 自动降低易分类样本的权重

**效果**:
- 模型不再过度关注类别2
- 更关注类别0和类别1（难分类）
- 提高整体准确率

---

## 四、训练配置

### 4.1 损失函数配置

```python
# Focal Loss (70%)
focal_loss = FocalLoss(alpha=class_weights, gamma=2.0)

# Label Smoothing (30%)
smooth_loss = LabelSmoothingCrossEntropy(smoothing=0.2, weight=class_weights)

# 组合损失
action_loss = 0.7 * focal_loss + 0.3 * smooth_loss
```

### 4.2 其他配置（保持）

- 模型容量: 26.9M参数（保持）
- Dropout: 0.3-0.5
- weight_decay: 5e-3
- 学习率调度: CosineAnnealingLR

---

## 五、预期结果

### 5.1 验证准确率

- **原始**: 33.57%（总是预测类别2）
- **第一版正则化**: 33.57%（没有改善）
- **预期（Focal Loss）**: 50%+（不再总是预测类别2）

### 5.2 训练稳定性

- **预期**: 验证准确率有变化
- **预期**: 模型在学习不同的类别

---

**报告生成时间**: 2026-01-27 10:50  
**状态**: 使用Focal Loss + 增强标签平滑，训练进行中
