# 大模型搜参设计方案（MoE / Transformer 交易模型）

> 本文为 **算法改进、提升收益率** 的一环：对 LLM、MoE、LSTM、Enhanced/Large/Huge Transformer 等**交易用模型**做超参搜索的设计与业界方案对比。  
> 需求与设计总入口见 [需求分析和Feature测试设计](../需求分析和Feature测试设计.md)；回测/实盘/训练流程对比见 [回测与实盘_输入与处理流程对比](回测与实盘_输入与处理流程对比.md)。

---

## 1. 目标与范围

### 1.1 目标

- **提升收益率**：通过系统化超参搜索，找到更优的模型配置（学习率、层数、hidden size、dropout 等），改善验证集/回测表现与实盘期望收益。
- **可复现、可纳入例行**：搜参流程可脚本化、可接入每日或周期性「算法优化」任务，与现有 DataDrivenOptimizer、回测、报告联动。

### 1.2 范围（本方案中的「大模型」）

- **本项目中**「大模型」指：**交易决策用的神经网络**，不特指百亿级 NLP 大模型。包括：
  - **MoE Transformer**（LLM 策略 + MoE 结构）
  - **LLM 策略**（hybrid / pure_ml）
  - **Enhanced / Large / Huge Transformer** 策略
  - **LSTM** 策略
- 共同特点：有 **学习率、层数、hidden/d_model、dropout、序列长度** 等超参；单次训练成本远低于千亿级 LLM，但仍希望用「有限试错」找到较好配置，而非穷举。

---

## 2. 业界方案对比

### 2.1 常见搜参方法（简要）

| 方法 | 思路 | 优点 | 缺点 | 适用场景 |
|------|------|------|------|----------|
| **Grid Search** | 参数空间离散化，遍历所有组合 | 实现简单、可复现 | 组合数爆炸，大模型/多超参时不现实 | 超参很少（1–2 个）、取值少 |
| **Random Search** | 在空间内均匀/对数均匀采样 | 实现简单，高维下往往比 Grid 更高效 | 不利用已有观测，可能浪费算力 | 中等维度、算力有限、快速摸底 |
| **贝叶斯优化（Bayesian Optimization）** | 用高斯过程等建代理模型，按采集函数（如 EI）选下一组超参 | 用少量 trial 逼近较优解，适合昂贵目标 | 实现与调参更复杂；高维时代理模型吃力 | 目标评估贵、trial 数受限（如几十次） |
| **Optuna** | 定义-by-run API，支持多种采样器（TPE、CMA-ES、Grid 等）与剪枝 | 易集成、可扩展、支持并行与剪枝 | 需把单次训练封装成 trial | 中小规模模型、单机/多进程 |
| **Ray Tune** | 分布式超参调优，支持 Optuna/ASHA/HyperBand/PBT 等 | 多机多 GPU、早停、Population-Based Training | 依赖 Ray 集群与资源管理 | 大规模、多机、需早停/动态调参 |
| **ASHA / HyperBand** | 早停：多组配置同时跑，表现差的提前终止 | 在有限预算内尝试更多配置 | 需支持「按 epoch/step 汇报指标」 | 单次训练可中断、有中间指标 |
| **Population-Based Training (PBT)** | 训练过程中动态调整超参，并复制强配置、替换弱配置 | 适合长训练、可边训边搜 | 实现与调试复杂 | 预训练/长周期微调 |
| **自适应 LoRA / 结构化调参** | 在参数子集（如 LoRA rank、学习率）上搜参 | 计算量小，适合微调 | 依赖已有基座与参数化方式 | 微调场景 |

### 2.2 业界实践要点（LLM/大模型相关）

- **Neptune / 业界总结**（如 [Neptune: Hyperparameter Optimization For LLMs](https://neptune.ai/blog/hyperparameter-optimization-for-llms)）：
  - 大模型单次训练成本高，**Grid Search 不现实**；更推荐 **贝叶斯优化、PBT、自适应 LoRA** 等「少 trial、高信息利用」或「训练中动态调参」的方式。
  - 关键超参：**学习率（及 schedule）、weight decay、dropout、层数、hidden size、梯度裁剪**等；学习率常配合 **warmup + cosine/linear decay**。
- **Ray Tune 文档**：小模型可用 Random + ASHA 早停；大模型更推荐 BayesOpt/Ax 或 PBT；若按 epoch 汇报指标，建议 ASHA scheduler。
- **本项目上下文**：模型为「交易用中小规模」Transformer/MoE/LSTM，单次训练在单机可接受（分钟级到小时级），trial 数可控制在数十量级。**不需要**多机 Ray 集群，但需要 **可接入现有每日/周期流程**（如与 DataDrivenOptimizer、回测、报告联动）。

### 2.3 方案选型建议（本项目）

| 优先级 | 方案 | 说明 |
|--------|------|------|
| **短期** | **Optuna（TPE 采样）+ 单机多 trial** | 实现快、与现有 Python 训练脚本兼容；把「单次训练 + 验证集/回测指标」封装成 objective；支持 pruning（如 Optuna 的 MedianPruner）做简单早停。 |
| **短期** | **将 DataDrivenOptimizer 接入每日/周期任务** | 当前 DataDrivenOptimizer 已根据近期数据给出 model_params（lstm_hidden_size、transformer_d_model、learning_rate、dropout 等）与 thresholds；可改为「建议值写入配置/报告」并由训练或推理读取，形成闭环。 |
| **中期** | **在 Optuna 上增加 ASHA/剪枝** | 若单次训练较久，可为 trial 按 epoch 汇报 val_loss/accuracy，用 Optuna 的 MedianPruner 或集成 Ray Tune 的 ASHA 做早停，节省算力。 |
| **中长期** | **贝叶斯/TPE 与回测指标结合** | 以「回测收益/夏普/胜率」或「验证集收益代理」为目标做超参搜索，避免只优化 loss 而与业务目标脱节。 |

---

## 3. 设计要点（落地时需满足）

### 3.1 超参空间（与现有模型一致）

- **MoE / LLM 策略**：学习率、层数、d_model、nhead、dropout、序列长度、predict_profit 等（与 `llm_strategy` / `moe_transformer` 构造参数对齐）。
- **Enhanced / Large / Huge Transformer**：input_size 由 prepare_features 决定；可搜 d_model、nhead、num_layers、dropout、学习率等。
- **LSTM**：hidden_size、num_layers、dropout、学习率等。
- 具体取值范围在实现时按现有默认值与业务约束设定（如显存、训练时间上限）。

### 3.2 目标指标

- **主目标**：验证集上的 **收益相关指标**（如 return_pct、夏普、胜率）或 **收益代理**（如验证集 action 准确率 + 收益加权）。  
- **辅助**：train/val loss、accuracy，用于早停与诊断；避免只优化 loss 而实盘/回测不佳。

### 3.3 与现有组件的衔接

- **数据**：与 [回测与实盘_输入与处理流程对比](回测与实盘_输入与处理流程对比.md) 一致，训练与推理用同一套 prepare_features 与数据源，避免搜参结果因输入不一致而失效。
- **DataDrivenOptimizer**：可作为「规则型建议」与搜参并行：先跑 DataDrivenOptimizer 得到建议区间或默认值，再在区间内用 Optuna 细化；或将 DataDrivenOptimizer 输出写入配置，供训练/推理使用。
- **每日例行**：搜参可为「每周/每周期」任务，或作为「算法优化」步骤之一（在 optimize_algorithm_and_profitability 或独立脚本中调用），最优超参写入配置或模型保存路径，并在报告中记录。

### 3.4 实现与文档

- **实现**：建议单独脚本（如 `scripts/hyperparameter_search.py` 或 `src/strategies/hpo_*.py`），接收策略类型、数据路径、目标指标、trial 数、超参空间等；内部调用现有 `train_model` / 回测接口。
- **文档**：本文档作为设计总览；具体超参空间、运行方式、与 DataDrivenOptimizer/报告的联动在实现后补充到本文或链接到 README/项目计划。

---

## 4. 与需求、STATUS、项目计划的关联

- **需求**：[需求分析和Feature测试设计](../需求分析和Feature测试设计.md) 中 **Feature 7（算法改进与优化）** 明确包含模型超参调优与大模型搜参方案；AR7.2/AR7.3 要求模型超参可影响训练/推理、且大模型搜参有设计文档并接入设计总入口。
- **STATUS**：[STATUS](../STATUS.md) 快速链接已包含「需求分析与设计总入口」「大模型搜参设计方案」；3 月草案含「模型超参纳入每日/周期优化、大模型搜参方案落地」。
- **项目计划**：在 [项目计划_月度周计划](../项目计划_月度周计划.md) 中增加「模型超参纳入优化」「大模型搜参方案实现」等任务项，与 Feature 7 及本文保持一致。

---

## 5. 参考文献与链接

- Neptune: [Hyperparameter Optimization For LLMs: Advanced Strategies](https://neptune.ai/blog/hyperparameter-optimization-for-llms)  
- Ray Tune: [Hyperparameter Tuning](https://docs.ray.io/en/latest/tune/), [Optuna example](https://docs.ray.io/en/latest/tune/examples/optuna_example.html)  
- Optuna: [Official docs](https://optuna.readthedocs.io/)（TPE、Pruning、多目标等）  
- 本项目：`src/strategies/data_driven_optimization.py`（DataDrivenOptimizer）、`scripts/optimize_algorithm_and_profitability.py`（每日优化流程）
