# MoE Transformer 策略设计文档

> 本文档描述 MoE（混合专家）Transformer 交易策略的**算法设计**、**训练过程设计**与**数据分析设计**。实现见 `src/strategies/moe_strategy.py`、`src/strategies/moe_transformer.py`。

---

## 1. 策略概述

MoE Transformer 策略使用**混合专家 + Transformer** 时序模型，对多时间尺度特征进行编码，输出**动作分类**（不操作/买入/卖出）与**收益率预测**，在满足风控条件下生成交易信号。为 DEMO/实盘主推策略之一。

---

## 2. Transformer/MoE 在金融量化中的实现原理

Transformer 在金融量化中的本质是：用**自注意力（Self-Attention）**与**多头注意力（Multi-Head Attention）**对金融时序建模，解决传统时序模型（如 LSTM）的长序列依赖不足、并行计算效率低的问题。本策略在此基础上引入 **MoE（混合专家）**，对不同市场状态（趋势/震荡等）做稀疏专家选择，核心可拆解为三层。

### 2.1 金融数据的时序适配与编码

- **输入层重构**：将原始金融数据（K 线开高低收量、多时间尺度指标、可选 Tick/盘口）转化为「时序 Token 序列」。每个 Token 对应一个时间切片（如 1 分钟/5 分钟）的**多维度特征向量**（默认 46 维），经线性投影映射到 `d_model`（如 256/512）。同时加入**位置编码（Positional Encoding）** 弥补 Transformer 无内置时序感知的缺陷；本实现采用正弦/余弦位置编码（与序列长度对齐）。
- **特征归一化**：金融数据量纲差异大（如价格百元级、成交量万手级），在进入模型前需做 Z-score、Min-Max 等归一化（在数据管道或特征计算阶段完成），避免数值范围差异导致注意力权重失真。

### 2.2 自注意力与 MoE 的核心作用

- **自注意力**：在量化场景中用于捕捉**跨时间、跨维度**的关联。对单标的：计算不同时间切片（如当前与过去 N 根 K 线）的注意力权重，识别「量价异动→趋势反转」「波动率放大→方向选择」等长周期依赖。**多头注意力**：不同头可聚焦不同规律（如一个头关注量价匹配、一个头关注均线/BOLL 突破），最终融合多维度规律。
- **MoE（混合专家）**：将部分 Transformer 层的 FFN 替换为多个「专家」FFN + 门控网络。门控按当前隐藏状态为每个专家打分，仅 **Top-K 个专家**参与前向（稀疏激活），实现：参数量大但单次计算可控、不同专家可对应不同市场状态（如趋势专家、震荡专家），减轻过拟合。训练时辅以**负载均衡损失**，鼓励各专家被均衡使用。

### 2.3 下游任务适配

- **预测类任务**：动作分类（不操作/买入/卖出）由全连接 + Softmax/argmax；收益率预测由回归头输出标量，用于过滤低收益信号。
- **策略信号生成**：择时信号（买/持/卖）结合动作置信度与预测收益，在仓位与风控约束下生成最终交易指令。
- **本策略中的 MoE**：将 Transformer 编码后的表示交给 MoE 层，动态选择适配当前市场状态的专家，再经池化与输出头得到动作与收益预测。

---

## 3. 模型架构（本策略实现）

本策略对应实现为 `MoETradingTransformerWithProfit`（`src/strategies/moe_transformer.py`），数据流与层级如下。

| 层级 | 模块 | 输入 → 输出 | 说明 |
|------|------|-------------|------|
| 输入 | `input_projection` | (B, L, 46) → (B, L, d_model) | 线性投影，L=seq_length（如 500） |
| 位置 | 正弦位置编码 | 与序列相加 | 固定或截断至 max_len |
| 编码 | N × `MoETransformerEncoderLayer` | (B, L, d_model) → (B, L, d_model) | 每层：Self-Attention（可稀疏/窗口）→ LayerNorm → MoE(FFN) → LayerNorm |
| 池化 | `attention_pool` | (B, L, d_model) → (B, d_model) | 对序列做注意力池化得到全局表示 |
| 输出 | `action_head` | (B, d_model) → (B, 3) | 多层 Linear+GELU+LayerNorm+Dropout → 动作 logits |
| 输出 | `profit_head` | (B, d_model) → (B, 1) | Linear+LayerNorm+GELU+Dropout → 收益率标量 |

- **MoETransformerEncoderLayer**：内部为 SparseMultiheadAttention（可设 `window_size` 做局部注意力；**因果掩码 causal_mask=True**，位置 t 仅可关注 t 及过去，防未来信息泄露）+ MoELayer（num_experts、top_k）。前向返回序列表示与 MoE 负载均衡辅助损失。
- **维度**：`input_size=46`，`d_model`（如 256/512），`num_layers`（如 6～8），`nhead`（如 8），`num_experts`（如 4～8），`top_k`（如 2）。

---

## 4. 预测原理（从输入到交易信号）

1. **输入**：当前时刻及历史共 `seq_length` 个时间步的多维特征（46 维），形状 (1, seq_length, 46)（单样本）或 (B, seq_length, 46)（批量）。
2. **编码**：经 input_projection、加位置编码、过 N 层 MoE Transformer 编码器，得到 (B, L, d_model)；再经 attention_pool 得到全局向量 (B, d_model)。
3. **输出**：`action_head` 输出 3 维 logits → softmax 得动作概率，argmax 或按阈值得动作（0/1/2）；`profit_head` 输出标量预测收益。
4. **信号生成**：结合动作置信度（如最大概率）与预测收益，在满足仓位上限、止损止盈、日亏损上限等风控下，决定是否发出买入/卖出；否则观望。逻辑在 `moe_strategy.py` 与基类策略中实现。

---

## 5. 模型可识别的金融数据核心特征

借助自注意力与多时间尺度输入，本模型可侧重捕捉以下类型的特征（无需全部人工设计，部分由注意力自动学习）：

- **量价类**：量价匹配/背离（如价涨量缩、价跌量增）、趋势延续/反转（均线、BOLL、RSI 等）、波动率时变（如跳空后波动率聚类）。多时间尺度（1m/5m 等）对齐后，注意力可学习不同周期的量价关系。
- **市场结构与流动性**：若输入中包含 Tick 或成交分布，可隐含反映盘口与流动性变化；多标的场景下可扩展为跨标的注意力（当前实现以单标的为主）。
- **跨时间依赖**：长序列（如 500 步）使模型能利用较远历史与当前状态的关联，优于仅看短窗口的规则或简单 RNN。
- **落地注意**：金融数据非平稳、信噪比低，需依赖正则（Dropout、权重衰减）、早停、回测与样本外验证，避免过拟合与「数据挖掘陷阱」；注意力权重可辅助可解释性，但仍需结合业务逻辑校验。

---

## 6. 优势与落地注意事项

- **优势**：长序列建模能力强；并行计算效率高；特征自动挖掘；MoE 稀疏激活兼顾表达力与过拟合控制。
- **注意事项**：数据需归一化与严格时序划分；过拟合风险大时需加强正则与小样本策略；部署时需保证训练与推理的 seq_length、input_size、d_model 等一致。

---

## 7. 算法设计（小结与损失）

### 7.1 模型结构小结

- **输入**：多时间尺度特征序列（默认 46 维），序列长度 500（与训练一致）。形状 `(batch, seq_len=500, input_size=46)`。
- **编码**：Transformer 编码器 + **MoE 层**（部分 FFN 替换为多个专家 + 门控，Top-K 稀疏激活）。
- **输出**：动作头 → 3 类 logits；收益头 → 收益率标量（predict_profit=True）。

### 7.2 MoE 与稀疏注意力

- **因果掩码**：编码器自注意力默认 `causal_mask=True`，位置 t 仅可关注 j≤t，避免未来信息泄露（金融回测/实盘必须）。
- **MoE**：部分层的 FFN 替换为 N 个专家 + 门控，仅 Top-K 专家参与前向；详见 [MoE和稀疏注意力方案说明](../MoE和稀疏注意力方案说明.md)。
- **稀疏注意力**：可设 `window_size` 做局部注意力或对注意力头做 dropout，减少长序列计算与过拟合。

### 7.3 损失与优化目标

- **动作**：交叉熵（可选类别权重）。**收益**：MSE 或 Smooth L1。总损失：`loss_action + λ * loss_profit` + MoE 负载均衡辅助损失。

### 7.4 信号生成（推理）

- 由动作置信度与预测收益，在仓位与风控约束下决定是否发出买入/卖出；否则观望。

---

## 8. 训练过程设计

### 8.1 数据划分

- **训练集 / 验证集**：按时间划分（如 80% 训练、20% 验证），避免未来信息泄露；不做随机打乱。
- **序列构造**：每个样本为一段连续时间步的特征序列 + 对应标签（下一段动作与收益）。

### 8.2 训练流程

1. **数据加载**：从多时间尺度特征文件（如 `training_data_multitimeframe_*.csv` 或 merged）按 seq_length 滑窗构造 (X, y_action, y_profit)。
2. **Epoch 与 batch**：多 epoch 遍历训练集；batch_size 根据显存设定（如 32、64）。
3. **优化器**：常用 Adam，学习率可设 1e-4～1e-3，可选学习率衰减或 warmup。
4. **早停**：根据验证集动作准确率或验证损失，若干 epoch 无提升则停止，并恢复最佳权重。
5. **保存**：最佳模型保存为 `best_moe_transformer.pth`（路径可由配置 `model_path` 指定），可同时保存 optimizer 与 epoch 便于恢复。

### 8.3 脚本与入口

- **多模型对比训练**：`scripts/train_multiple_models_comparison.py`，同时训练 LSTM、Transformer、Enhanced Transformer、MoE 等，便于对比。
- MoE 单独训练逻辑在同一脚本或项目内 MoE 相关模块中调用 `MoETradingTransformerWithProfit` 与上述数据接口。

### 8.4 与 LSTM 对比

- 见 [Transformer_vs_LSTM理论分析](../Transformer_vs_LSTM理论分析.md)：Transformer 擅长长程依赖，LSTM 参数量小、小数据更易收敛。

---

## 9. 数据分析设计

### 9.1 特征来源

- **多时间尺度**：1 分钟、5 分钟（及可选 1h/日线）K 线衍生指标（价格、ATR、RSI、BOLL、成交量等），对齐到同一时间轴。详见 [多时间尺度设计方案](../多时间尺度设计方案.md)。
- **维度**：默认 46 维（与 `input_size` 一致），包含价、量、技术指标与可选 Tick 相关字段。见 [训练输入数据说明](../训练输入数据说明.md)。

### 9.2 标签定义

- **动作标签**：根据未来一段时间（如 120 根 K 线）内的涨跌与阈值，定义为 0=不操作、1=买入、2=卖出；考虑持仓状态（当前多/空）与收益阈值、最小价差，避免噪音标签。
- **收益标签**：与动作一致时段内的实际收益率（如做多取区间最大涨幅，做空取区间最大跌幅），用于收益头监督。

### 9.3 验证集与评估

- **验证集**：严格按时间切分，仅用验证集做早停与模型选择，不参与训练。
- **评估指标**：动作准确率、混淆矩阵；收益头 MSE/MAE；可选回测夏普、胜率等（需单独回测脚本）。

### 9.4 数据管道

- 原始 K 线 / Tick → 特征计算与多时间尺度对齐 → 写入 `training_data_multitimeframe_*.csv` 或 merged 文件。
- 训练脚本读取该文件，按 seq_length 滑窗生成 (X, y_action, y_profit)，再 DataLoader 喂给模型。

---

## 10. 主要参数含义

| 参数 | 含义 | 典型值/说明 |
|------|------|-------------|
| input_size | 输入特征维度 | 46（多时间尺度特征） |
| d_model | Transformer 隐藏维度 | 512 |
| num_layers | Transformer 层数 | 8 |
| nhead | 注意力头数 | 8 |
| num_experts | MoE 专家数量 | 8 |
| top_k | 每层激活的专家数 | 2 |
| output_size | 动作类别数 | 3 |
| predict_profit | 是否预测收益率 | True |
| seq_length | 推理/训练序列长度 | 500（需一致） |
| window_size | 局部窗口等 | 20 |
| attention_dropout_rate | 注意力 dropout | 0.1 |

---

## 11. 相关文档

- [MoE和稀疏注意力方案说明](../MoE和稀疏注意力方案说明.md) — MoE 与稀疏注意力原理与实现
- [多时间尺度设计方案](../多时间尺度设计方案.md) — 特征与数据
- [训练输入数据说明](../训练输入数据说明.md) — 训练数据格式
- [Transformer_vs_LSTM理论分析](../Transformer_vs_LSTM理论分析.md) — 与 LSTM 对比