# MoE和稀疏注意力方案说明

**实施时间**: 2026-01-27 12:00  
**方案**: MoE（混合专家模型）+ 稀疏注意力

---

## 一、MoE（混合专家模型）原理

### 1.1 核心思想

**MoE的核心原理**：
- 将单一模型拆分为多个"专家层"（Expert）
- 训练/推理时只激活部分专家（稀疏激活）
- 减少实际参与计算的参数量
- 避免模型过度记忆训练集特征

### 1.2 实现细节

**专家结构**：
- 每个专家是一个独立的FFN（前馈网络）
- 专家数量：8个
- 每次激活的专家数：2个（稀疏激活率：25%）

**门控网络（Gating Network）**：
- 决定激活哪些专家
- 使用softmax计算每个专家的权重
- Top-K选择：选择权重最高的K个专家

**负载均衡损失（Load Balancing Loss）**：
- 鼓励均匀使用所有专家
- 防止某些专家被过度使用，其他专家被忽略
- 损失权重：0.01

### 1.3 优势

✅ **稀疏激活**：
- 每次只激活2/8的专家（25%）
- 减少实际计算量
- 降低过拟合风险

✅ **专家多样性**：
- 不同专家可以学习不同的特征
- 提高模型的表达能力

✅ **保持大容量**：
- 总参数量仍然很大（26.9M+）
- 但实际计算时只使用部分参数

---

## 二、稀疏注意力（Sparse Attention）原理

### 2.1 核心思想

**稀疏注意力的核心原理**：
- 限制每个位置只关注局部窗口内的邻居
- 或随机屏蔽部分注意力头
- 减少注意力计算的复杂度
- 防止注意力过度集中在局部token

### 2.2 实现细节

**局部窗口注意力**：
- 窗口大小：20（每个位置只关注前后20个位置）
- 计算复杂度：O(n×w) 而不是 O(n²)
- 适合时间序列数据（局部相关性更强）

**注意力头Dropout**：
- Dropout率：10%
- 随机屏蔽部分注意力头
- 防止注意力过度集中

### 2.3 优势

✅ **计算效率**：
- 从O(n²)降低到O(n×w)
- 对于长序列更高效

✅ **局部相关性**：
- 时间序列数据通常具有局部相关性
- 局部窗口注意力更符合数据特性

✅ **防止过拟合**：
- 限制注意力范围
- 减少模型对局部特征的过度依赖

---

## 三、MoE + 稀疏注意力组合

### 3.1 架构设计

```
输入 → 输入投影 → 位置编码
  ↓
MoE Transformer编码器（8层）
  ├─ 稀疏多头注意力（窗口=20，头dropout=10%）
  └─ MoE层（8个专家，每次激活2个）
  ↓
稀疏注意力池化
  ↓
分类头 + 收益率头
```

### 3.2 参数统计

**模型结构**：
- 总参数量：~30M（比标准模型略大，因为MoE有多个专家）
- 可训练参数：~30M（全部可训练）
- 实际激活参数：~7.5M（25%稀疏激活）

**数据/参数比例**：
- 数据量：7,886
- 可训练参数：30M
- 比例：1:3,800（仍然较高）
- **但实际激活参数只有7.5M，比例：1:950（更合理）**

### 3.3 训练配置

- **优化器**: AdamW(lr=2e-4, weight_decay=1e-3)
- **学习率调度**: CosineAnnealingLR
- **损失函数**: Label Smoothing (0.2) + 类别权重 + MoE负载均衡损失
- **数据增强**: Mixup (alpha=0.2, 50%概率)
- **MoE配置**:
  - 专家数量: 8
  - 每次激活: 2（稀疏激活率25%）
  - 负载均衡损失权重: 0.01
- **稀疏注意力配置**:
  - 窗口大小: 20
  - 注意力头dropout: 10%

---

## 四、为什么MoE和稀疏注意力可能有效？

### 4.1 理论支持

**MoE的优势**：
- 稀疏激活减少实际计算量
- 专家多样性提高表达能力
- 负载均衡确保所有专家都被使用

**稀疏注意力的优势**：
- 局部窗口更适合时间序列
- 减少计算复杂度
- 防止注意力过度集中

**组合效果**：
- MoE减少FFN的计算量
- 稀疏注意力减少注意力计算量
- 两者结合大幅降低实际激活的参数量

### 4.2 对比其他方案

| 方案 | 总参数 | 实际激活参数 | 数据/激活参数比例 | 方法 |
|------|--------|------------|----------------|------|
| **原始模型** | 26.9M | 26.9M | 1:3,400 | 无优化 |
| **PEFT** | 26.9M | 1.9M | 1:4 | 冻结Transformer |
| **MoE+稀疏注意力** | ~30M | **~7.5M** | **1:950** | **稀疏激活** ⭐ |

### 4.3 适用场景

✅ **适合**：
- 大模型（参数量大）
- 数据量相对较小
- 需要保持模型容量
- 时间序列数据（局部相关性）

❌ **不适合**：
- 小模型（MoE开销大于收益）
- 数据量非常大（不需要稀疏激活）
- 需要全局注意力（不适合局部窗口）

---

## 五、实施细节

### 5.1 MoE实现

```python
class MoELayer(nn.Module):
    def __init__(self, d_model, dim_feedforward, num_experts=8, top_k=2):
        # 8个专家
        self.experts = nn.ModuleList([Expert(...) for _ in range(8)])
        # 门控网络
        self.gate = nn.Linear(d_model, num_experts)
    
    def forward(self, x):
        # 计算门控权重
        gate_logits = self.gate(x)
        # Top-K选择
        top_k_gate_logits, top_k_indices = torch.topk(gate_logits, top_k=2, dim=-1)
        # 稀疏激活
        output = sum(expert(x) * weight for expert, weight in ...)
        # 负载均衡损失
        aux_loss = num_experts * sum(expert_usage ** 2)
        return output, aux_loss
```

### 5.2 稀疏注意力实现

```python
class SparseMultiheadAttention(nn.Module):
    def __init__(self, window_size=20, attention_dropout_rate=0.1):
        self.window_size = window_size
        self.attention_dropout_rate = attention_dropout_rate
    
    def forward(self, query, key, value):
        # 局部窗口掩码
        mask = create_window_mask(seq_len, window_size=20)
        # 应用掩码
        attn_weights = attn_weights.masked_fill(~mask, float('-inf'))
        # 注意力头dropout
        if self.training:
            attn_output = F.dropout(attn_output, p=0.1)
        return attn_output
```

---

## 六、预期效果

### 6.1 理论分析

**MoE的优势**：
- 稀疏激活：每次只激活25%的参数
- 专家多样性：8个专家学习不同特征
- 负载均衡：确保所有专家都被使用

**稀疏注意力的优势**：
- 局部窗口：更适合时间序列
- 计算效率：O(n×w)而不是O(n²)
- 防止过拟合：限制注意力范围

**组合效果**：
- 实际激活参数：~7.5M（25%）
- 数据/激活参数比例：1:950（合理）
- 预期验证准确率：50%+（不再总是预测类别2）

### 6.2 对比其他方案

| 指标 | PEFT | MoE+稀疏注意力 |
|------|------|---------------|
| 总参数 | 26.9M | ~30M |
| 可训练参数 | 1.9M | ~30M |
| 实际激活参数 | 1.9M | **~7.5M** |
| 数据/激活参数比例 | 1:4 | **1:950** |
| 模型容量 | 保持 | **更大** |
| 计算效率 | 高 | **中等** |

---

## 七、后续优化方向

如果MoE+稀疏注意力仍然不够，可以进一步：

1. **增加专家数量**
   - 从8个增加到16个或32个
   - 提高专家多样性

2. **调整稀疏激活率**
   - 从25%（2/8）调整到12.5%（1/8）
   - 进一步减少激活参数

3. **优化窗口大小**
   - 根据数据特性调整窗口大小
   - 或使用自适应窗口

4. **结合PEFT**
   - MoE+稀疏注意力+PEFT
   - 进一步减少可训练参数

---

**报告生成时间**: 2026-01-27 12:00  
**状态**: MoE+稀疏注意力方案已实施，训练进行中
