# 多头注意力与代码刷新说明

> 根据策略设计文档与「多头注意力建模金融数据」参考，对 MoE/Transformer 相关代码做了刷新；测试已通过。**重大算法变更已纳入算法版本管理**，便于多版本对比（见 [algorithm_versions.md](algorithm_versions.md)）。

---

## 一、本次代码刷新（对齐设计文档与参考）

### 1. 因果掩码（防未来信息泄露）

- **位置**：`src/strategies/moe_transformer.py` — `SparseMultiheadAttention`。
- **逻辑**：编码器自注意力默认 `causal_mask=True`，构建 (query_len, key_len) 掩码，位置 i 仅可关注 j≤i；与 `window_size` 同时使用时，仅可关注过去窗口内位置。
- **价值**：严格避免未来信息泄露，满足金融回测与实盘时序一致性。

### 2. 滚动窗口归一化（数据管道可选）

- **位置**：`src/data_processor/normalizer.py` — `DataNormalizer.fit_transform_rolling(df, feature_cols, window=60)`。
- **逻辑**：按列做滚动 Z-score，`(x - rolling_mean) / (rolling_std + 1e-8)`，避免全局归一化带来的未来信息泄露。
- **使用**：训练数据预处理时可按需调用，与现有 `fit_transform`（全局）二选一或组合。

### 3. MoE 训练使用 Focal Loss（类别不平衡）

- **位置**：`scripts/train_multiple_models_comparison.py` — `train_moe_transformer_model`。
- **逻辑**：`use_focal_loss=True` 时，动作分类损失使用 `FocalLoss(alpha=class_weights, gamma=2.0)`，缓解涨/跌/平样本不平衡。
- **参考**：设计文档与「金融数据类别不平衡，使用 Focal Loss 替代交叉熵」一致。

---

## 二、测试与例行

- **单元/集成**：`SparseMultiheadAttention` 与 `MoETradingTransformerWithProfit` 前向通过；`tests/test_feature_strategy_prediction.py`、`tests/test_run_moe_demo_integration.py` 共 11 个用例通过。
- **例行**：`python scripts/optimize_algorithm_and_profitability.py` 已跑通，策略表现与今日收益率、报告已刷新。

---

## 三、效果对比说明

- **代码层面**：因果掩码、Focal Loss、滚动归一化接口已就绪，与设计文档及多头注意力参考一致。
- **版本管理**：当前为 **v2.0**（`src/algorithm_version.py`）。训练会保存 `best_moe_transformer_v2.0.pth` 与 `best_moe_transformer.pth`，报告与对比页会显示 `algorithm_version`，便于与 v1.0 等历史版本对比。
- **数值效果**：在相同数据与划分下跑训练，对比各版本的验证准确率、收益头 MAE、回测收益/夏普；历史版本通过加载对应 `*_vX.X.pth` 做对比。

---

## 四、参考

- 策略设计文档：`docs/strategy_designs/设计_MoE策略.md`、`设计_LSTM策略.md`
- 多头注意力参考：用户提供的「多头注意力机制核心逻辑、金融数据预处理→编码→多头注意力层→特征融合→下游任务、时序掩码与 Focal Loss」
