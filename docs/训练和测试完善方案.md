# 训练和测试完善方案

**设计时间**: 2026-01-23  
**目标**: 完善训练逻辑，改进测试方法，公平对比LSTM和Transformer

---

## 🔍 一、当前问题分析

### 1.1 训练逻辑问题

**问题1: 网格调整系数未训练**
- 模型输出网格调整系数，但损失函数只包含动作分类损失
- 网格调整系数没有标签，无法训练

**问题2: 损失函数不完整**
- 只有动作分类损失（CrossEntropyLoss）
- 缺少网格调整系数回归损失

**问题3: 训练轮次可能不足**
- 当前只有20轮
- Transformer可能需要更多轮次才能收敛

### 1.2 测试和评估问题

**问题1: 准确率定义不合理**
- 当前：准确率 = 预测动作 == 标签
- 问题：标签本身可能不准确（不考虑网格参数）
- 高准确率（99.42%）但低收益（2.3%）

**问题2: 缺少收益加权准确率**
- 应该使用实际收益而不是动作匹配
- `accuracy = predicted_profit / best_profit`

**问题3: 对比测试不公平**
- LSTM和Transformer使用不同的训练配置
- 序列长度、训练轮次、数据可能不同

---

## 🔧 二、完善方案

### 2.1 完善训练逻辑

#### 2.1.1 支持网格调整系数训练

**方案**:
1. 计算最优网格调整系数（基于历史数据）
2. 生成网格调整系数标签
3. 添加回归损失（MSELoss）

**代码**:
```python
# 计算最优网格调整系数
def calculate_optimal_grid_adjustment(current_price, future_prices, grid_base):
    """基于历史数据计算最优网格调整系数"""
    # 尝试不同的调整系数，找到收益最大的
    best_adjustment = 1.0
    best_profit = 0.0
    
    for adjustment in np.arange(0.8, 1.2, 0.05):
        grid_step = grid_base * adjustment
        # 计算在此网格参数下的收益
        profit = calculate_profit_with_grid(current_price, future_prices, grid_step)
        if profit > best_profit:
            best_profit = profit
            best_adjustment = adjustment
    
    return best_adjustment

# 修改损失函数
action_loss = CrossEntropyLoss(action_logits, action_labels)
grid_loss = MSELoss(grid_adjustment, optimal_adjustment)
total_loss = action_loss + 0.1 * grid_loss  # 权重可调
```

#### 2.1.2 增加训练轮次和早停

**方案**:
- 增加训练轮次到50-100
- 使用早停机制（patience=10）
- 使用学习率调度

**代码**:
```python
from torch.optim.lr_scheduler import ReduceLROnPlateau

scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)

best_val_acc = 0.0
patience_counter = 0
max_epochs = 100
patience = 10

for epoch in range(max_epochs):
    # 训练...
    # 验证...
    
    scheduler.step(val_accuracy)
    
    if val_accuracy > best_val_acc:
        best_val_acc = val_accuracy
        patience_counter = 0
        # 保存最佳模型
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"早停于第 {epoch+1} 轮")
            break
```

### 2.2 改进测试和评估

#### 2.2.1 实现收益加权准确率

**代码**:
```python
def calculate_profit_based_accuracy(predictions, labels, prices, grid_params, look_ahead=10):
    """计算基于收益的准确率"""
    total_profit = 0.0
    max_possible_profit = 0.0
    
    for i in range(len(predictions)):
        if i + look_ahead >= len(prices):
            break
        
        current_price = prices[i]
        future_prices = prices[i+1:i+look_ahead+1]
        
        # 计算所有动作的收益
        profits = {
            0: 0.0,  # 不操作
            1: (max(future_prices) - current_price) / current_price,  # 买入
            2: (current_price - min(future_prices)) / current_price   # 卖出
        }
        
        # 最优动作的收益
        best_action = max(profits, key=profits.get)
        max_possible_profit += profits[best_action]
        
        # 预测动作的收益
        predicted_action = predictions[i]
        total_profit += profits[predicted_action]
    
    # 收益加权准确率
    if max_possible_profit > 0:
        profit_accuracy = total_profit / max_possible_profit
    else:
        profit_accuracy = 0.0
    
    return profit_accuracy
```

#### 2.2.2 公平对比测试

**方案**:
- 使用相同的数据
- 使用相同的训练配置（轮次、学习率、批次大小）
- 使用相同的评估指标

**代码**: 见 `fair_model_comparison.py`

---

## 📊 三、Transformer vs LSTM 理论分析

### 3.1 理论上Transformer应该更好

**原因**:
1. **自注意力机制**: 可以同时关注序列中的所有位置
2. **长距离依赖**: 直接建模任意距离的依赖关系
3. **并行计算**: 训练速度快
4. **表达能力**: 可以学习更复杂的模式

### 3.2 但实际结果受限于

**数据量**:
- Transformer需要大量数据（百万级）
- 当前数据量可能不足（几万条）

**序列长度**:
- Transformer优势在长序列上
- 当前使用短序列（10），优势不明显

**训练方式**:
- 可能训练不充分（20轮不够）
- 可能过拟合（模型太大）

**特征工程**:
- 当前使用计算好的特征
- Transformer更适合原始数据

---

## 🎯 四、实施计划

### 4.1 立即实施

1. **完善训练逻辑**
   - 支持网格调整系数训练
   - 增加训练轮次和早停
   - 使用学习率调度

2. **改进评估指标**
   - 实现收益加权准确率
   - 使用实际收益而不是动作匹配

### 4.2 短期实施

1. **公平对比测试**
   - 实现 `fair_model_comparison.py`
   - 在相同条件下对比LSTM和Transformer

2. **增加数据量**
   - 收集更多历史数据
   - 使用数据增强

### 4.3 长期优化

1. **使用更长序列**
   - 测试序列长度50-200
   - 但需要更多数据支持

2. **改进Transformer训练**
   - 使用预训练模型
   - 改进正则化

---

## ✅ 五、预期效果

### 5.1 训练改进

- ✅ 网格调整系数可以训练
- ✅ 训练更充分（更多轮次，早停）
- ✅ 更好的收敛（学习率调度）

### 5.2 评估改进

- ✅ 更准确的性能评估（收益加权准确率）
- ✅ 公平的模型对比
- ✅ 更可靠的结论

### 5.3 理论验证

- ✅ 验证Transformer是否真的更好
- ✅ 找到最优的序列长度
- ✅ 找到最优的模型配置

---

**状态**: 方案设计完成，准备实施
