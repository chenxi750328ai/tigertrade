# 回测原理与模型分析报告

**日期**: 2026-01-23  
**分析内容**: 回测原理、模型架构、小模型表现更好的原因

---

## 📊 一、回测原理分析

### 1.1 回测方法：事件驱动回测

基于 `backtest_grid_trading_strategy_pro1` 函数的分析：

#### 核心流程

```
1. 获取历史K线数据（1分钟和5分钟）
   ↓
2. 遍历每个时间点（从第i个数据点开始）
   ↓
3. 使用历史数据（到当前时间点）计算技术指标
   - 布林带、ATR、RSI等
   - 判断市场趋势
   - 计算网格上下轨
   ↓
4. 检查是否触发买入信号
   - near_lower: 价格接近网格下轨
   - rsi_ok: RSI满足条件
   - trend_check/rebound/vol_ok: 趋势/反弹/成交量确认
   ↓
5. 如果触发买入信号：
   - 设置止盈目标：grid_upper - tp_offset
   - 设置止损价格：compute_stop_loss()
   ↓
6. 向前扫描（lookahead个时间步，默认120分钟）
   - 如果价格先触及止损 → 记录为亏损
   - 如果价格先触及止盈 → 记录为盈利
   - 如果120分钟内都未触及 → 记录为未判定
   ↓
7. 跳过已评估的时间窗口，继续下一个时间点
   ↓
8. 统计结果：
   - 胜率 = wins / (wins + losses)
   - 平均盈亏比 = 平均盈利 / 平均亏损
   - 期望值 = 胜率 × 平均盈亏比 - (1-胜率) × 1.0
```

#### 关键特点

1. **时间点隔离**: 每个时间点只使用该时间点之前的历史数据，避免未来信息泄漏
2. **向前扫描**: 使用`lookahead`参数向前看，模拟实际交易中的止盈止损
3. **事件驱动**: 只在触发买入信号时才评估，不是每个时间点都交易
4. **避免重叠**: 评估后跳过`lookahead`个时间步，避免重叠交易

#### 代码关键片段

```python
# 1. 遍历历史数据
i = max(GRID_BOLL_PERIOD, 10)
while i < len(df_1m) - 1:
    # 2. 使用历史数据（到当前时间点）
    sub1 = df_1m.iloc[:i+1]  # 只使用i之前的数据
    sub5 = df_5m[df_5m.index <= t_cur]
    
    # 3. 计算指标和判断信号
    inds = calculate_indicators(sub1, sub5)
    buy_signal = near_lower and rsi_ok and ...
    
    # 4. 如果触发信号，向前扫描评估结果
    if buy_signal:
        forward = df_1m.iloc[i+1:min(i+1+lookahead, len(df_1m))]
        for _, row in forward.iterrows():
            if row['low'] <= stop:
                outcome = 'loss'
                break
            if row['high'] >= target:
                outcome = 'win'
                break
    
    # 5. 跳过已评估的窗口
    i += lookahead
```

### 1.2 回测 vs 实际交易的区别

| 项目 | 回测 | 实际交易 |
|------|------|----------|
| **数据使用** | 只使用历史数据到当前时间点 | 实时数据 |
| **执行假设** | 假设可以按目标价成交 | 需要考虑滑点和流动性 |
| **成本** | 可能不考虑或简化手续费 | 实际手续费和滑点 |
| **止盈止损** | 假设完美执行 | 可能因滑点提前触发 |
| **市场影响** | 不考虑订单对市场的影响 | 大单可能影响价格 |

**关键发现**: 回测收益率（+2.3%）低于训练准确率（99%+）的原因：
1. **滑点成本**: 实际交易中无法按目标价完美成交
2. **手续费**: 频繁交易导致手续费累积
3. **执行延迟**: 信号产生到订单执行有时间差
4. **流动性**: 某些时段流动性不足，无法按目标价成交

---

## 🤖 二、模型架构分析

### 2.1 模型输入特征

所有模型都使用相同的12个特征：

```python
features = [
    price_current,      # 当前价格
    atr,               # ATR指标
    rsi_1m,            # 1分钟RSI
    rsi_5m,            # 5分钟RSI
    boll_upper,        # 布林带上轨
    boll_mid,          # 布林带中轨
    boll_lower,        # 布林带下轨
    boll_position,     # 价格在布林带中的位置
    price_change_1,    # 1分钟价格变化
    price_change_5,    # 5分钟价格变化
    volatility,        # 波动率
    volume_1m          # 1分钟成交量
]
```

### 2.2 关键发现：序列长度问题

#### LSTM模型

```python
# llm_strategy.py
def forward(self, x):
    if len(x.shape) == 2:
        x = x.unsqueeze(1)  # 添加序列维度 (batch, seq, features)
    # x shape: (batch, 1, 12)  ← 序列长度为1！
    
    out, _ = self.lstm(x, (h0, c0))
    out = out[:, -1, :]  # 取最后一个时间步（也就是唯一的时间步）
```

**问题**: LSTM的序列长度实际上是**1**，只看到当前时刻的数据！

#### Transformer模型

```python
# model_comparison_strategy.py
def predict_both_models(self, current_data):
    features = self.prepare_features(current_data)  # 单个时间点的特征
    input_tensor = torch.tensor([features], dtype=torch.float32).unsqueeze(1)
    # input_tensor shape: (1, 1, 12)  ← 序列长度也是1！
    
    transformer_prediction = self.transformer_model(input_tensor)
```

**问题**: Transformer虽然设计了位置编码（最大长度100或1000），但实际输入序列长度也是**1**！

### 2.3 模型架构对比

| 模型 | 参数量 | 序列长度 | 隐藏层 | 层数 | 验证准确率 |
|------|--------|----------|--------|------|------------|
| **对比模型-LSTM** | 53K | **1** | 64 | 2 | **99.42%** |
| **LSTM模型** | 53K | **1** | 64 | 2 | 99.13% |
| **大型LSTM模型** | 2M | **1** | 256 | 4 | 98.84% |
| **对比模型-Transformer** | 563K | **1** | 64 | 2 | 99.13% |
| **大型Transformer** | 4.8M | **1** | 256 | 6 | 65.70% |
| **超大Transformer** | 25M | **1** | 512 | 8 | 32.85% |

**关键发现**: 所有模型的序列长度都是**1**，没有使用历史序列数据！

---

## 🔍 三、为什么小模型表现更好？

### 3.1 任务复杂度 vs 模型复杂度

#### 实际任务

- **输入**: 12个当前时刻的特征（价格、指标等）
- **输出**: 3分类（买入/卖出/持有）
- **任务类型**: 简单的特征分类任务

#### 模型复杂度

- **小模型（LSTM, 53K参数）**: 
  - 2层LSTM + 1层全连接
  - 参数量与任务复杂度匹配 ✅
  
- **大模型（Transformer, 25M参数）**:
  - 8层Transformer + 多层分类头
  - 参数量是任务复杂度的**470倍** ❌

### 3.2 过拟合分析

#### 训练数据量

根据代码分析，训练数据量约为：
- 训练集: 约800条（80%）
- 验证集: 约200条（20%）
- **总数据量: 约1000条**

#### 参数量 vs 数据量

| 模型 | 参数量 | 数据量 | 参数量/数据量 | 过拟合风险 |
|------|--------|--------|---------------|------------|
| LSTM | 53K | 1000 | 53 | 中等 |
| 大型LSTM | 2M | 1000 | 2000 | **极高** |
| Transformer | 563K | 1000 | 563 | 高 |
| 超大Transformer | 25M | 1000 | 25000 | **极高** |

**结论**: 大模型的参数量远超数据量，容易过拟合训练数据，泛化能力差。

### 3.3 序列长度的影响

#### 当前实现的问题

```python
# 所有模型都只使用当前时刻的特征
features = prepare_features(current_row)  # 单个时间点
input_tensor = torch.tensor([features]).unsqueeze(1)  # (1, 1, 12)
```

**问题**: 
- LSTM和Transformer都设计为处理序列数据
- 但实际输入序列长度为1，没有利用历史信息
- Transformer的位置编码和注意力机制完全浪费

#### 应该如何使用历史数据

**正确的实现方式**:

```python
# 使用滑动窗口构建序列
def prepare_sequence(df, current_idx, window_size=20):
    """准备历史序列数据"""
    start_idx = max(0, current_idx - window_size)
    sequence = df.iloc[start_idx:current_idx+1]
    features = [prepare_features(row) for _, row in sequence.iterrows()]
    return np.array(features)  # (window_size, 12)

# 输入序列
sequence = prepare_sequence(df, i, window_size=20)  # (20, 12)
input_tensor = torch.tensor([sequence])  # (1, 20, 12)
```

**效果**:
- LSTM可以学习历史价格模式
- Transformer可以捕捉长期依赖关系
- 模型才能真正发挥序列建模的优势

### 3.4 为什么小模型在序列长度为1时表现好？

#### 1. 任务本质是特征分类

当序列长度为1时，任务实际上变成了：
- **输入**: 12维特征向量
- **输出**: 3分类
- **本质**: 简单的特征分类任务

#### 2. 小模型适合简单任务

- **LSTM (53K参数)**: 
  - 2层LSTM可以学习简单的特征组合
  - 参数量适中，不容易过拟合
  - 适合当前任务复杂度 ✅

#### 3. 大模型过度复杂

- **Transformer (25M参数)**:
  - 8层Transformer + 注意力机制
  - 设计用于处理长序列和复杂依赖
  - 但序列长度为1，无法发挥优势
  - 参数量过大，容易过拟合 ❌

### 3.5 训练准确率 vs 回测收益率差异

#### 训练准确率（99%+）

- **计算方式**: 预测动作与标签是否一致
- **标签生成**: 基于未来10步的价格变化
- **问题**: 
  - 标签可能包含未来信息（数据泄漏）
  - 不考虑滑点和手续费
  - 不考虑实际执行难度

#### 回测收益率（+2.3%）

- **计算方式**: 实际模拟交易，考虑止盈止损
- **更真实**: 
  - 使用历史数据到当前时间点
  - 考虑止盈止损执行
  - 但可能仍简化了滑点和手续费

#### 差异原因

1. **数据泄漏**: 训练标签可能包含未来信息
2. **执行成本**: 实际交易有滑点和手续费
3. **信号质量**: 高准确率不等于高收益
4. **交易频率**: 频繁交易导致成本累积

---

## 💡 四、改进建议

### 4.1 使用历史序列数据

#### 实现滑动窗口

```python
def prepare_sequence_features(df, current_idx, window_size=20):
    """准备历史序列特征"""
    start_idx = max(0, current_idx - window_size + 1)
    sequence_df = df.iloc[start_idx:current_idx+1]
    
    sequences = []
    for _, row in sequence_df.iterrows():
        features = prepare_features(row)
        sequences.append(features)
    
    # 如果序列不足window_size，用第一个值填充
    while len(sequences) < window_size:
        sequences.insert(0, sequences[0] if sequences else [0]*12)
    
    return np.array(sequences)  # (window_size, 12)
```

#### 修改模型输入

```python
# 在训练和预测时使用序列
sequence = prepare_sequence_features(df, i, window_size=20)
input_tensor = torch.tensor([sequence])  # (1, 20, 12)

# LSTM和Transformer可以真正利用历史信息
prediction = model(input_tensor)
```

### 4.2 调整模型复杂度

#### 根据数据量选择模型

- **数据量 < 5000**: 使用小模型（LSTM, 53K参数）
- **数据量 5000-20000**: 使用中等模型（LSTM, 200K参数）
- **数据量 > 20000**: 可以使用大模型（Transformer）

#### 序列长度建议

- **LSTM**: 序列长度 10-30（短期模式）
- **Transformer**: 序列长度 20-100（长期依赖）

### 4.3 改进标签生成

#### 避免数据泄漏

```python
# 当前方式（可能有数据泄漏）
future_prices = df.iloc[i+1:i+look_ahead+1]['price_current'].values
buy_profit = (max(future_prices) - current_price) / current_price

# 改进方式（考虑滑点和手续费）
def calculate_real_profit(current_price, future_prices, slippage=0.001, fee=0.0002):
    """计算考虑滑点和手续费的盈利"""
    buy_price = current_price * (1 + slippage)  # 买入滑点
    sell_price = max(future_prices) * (1 - slippage)  # 卖出滑点
    profit = (sell_price - buy_price) / buy_price - 2 * fee  # 扣除手续费
    return profit
```

### 4.4 改进回测方法

#### 考虑滑点和手续费

```python
def backtest_with_slippage(...):
    """考虑滑点的回测"""
    # 买入价格 = 目标价 + 滑点
    buy_price = target_price * (1 + buy_slippage)
    
    # 止盈价格 = 目标价 - 滑点
    take_profit_price = target * (1 - sell_slippage)
    
    # 止损价格 = 止损价 + 滑点（更严格）
    stop_loss_price = stop * (1 + sell_slippage)
    
    # 计算实际盈亏（扣除手续费）
    if outcome == 'win':
        profit = (take_profit_price - buy_price) * quantity - 2 * fee
    else:
        loss = (buy_price - stop_loss_price) * quantity + 2 * fee
```

---

## 📊 五、总结

### 5.1 回测原理

- **方法**: 事件驱动回测，使用历史数据到当前时间点
- **评估**: 向前扫描，检查止盈止损是否触发
- **特点**: 避免未来信息泄漏，但可能简化执行成本

### 5.2 模型表现差异的原因

1. **序列长度为1**: 所有模型都没有使用历史序列数据
2. **任务简单**: 实际是12维特征分类，不需要复杂模型
3. **过拟合**: 大模型参数量远超数据量，容易过拟合
4. **模型设计不匹配**: Transformer设计用于长序列，但输入序列长度为1

### 5.3 为什么小模型表现好

- **复杂度匹配**: 小模型复杂度与任务复杂度匹配
- **不易过拟合**: 参数量适中，泛化能力好
- **适合简单任务**: 12维特征分类不需要复杂模型

### 5.4 改进方向

1. **使用历史序列**: 实现滑动窗口，真正利用历史数据
2. **调整模型复杂度**: 根据数据量选择合适的模型大小
3. **改进标签生成**: 考虑滑点和手续费，避免数据泄漏
4. **改进回测方法**: 更真实地模拟实际交易成本

---

**关键结论**: 
- 当前所有模型的序列长度都是1，没有利用历史信息
- 小模型表现好是因为任务简单，不需要复杂模型
- 要真正发挥LSTM和Transformer的优势，需要使用历史序列数据
- 训练准确率高但回测收益率低，主要是因为执行成本和数据泄漏
