# ä¸ºä»€ä¹ˆåº”è¯¥ç”¨ç«¯åˆ°ç«¯å­¦ä¹ ä»£æ›¿æ‰‹å·¥ç‰¹å¾å·¥ç¨‹

**æ—¥æœŸ**: 2026-01-20  
**èƒŒæ™¯**: Tiger Tradingé¡¹ç›®ä¸­æ‰‹å·¥ç‰¹å¾å·¥ç¨‹å®Œå…¨å¤±è´¥çš„åæ€

---

## ðŸ¤” æ ¸å¿ƒé—®é¢˜

**ç”¨æˆ·æé—®**:
> ä¸ºä½•è¦æ‰‹å·¥å®žçŽ°ç‰¹å¾å·¥ç¨‹ï¼Œä¸ºä½•ä¸è®©æ¨¡åž‹è‡ªå·±è¯†åˆ«ï¼Ÿ

**è¿™æ˜¯ä¸€ä¸ªæ·±åˆ»çš„é—®é¢˜ï¼**

---

## ðŸ“Š æ‰‹å·¥ç‰¹å¾å·¥ç¨‹ vs ç«¯åˆ°ç«¯å­¦ä¹ 

### æ‰‹å·¥ç‰¹å¾å·¥ç¨‹æ–¹æ³•ï¼ˆå½“å‰ï¼‰

**æµç¨‹**:
```
åŽŸå§‹æ•°æ® (OHLCV) 
    â†“ 
äººå·¥è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    â†“
RSI, MACD, BOLL, ATR, æ³¢åŠ¨çŽ‡ç­‰
    â†“
æ¨¡åž‹è¾“å…¥ (12ä¸ªç‰¹å¾)
    â†“
é¢„æµ‹
```

**ä¼˜ç‚¹**:
- âœ… åˆ©ç”¨é¢†åŸŸçŸ¥è¯†ï¼ˆæŠ€æœ¯åˆ†æžç†è®ºï¼‰
- âœ… ç‰¹å¾æœ‰æ˜Žç¡®å«ä¹‰ï¼ˆå¯è§£é‡Šæ€§ï¼‰
- âœ… ç‰¹å¾æ•°é‡å°‘ï¼ˆè®¡ç®—å¿«ï¼‰

**ç¼ºç‚¹**:
- âŒ **å®¹æ˜“å‡ºé”™**ï¼ˆå¦‚æœ¬æ¡ˆä¾‹ï¼Œ10/12ç‰¹å¾å¤±è´¥ï¼‰
- âŒ **ç»´æŠ¤æˆæœ¬é«˜**ï¼ˆæ¯ä¸ªæŒ‡æ ‡éƒ½è¦éªŒè¯ã€è°ƒè¯•ï¼‰
- âŒ **å›ºå®šæ¨¡å¼**ï¼ˆæ— æ³•é€‚åº”å¸‚åœºå˜åŒ–ï¼‰
- âŒ **å¯èƒ½é—æ¼é‡è¦ä¿¡æ¯**ï¼ˆäººç±»è®¤çŸ¥æœ‰é™ï¼‰
- âŒ **éœ€è¦ä¸“ä¸šçŸ¥è¯†**ï¼ˆä¸æ˜¯æ¯ä¸ªå¼€å‘è€…éƒ½æ‡‚æŠ€æœ¯åˆ†æžï¼‰

### ç«¯åˆ°ç«¯å­¦ä¹ æ–¹æ³•ï¼ˆæŽ¨èï¼‰

**æµç¨‹**:
```
åŽŸå§‹æ•°æ® (OHLCVæ—¶åº)
    â†“
æ·±åº¦å­¦ä¹ æ¨¡åž‹ï¼ˆTransformer/LSTMï¼‰
    â†“
è‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤º
    â†“
é¢„æµ‹
```

**ä¼˜ç‚¹**:
- âœ… **è‡ªåŠ¨ç‰¹å¾å­¦ä¹ **ï¼ˆæ— éœ€äººå·¥è®¡ç®—ï¼‰
- âœ… **å‘çŽ°æœªçŸ¥æ¨¡å¼**ï¼ˆå¯èƒ½è¶…è¶Šäººç±»è®¤çŸ¥ï¼‰
- âœ… **é€‚åº”æ€§å¼º**ï¼ˆéšæ•°æ®è‡ªåŠ¨è°ƒæ•´ï¼‰
- âœ… **å‡å°‘é”™è¯¯**ï¼ˆæ— éœ€ç»´æŠ¤å¤æ‚çš„ç‰¹å¾å·¥ç¨‹ä»£ç ï¼‰
- âœ… **æ˜“äºŽç»´æŠ¤**ï¼ˆåªéœ€ç»´æŠ¤æ¨¡åž‹ç»“æž„ï¼‰

**ç¼ºç‚¹**:
- âŒ éœ€è¦æ›´å¤šæ•°æ®
- âŒ è®¡ç®—æˆæœ¬è¾ƒé«˜
- âŒ å¯è§£é‡Šæ€§è¾ƒå·®ï¼ˆé»‘ç›’ï¼‰

---

## ðŸ’¡ æœ¬æ¡ˆä¾‹çš„æ•™è®­

### æ‰‹å·¥ç‰¹å¾å·¥ç¨‹å¤±è´¥çš„æ ¹æœ¬åŽŸå› 

1. **è®¡ç®—é”™è¯¯**:
   ```python
   # é¢„æœŸï¼šè®¡ç®—ä»·æ ¼å˜åŒ–çŽ‡
   price_change_1 = (current - prev) / prev * 100
   
   # å®žé™…ï¼šå…¨æ˜¯0
   price_change_1 = 0  # âŒ Bug!
   ```

2. **æ•°æ®ä¾èµ–**:
   - RSIéœ€è¦è¶³å¤Ÿçš„åŽ†å²æ•°æ®
   - BOLLéœ€è¦æ»šåŠ¨çª—å£
   - ä¸€æ—¦æ•°æ®ä¸è¶³æˆ–æ ¼å¼é”™è¯¯ï¼Œå…¨éƒ¨å¤±è´¥

3. **ç»´æŠ¤å™©æ¢¦**:
   - 12ä¸ªç‰¹å¾
   - æ¯ä¸ªæœ‰è‡ªå·±çš„è®¡ç®—é€»è¾‘
   - æ¯ä¸ªéƒ½å¯èƒ½å‡ºé”™
   - è°ƒè¯•å›°éš¾

### å¦‚æžœç”¨ç«¯åˆ°ç«¯å­¦ä¹ 

```python
# ç®€å•ç›´æŽ¥
input_data = df[['open', 'high', 'low', 'close', 'volume']].values
# shape: (samples, seq_len, 5)

model = TransformerModel(
    input_dim=5,  # OHLCV
    seq_len=100,   # ä½¿ç”¨100ä¸ªæ—¶é—´æ­¥
    d_model=128,
    nhead=8,
    num_layers=4
)

# æ¨¡åž‹è‡ªåŠ¨å­¦ä¹ ï¼š
# - ä»·æ ¼è¶‹åŠ¿
# - æ³¢åŠ¨æ¨¡å¼
# - æˆäº¤é‡å…³ç³»
# - æ—¶é—´ä¾èµ–
# - å…¶ä»–æœªçŸ¥æ¨¡å¼
```

**ä¼˜åŠ¿**:
- âœ… æ— éœ€æ‰‹å·¥è®¡ç®—RSIã€BOLLç­‰
- âœ… æ¨¡åž‹è‡ªå·±å‘çŽ°æœ‰ç”¨çš„æ¨¡å¼
- âœ… ä»£ç ç®€å•ï¼Œä¸æ˜“å‡ºé”™
- âœ… è‡ªåŠ¨é€‚åº”ä¸åŒå¸‚åœºçŠ¶æ€

---

## ðŸ—ï¸ ç«¯åˆ°ç«¯å­¦ä¹ æž¶æž„è®¾è®¡

### æ–¹æ¡ˆ1: Temporal Fusion Transformer

```python
class TemporalFusionTransformer(nn.Module):
    """
    æ—¶åºèžåˆTransformer
    ç›´æŽ¥å¤„ç†åŽŸå§‹OHLCVæ•°æ®
    """
    def __init__(
        self,
        input_dim=5,      # OHLCV + Volume
        seq_len=100,       # åºåˆ—é•¿åº¦
        d_model=128,       # æ¨¡åž‹ç»´åº¦
        nhead=8,           # æ³¨æ„åŠ›å¤´æ•°
        num_layers=4,      # Transformerå±‚æ•°
        num_classes=3      # ä¹°å…¥/æŒæœ‰/å–å‡º
    ):
        super().__init__()
        
        # è¾“å…¥åµŒå…¥
        self.embedding = nn.Linear(input_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_encoding = PositionalEncoding(d_model, seq_len)
        
        # Transformerç¼–ç å™¨
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=0.1,
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )
        
        # è¾“å‡ºå±‚
        self.fc = nn.Sequential(
            nn.Linear(d_model, d_model // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(d_model // 2, num_classes)
        )
    
    def forward(self, x):
        # x: (batch, seq_len, 5)  # OHLCV + Volume
        
        # åµŒå…¥ + ä½ç½®ç¼–ç 
        x = self.embedding(x)  # (batch, seq_len, d_model)
        x = self.pos_encoding(x)
        
        # Transformerç¼–ç 
        x = self.transformer(x)  # (batch, seq_len, d_model)
        
        # ä½¿ç”¨æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥
        x = x[:, -1, :]  # (batch, d_model)
        
        # åˆ†ç±»
        out = self.fc(x)  # (batch, num_classes)
        
        return out
```

**ä½¿ç”¨**:
```python
# æ•°æ®å‡†å¤‡
def prepare_sequences(df, seq_len=100):
    """
    ç›´æŽ¥ä½¿ç”¨åŽŸå§‹OHLCVæ•°æ®
    æ— éœ€æ‰‹å·¥ç‰¹å¾å·¥ç¨‹
    """
    sequences = []
    labels = []
    
    for i in range(len(df) - seq_len):
        # å–seq_lenä¸ªæ—¶é—´æ­¥çš„OHLCVæ•°æ®
        seq = df.iloc[i:i+seq_len][
            ['open', 'high', 'low', 'close', 'volume']
        ].values
        
        # æ ‡ç­¾ï¼ˆä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„æ–¹å‘ï¼‰
        label = df.iloc[i+seq_len]['label']
        
        sequences.append(seq)
        labels.append(label)
    
    return np.array(sequences), np.array(labels)

# è®­ç»ƒ
X, y = prepare_sequences(df, seq_len=100)
# X: (samples, 100, 5)
# y: (samples,)

model = TemporalFusionTransformer()
model.fit(X, y)
```

### æ–¹æ¡ˆ2: LSTM + Attention

```python
class LSTMAttention(nn.Module):
    """
    LSTM + æ³¨æ„åŠ›æœºåˆ¶
    ç«¯åˆ°ç«¯å­¦ä¹ æ—¶åºæ¨¡å¼
    """
    def __init__(
        self,
        input_dim=5,
        hidden_dim=128,
        num_layers=3,
        num_classes=3
    ):
        super().__init__()
        
        # LSTM
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.2
        )
        
        # æ³¨æ„åŠ›å±‚
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            batch_first=True
        )
        
        # è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        # x: (batch, seq_len, 5)
        
        # LSTMç¼–ç 
        lstm_out, _ = self.lstm(x)  # (batch, seq_len, hidden_dim)
        
        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.attention(
            lstm_out, lstm_out, lstm_out
        )  # (batch, seq_len, hidden_dim)
        
        # ä½¿ç”¨æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥
        x = attn_out[:, -1, :]
        
        # åˆ†ç±»
        out = self.fc(x)
        
        return out
```

---

## ðŸ“ˆ é¢„æœŸæ•ˆæžœå¯¹æ¯”

### æ‰‹å·¥ç‰¹å¾å·¥ç¨‹ï¼ˆå½“å‰å¤±è´¥ï¼‰

```
è¾“å…¥: 12ä¸ªæ‰‹å·¥ç‰¹å¾
ç»“æžœ: 
  - 10ä¸ªç‰¹å¾æ— æ•ˆï¼ˆå…¨0æˆ–å¸¸é‡ï¼‰
  - åªç”¨price_currentä¸€ä¸ªæ•°å­—
  - è™šå‡å‡†ç¡®çŽ‡98%ï¼ˆå®žé™…æ— ç”¨ï¼‰
```

### ç«¯åˆ°ç«¯å­¦ä¹ ï¼ˆé¢„æœŸï¼‰

```
è¾“å…¥: 100ä¸ªæ—¶é—´æ­¥ Ã— 5ä¸ªåŽŸå§‹ç‰¹å¾ (OHLCV)
æ¨¡åž‹è‡ªåŠ¨å­¦ä¹ :
  - ä»·æ ¼è¶‹åŠ¿ï¼ˆä¸Šå‡/ä¸‹é™/éœ‡è¡ï¼‰
  - æ”¯æ’‘/é˜»åŠ›ä½
  - æˆäº¤é‡æ¨¡å¼
  - æ—¶é—´ä¾èµ–å…³ç³»
  - å¯èƒ½çš„å‘¨æœŸæ€§
  - å…¶ä»–æœªçŸ¥æ¨¡å¼

é¢„æœŸå‡†ç¡®çŽ‡: 50-70%ï¼ˆçœŸå®žæœ‰æ•ˆï¼‰
```

---

## ðŸŽ¯ å®žæ–½å»ºè®®

### ç«‹å³è¡ŒåŠ¨

1. **æ•°æ®å‡†å¤‡**:
   ```python
   # åªéœ€è¦åŽŸå§‹OHLCVæ•°æ®
   # æ— éœ€è®¡ç®—ä»»ä½•æŠ€æœ¯æŒ‡æ ‡
   df = get_ohlcv_data(symbol='SIL2603', days=90)
   ```

2. **ç®€å•æ¨¡åž‹**:
   ```python
   # ä»Žç®€å•çš„LSTMå¼€å§‹
   model = SimpleLSTM(
       input_dim=5,  # OHLCV
       hidden_dim=64,
       num_layers=2
   )
   ```

3. **é€æ­¥å‡çº§**:
   ```python
   # å¦‚æžœæ•ˆæžœä¸å¥½ï¼Œå†å‡çº§åˆ°Transformer
   model = TransformerModel(...)
   ```

### é•¿æœŸè§„åˆ’

1. **å°è¯•ä¸åŒæž¶æž„**:
   - LSTM
   - GRU
   - Transformer
   - Temporal Convolutional Network (TCN)
   - Neural ODE

2. **å¤šæ—¶é—´å°ºåº¦èžåˆ**:
   ```python
   # åŒæ—¶ä½¿ç”¨1åˆ†é’Ÿã€5åˆ†é’Ÿã€15åˆ†é’Ÿæ•°æ®
   model = MultiScaleTransformer(
       scales=['1min', '5min', '15min']
   )
   ```

3. **æ³¨æ„åŠ›å¯è§†åŒ–**:
   ```python
   # ç†è§£æ¨¡åž‹å…³æ³¨ä»€ä¹ˆ
   attention_weights = model.get_attention_weights(x)
   visualize_attention(attention_weights)
   ```

---

## ðŸ’­ å›žç­”ç”¨æˆ·çš„æ ¸å¿ƒé—®é¢˜

### ä¸ºä½•è¦æ‰‹å·¥å®žçŽ°ç‰¹å¾å·¥ç¨‹ï¼Ÿ

**åŽ†å²åŽŸå› **:
- ä¼ ç»ŸMLï¼ˆSVMã€éšæœºæ£®æž—ç­‰ï¼‰éœ€è¦æ‰‹å·¥ç‰¹å¾
- æŠ€æœ¯åˆ†æžæœ‰æˆç†Ÿçš„æŒ‡æ ‡ä½“ç³»
- æ—©æœŸæ·±åº¦å­¦ä¹ æ•°æ®éœ€æ±‚å¤§

**çŽ°åœ¨ä¸å¿…è¦**:
- âœ… æ·±åº¦å­¦ä¹ å¯ä»¥è‡ªåŠ¨å­¦ä¹ ç‰¹å¾
- âœ… Transformerå¯ä»¥å¤„ç†é•¿åºåˆ—
- âœ… æ³¨æ„åŠ›æœºåˆ¶å¯ä»¥æ•èŽ·å¤æ‚ä¾èµ–
- âœ… æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ•°æ®ï¼ˆ6970æ¡ï¼‰

### ä¸ºä½•ä¸è®©æ¨¡åž‹è‡ªå·±è¯†åˆ«ï¼Ÿ

**åº”è¯¥è®©æ¨¡åž‹è‡ªå·±è¯†åˆ«ï¼**

**ç†ç”±**:
1. **å‡å°‘äººä¸ºé”™è¯¯**ï¼ˆå¦‚æœ¬æ¡ˆä¾‹ï¼‰
2. **å‘çŽ°æœªçŸ¥æ¨¡å¼**ï¼ˆè¶…è¶Šäººç±»è®¤çŸ¥ï¼‰
3. **é€‚åº”æ€§æ›´å¼º**ï¼ˆè‡ªåŠ¨è°ƒæ•´ï¼‰
4. **ç»´æŠ¤æ›´ç®€å•**ï¼ˆåªç»´æŠ¤æ¨¡åž‹ç»“æž„ï¼‰
5. **æ‰©å±•æ€§æ›´å¥½**ï¼ˆå®¹æ˜“æ·»åŠ æ–°æ•°æ®æºï¼‰

---

## ðŸš€ æŽ¨èæ–¹æ¡ˆ

### æœ€å°å¯è¡Œæ–¹æ¡ˆï¼ˆMVPï¼‰

```python
# 1. æ•°æ®å‡†å¤‡ï¼ˆæ— ç‰¹å¾å·¥ç¨‹ï¼‰
def load_raw_data(symbol, days):
    df = fetch_ohlcv(symbol, days)
    # åªåšç®€å•å½’ä¸€åŒ–
    scaler = StandardScaler()
    df[['open','high','low','close','volume']] = scaler.fit_transform(
        df[['open','high','low','close','volume']]
    )
    return df

# 2. åºåˆ—ç”Ÿæˆ
def create_sequences(df, seq_len=100, look_ahead=5):
    X, y = [], []
    for i in range(len(df) - seq_len - look_ahead):
        X.append(df.iloc[i:i+seq_len][
            ['open','high','low','close','volume']
        ].values)
        
        # æ ‡ç­¾ï¼šæœªæ¥ä»·æ ¼å˜åŒ–æ–¹å‘
        future_price = df.iloc[i+seq_len+look_ahead]['close']
        current_price = df.iloc[i+seq_len]['close']
        change = (future_price - current_price) / current_price
        
        if change > 0.001:  # >0.1%
            y.append(1)  # ä¹°å…¥
        elif change < -0.001:  # <-0.1%
            y.append(2)  # å–å‡º
        else:
            y.append(0)  # æŒæœ‰
    
    return np.array(X), np.array(y)

# 3. ç®€å•LSTMæ¨¡åž‹
class SimpleLSTM(nn.Module):
    def __init__(self, input_dim=5, hidden_dim=64, num_classes=3):
        super().__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, 2, 
                           batch_first=True, dropout=0.2)
        self.fc = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        _, (h, _) = self.lstm(x)
        return self.fc(h[-1])

# 4. è®­ç»ƒ
model = SimpleLSTM()
optimizer = Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(50):
    for batch_x, batch_y in train_loader:
        pred = model(batch_x)
        loss = criterion(pred, batch_y)
        loss.backward()
        optimizer.step()
```

**é¢„æœŸæ•ˆæžœ**:
- ä»£ç ç®€å•ï¼ˆ<100è¡Œï¼‰
- æ— ç‰¹å¾å·¥ç¨‹é”™è¯¯
- å‡†ç¡®çŽ‡50-65%ï¼ˆçœŸå®žæœ‰æ•ˆï¼‰
- æ˜“äºŽç»´æŠ¤å’Œæ‰©å±•

---

## ðŸ“š å‚è€ƒèµ„æº

### è®ºæ–‡

1. **Attention Is All You Need** (Vaswani et al., 2017)
   - Transformeræž¶æž„

2. **Temporal Fusion Transformers** (Lim et al., 2020)
   - æ—¶åºé¢„æµ‹çš„æœ€ä½³å®žè·µ

3. **Neural Oblivious Decision Trees** (Popov et al., 2019)
   - ç»“åˆæ·±åº¦å­¦ä¹ å’Œå¯è§£é‡Šæ€§

### å¼€æºé¡¹ç›®

1. **PyTorch Forecasting**
   - åŒ…å«Temporal Fusion Transformerå®žçŽ°

2. **Darts**
   - æ—¶åºé¢„æµ‹åº“ï¼Œå¤šç§æ¨¡åž‹

3. **Sktime**
   - æ—¶åºåˆ†æžå·¥å…·åŒ…

---

## âœ… è¡ŒåŠ¨æ¸…å•

**çŸ­æœŸ**ï¼ˆç«‹å³ï¼‰:
- [ ] åœæ­¢æ‰‹å·¥ç‰¹å¾å·¥ç¨‹
- [ ] å®žçŽ°ç®€å•çš„LSTMç«¯åˆ°ç«¯æ¨¡åž‹
- [ ] å¯¹æ¯”æ•ˆæžœï¼ˆæ‰‹å·¥ç‰¹å¾ vs ç«¯åˆ°ç«¯ï¼‰

**ä¸­æœŸ**ï¼ˆæœ¬å‘¨ï¼‰:
- [ ] å®žçŽ°Transformeræ¨¡åž‹
- [ ] å¤šæ—¶é—´å°ºåº¦èžåˆ
- [ ] æ³¨æ„åŠ›å¯è§†åŒ–

**é•¿æœŸ**ï¼ˆä¸‹å‘¨ï¼‰:
- [ ] å®žéªŒä¸åŒæž¶æž„
- [ ] è¶…å‚æ•°ä¼˜åŒ–
- [ ] åœ¨çº¿å­¦ä¹ /å¢žé‡å­¦ä¹ 

---

**ç»“è®º**: 

**æ‰‹å·¥ç‰¹å¾å·¥ç¨‹åœ¨æ·±åº¦å­¦ä¹ æ—¶ä»£å·²ç»ä¸æ˜¯å¿…é¡»çš„**ã€‚ç«¯åˆ°ç«¯å­¦ä¹ å¯ä»¥ï¼š
1. å‡å°‘äººä¸ºé”™è¯¯
2. å‘çŽ°æœªçŸ¥æ¨¡å¼
3. è‡ªåŠ¨é€‚åº”
4. æ˜“äºŽç»´æŠ¤

**å»ºè®®**: ä»Žç®€å•çš„LSTMå¼€å§‹ï¼Œè®©æ¨¡åž‹è‡ªå·±å­¦ä¹ ç‰¹å¾ï¼
