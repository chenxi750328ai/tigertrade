# 大模型防过拟合方案实施报告（更新版）

**实施时间**: 2026-01-27 11:30  
**原则**: 保持大容量（26.9M参数），使用大模型防过拟合方案

---

## 一、方案调整

### 1.1 从LoRA到PEFT（参数高效微调）

**问题**:
- LoRA实现复杂，需要处理PyTorch内部访问
- TransformerEncoderLayer内部的注意力层结构复杂
- 可训练参数占比仍然很高（91.69%）

**新方案**:
- **冻结Transformer所有层**（input_projection, transformer, attention_pool）
- **只训练分类头和收益率头**（这些层相对较小）

**优势**:
- 实现简单，无需修改模型结构
- 可训练参数大幅减少（只训练分类头和收益率头）
- 保持模型大容量（26.9M参数）

---

## 二、PEFT方案详情

### 2.1 参数统计（预期）

**模型结构**:
- Transformer层（冻结）: ~25M参数
- 分类头（可训练）: ~1M参数
- 收益率头（可训练）: ~0.9M参数

**预期可训练参数**: ~1.9M（约7%）
**数据/可训练参数比例**: 7,886 / 1.9M ≈ 4:1（合理）

### 2.2 训练配置

- **优化器**: AdamW(lr=2e-4, weight_decay=1e-3)
- **学习率调度**: CosineAnnealingLR(T_max=50, eta_min=1e-6)
- **损失函数**: Label Smoothing (0.2) + 类别权重
- **数据增强**: Mixup (alpha=0.2, 50%概率)
- **早停**: patience=10

---

## 三、为什么PEFT可能有效？

### 3.1 理论支持

**PEFT的核心思想**:
- 预训练模型已经学习了通用特征（Transformer层）
- 下游任务只需要微调任务特定的层（分类头和收益率头）
- 避免小数据覆盖通用特征

**优势**:
- 可训练参数大幅减少（26.9M → ~1.9M）
- 数据量/可训练参数比例从3,400:1降低到4:1
- 保持模型大容量，利用预训练知识

### 3.2 对比方案

| 方案 | 模型容量 | 可训练参数 | 数据/参数比例 | 方法 |
|------|---------|-----------|-------------|------|
| **原始模型** | 26.9M | 26.9M | 3,400:1 | 无正则化 |
| **降低容量** | 5.2M | 5.2M | 1.5:1 | 减少容量 |
| **PEFT（保持容量）** | 26.9M | **~1.9M** | **4:1** | **冻结Transformer** ⭐ |

---

## 四、实施细节

### 4.1 代码实现

```python
# 冻结Transformer的所有层
for name, param in base_model.named_parameters():
    if 'input_projection' in name or 'transformer' in name or 'attention_pool' in name:
        param.requires_grad = False

# 保持分类头和收益率头可训练
for name, param in base_model.named_parameters():
    if 'action_head' in name or 'profit_head' in name:
        param.requires_grad = True
```

### 4.2 训练配置

- **优化器**: 只优化可训练参数
- **学习率**: 2e-4（适合微调）
- **数据增强**: Mixup
- **标签平滑**: 0.2

---

## 五、预期效果

### 5.1 理论分析

**PEFT的优势**:
- 可训练参数从26.9M减少到~1.9M
- 数据量/可训练参数比例从3,400:1降低到4:1
- 过拟合风险大幅降低

**预期结果**:
- 验证准确率: 50%+（不再总是预测类别2）
- 训练稳定性: 训练和验证准确率差距减小

### 5.2 对比降低容量的方案

| 指标 | 降低容量 | PEFT（保持容量） |
|------|---------|----------------|
| 模型容量 | 5.2M | 26.9M |
| 可训练参数 | 5.2M | ~1.9M |
| 数据/参数比例 | 1.5:1 | 4:1 |
| 预训练知识 | ❌ | ✅ |
| 预期准确率 | 64.35% | 50%+ |

---

## 六、后续优化方向

如果PEFT仍然不够，可以进一步：

1. **应用LoRA到Transformer层**
   - 使用更复杂的LoRA实现
   - 应用到TransformerEncoderLayer内部

2. **获取更多数据**
   - 通过API获取更多历史数据
   - 目标: 20K+样本

3. **使用更强的数据增强**
   - 实现CutMix
   - 实现时间序列特定的增强

4. **模型融合**
   - 训练多个不同初始化的模型
   - 融合预测结果

---

**报告生成时间**: 2026-01-27 11:30  
**状态**: PEFT方案已实施，训练进行中
