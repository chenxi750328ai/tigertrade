# 大模型防过拟合方案实施报告

**实施时间**: 2026-01-27 11:00  
**原则**: 保持大容量（26.9M参数），使用大模型防过拟合方案

---

## 一、实施的方案

### 1.1 PEFT（参数高效微调）- LoRA ⭐核心

**原理**: 
- 冻结大模型的主权重（预训练的通用特征）
- 只训练少量新增的LoRA参数（占总参数量的0.1%-5%）
- 避免全量微调时小数据覆盖通用特征

**实施**:
- LoRA rank: 16
- LoRA alpha: 32
- 应用到: 注意力层（self_attn, attention_pool, input_projection）
- 预期可训练参数占比: <5%

**优势**:
- 保持模型大容量（26.9M参数）
- 只训练少量参数，大幅降低过拟合风险
- 利用预训练的知识

### 1.2 数据增强 - Mixup

**原理**: 
- 混合两个样本，生成新的有效样本
- 提升数据分布的丰富性

**实施**:
- Mixup alpha: 0.2
- 应用概率: 50%（随机应用）
- 混合: 输入特征和收益率标签

**优势**:
- 增加训练数据的多样性
- 提高模型泛化能力

### 1.3 预训练模型初始化

**原理**: 
- 利用无监督预训练的知识
- 从更好的起点开始训练

**实施**:
- 尝试加载`pretrained_return_model.pth`
- 只加载兼容的层

### 1.4 标签平滑 + 类别权重

**原理**: 
- 标签平滑：防止模型过度自信
- 类别权重：处理类别不平衡

**实施**:
- 标签平滑: 0.2
- 类别权重: [17.64, 0.76, 0.62]

### 1.5 训练策略优化

**实施**:
- 学习率: 2e-4（LoRA可以使用稍高的学习率）
- weight_decay: 1e-3
- CosineAnnealingLR: 平滑的学习率衰减
- 早停: patience=10

---

## 二、方案对比

### 2.1 所有方案对比

| 方案 | 模型容量 | 可训练参数 | 验证准确率 | 方法 |
|------|---------|-----------|-----------|------|
| **原始模型** | 26.9M | 26.9M | 33.57% | 无正则化 |
| **降低容量** | 5.2M | 5.2M | 64.35% | 减少容量 |
| **正则化（保持容量）** | 26.9M | 26.9M | 33.57% | 标签平滑+Dropout |
| **LoRA（保持容量）** | 26.9M | **<5%** | **待验证** | **LoRA+PEFT** ⭐ |

### 2.2 LoRA方案的优势

✅ **保持大容量**
- 模型参数量: 26.9M（保持）
- 不降低模型的学习能力

✅ **大幅减少可训练参数**
- 可训练参数: <5%（约1.3M）
- 数据量/可训练参数比例: 7.8K / 1.3M ≈ 6:1（合理）

✅ **利用预训练知识**
- 冻结预训练权重
- 只微调少量参数
- 避免覆盖通用特征

---

## 三、预期效果

### 3.1 理论分析

**LoRA的优势**:
- 可训练参数大幅减少（26.9M → <1.3M）
- 数据量/可训练参数比例从3,400:1降低到6:1
- 过拟合风险大幅降低

**Mixup的优势**:
- 增加训练数据的多样性
- 提高模型泛化能力

**组合效果**:
- 预期验证准确率: 50%+（不再总是预测类别2）
- 预期训练稳定性: 训练和验证准确率差距减小

### 3.2 对比降低容量的方案

| 指标 | 降低容量 | LoRA（保持容量） |
|------|---------|----------------|
| 模型容量 | 5.2M | 26.9M |
| 可训练参数 | 5.2M | <1.3M |
| 数据/参数比例 | 1.5:1 | 6:1 |
| 预训练知识 | ❌ | ✅ |
| 预期准确率 | 64.35% | 50%+ |

---

## 四、实施细节

### 4.1 LoRA实现

```python
# LoRA层结构
class LoRALinearWrapper:
    def forward(self, x):
        return original(x) + (dropout(x) @ lora_A.T @ lora_B.T * scaling)
```

**参数**:
- rank: 16（低秩矩阵的维度）
- alpha: 32（缩放因子）
- scaling: alpha / rank = 2.0

### 4.2 Mixup实现

```python
# Mixup数据增强
lam = np.random.beta(0.2, 0.2)
mixed_x = lam * x1 + (1 - lam) * x2
mixed_y_profit = lam * y_profit1 + (1 - lam) * y_profit2
```

### 4.3 训练配置

- **优化器**: AdamW(lr=2e-4, weight_decay=1e-3)
- **学习率调度**: CosineAnnealingLR(T_max=50, eta_min=1e-6)
- **损失函数**: Label Smoothing (0.2) + 类别权重
- **数据增强**: Mixup (alpha=0.2, 50%概率)

---

## 五、为什么LoRA可能有效？

### 5.1 理论支持

**PEFT的核心思想**:
- 预训练模型已经学习了通用特征
- 下游任务只需要微调少量参数
- 避免小数据覆盖通用特征

**LoRA的优势**:
- 低秩矩阵可以学习任务特定的适配
- 参数量少，过拟合风险低
- 保持模型的大容量

### 5.2 数据支持

**当前情况**:
- 数据量: 7,886个样本
- 可训练参数（LoRA）: <1.3M
- 比例: 6:1（合理范围）

**对比**:
- 原始模型: 3,400:1（严重过拟合）
- LoRA模型: 6:1（合理）

---

## 六、后续优化方向

如果LoRA仍然不够，可以进一步：

1. **增加LoRA rank**
   - rank: 16 → 32或64
   - 增加LoRA的容量

2. **应用到更多层**
   - 不仅注意力层，还包括FFN层
   - 增加LoRA的覆盖范围

3. **获取更多数据**
   - 通过API获取更多历史数据
   - 目标: 20K+样本

4. **使用更强的数据增强**
   - 实现CutMix
   - 实现时间序列特定的增强

---

**报告生成时间**: 2026-01-27 11:00  
**状态**: LoRA + Mixup方案已实施，训练进行中
