#!/usr/bin/env python3
"""
è®­ç»ƒæ‰€æœ‰çœŸå®æ¨¡å‹ï¼ˆLSTMã€Transformerã€åŒå¯¹æ¯”ï¼‰
"""

import sys
import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import json

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import TrainingConfig, FeatureConfig

# å¯¼å…¥æ‰€æœ‰æ¨¡å‹ç±»ï¼ˆä¸æ˜¯ç­–ç•¥ç±»ï¼‰
from strategies.llm_strategy import TradingLSTM
from strategies.large_model_strategy import LargeTradingNetwork  
from strategies.huge_transformer_strategy import HugeTransformer
from strategies.enhanced_transformer_strategy import EnhancedTradingTransformer
from strategies.rl_trading_strategy import RLTradingNetwork
from strategies.large_transformer_strategy import LargeTradingTransformer
from strategies.model_comparison_strategy import TradingLSTM as ComparisonLSTM, TradingTransformer as ComparisonTransformer


class TradingDataset(Dataset):
    """äº¤æ˜“æ•°æ®é›†"""
    
    def __init__(self, dataframe, feature_cols, label_col='label'):
        self.features = dataframe[feature_cols].values.astype(np.float32)
        self.labels = dataframe[label_col].values.astype(np.int64)
        
        # æ ‡å‡†åŒ–
        self.mean = self.features.mean(axis=0)
        self.std = self.features.std(axis=0) + 1e-8
        self.features = (self.features - self.mean) / self.std
        
        # æ£€æŸ¥NaN
        if np.isnan(self.features).any():
            print("âš ï¸ è­¦å‘Šï¼šç‰¹å¾ä¸­å­˜åœ¨NaNå€¼ï¼Œå·²æ›¿æ¢ä¸º0")
            self.features = np.nan_to_num(self.features)
    
    def __len__(self):
        return len(self.labels)
    
    def __getitem__(self, idx):
        return (
            torch.tensor(self.features[idx].copy(), dtype=torch.float32),
            torch.tensor(self.labels[idx], dtype=torch.long)
        )


def train_epoch(model, dataloader, criterion, optimizer, device):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for batch_data, batch_labels in dataloader:
        batch_data = batch_data.to(device)
        batch_labels = batch_labels.to(device)
        
        # æŸäº›æ¨¡å‹éœ€è¦3Dè¾“å…¥ï¼ˆbatch, seq, featuresï¼‰
        if len(batch_data.shape) == 2:
            batch_data = batch_data.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
        
        optimizer.zero_grad()
        outputs = model(batch_data)
        
        # æŸäº›æ¨¡å‹å¯èƒ½è¿”å›å¤šä¸ªå€¼
        if isinstance(outputs, tuple):
            outputs = outputs[0]
        
        # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
        if len(outputs.shape) == 3:
            outputs = outputs.squeeze(1)
        
        loss = criterion(outputs, batch_labels)
        
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        total += batch_labels.size(0)
        correct += (predicted == batch_labels).sum().item()
    
    return total_loss / len(dataloader), correct / total


def validate(model, dataloader, criterion, device):
    """éªŒè¯"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for batch_data, batch_labels in dataloader:
            batch_data = batch_data.to(device)
            batch_labels = batch_labels.to(device)
            
            # æŸäº›æ¨¡å‹éœ€è¦3Dè¾“å…¥
            if len(batch_data.shape) == 2:
                batch_data = batch_data.unsqueeze(1)
            
            outputs = model(batch_data)
            
            # æŸäº›æ¨¡å‹å¯èƒ½è¿”å›å¤šä¸ªå€¼
            if isinstance(outputs, tuple):
                outputs = outputs[0]
            
            # ç¡®ä¿è¾“å‡ºç»´åº¦æ­£ç¡®
            if len(outputs.shape) == 3:
                outputs = outputs.squeeze(1)
            
            loss = criterion(outputs, batch_labels)
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_labels.size(0)
            correct += (predicted == batch_labels).sum().item()
    
    return total_loss / len(dataloader), correct / total


def get_all_models(input_dim):
    """è·å–æ‰€æœ‰æ¨¡å‹é…ç½®"""
    models = {
        'LSTMæ¨¡å‹ (LLMç­–ç•¥)': lambda: TradingLSTM(input_size=input_dim, hidden_size=64, num_layers=2, output_size=3),
        'å¤§å‹LSTMæ¨¡å‹': lambda: LargeTradingNetwork(input_size=input_dim, hidden_size=256, num_layers=4, output_size=3),
        'å¼ºåŒ–å­¦ä¹ ç½‘ç»œ (LSTM)': lambda: RLTradingNetwork(input_size=input_dim, action_size=3, hidden_size=512, num_layers=4),
        'å¤§å‹Transformer (256ç»´-6å±‚)': lambda: LargeTradingTransformer(input_size=input_dim, nhead=8, num_layers=6, output_size=3, d_model=256),
        'è¶…å¤§Transformer (512ç»´-8å±‚)': lambda: HugeTransformer(input_size=input_dim, d_model=512, nhead=8, num_layers=8, output_size=3),
        'å¢å¼ºå‹Transformer (512ç»´-8å±‚+æ³¨æ„åŠ›æ± åŒ–)': lambda: EnhancedTradingTransformer(input_size=input_dim, nhead=8, num_layers=8, output_size=3, d_model=512),
        'å¯¹æ¯”æ¨¡å‹-LSTM': lambda: ComparisonLSTM(input_size=input_dim, hidden_size=64, num_layers=2, output_size=3),
        'å¯¹æ¯”æ¨¡å‹-Transformer': lambda: ComparisonTransformer(input_size=input_dim, nhead=2, num_layers=2, output_size=3, d_model=64),
    }
    return models


def train_single_model(model_name, model_fn, train_loader, val_loader, device, output_dir):
    """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {model_name}")
    print(f"{'='*80}")
    
    try:
        model = model_fn().to(device)
        
        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        
        # ä¼˜åŒ–å™¨å’ŒæŸå¤±
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        label_counts = np.bincount([label for _, label in train_loader.dataset])
        class_weights = torch.FloatTensor([1.0 / count if count > 0 else 1.0 for count in label_counts])
        class_weights = class_weights / class_weights.sum() * len(class_weights)
        class_weights = class_weights.to(device)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5)
        
        # è®­ç»ƒ
        best_val_acc = 0
        patience_counter = 0
        results = {
            'model_name': model_name,
            'total_params': total_params,
            'epochs': [],
            'best_val_acc': 0,
            'best_epoch': 0,
            'total_time': 0
        }
        
        start_time = time.time()
        
        for epoch in range(1, 31):  # æœ€å¤š30è½®
            train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)
            val_loss, val_acc = validate(model, val_loader, criterion, device)
            
            scheduler.step(val_acc)
            current_lr = optimizer.param_groups[0]['lr']
            
            results['epochs'].append({
                'epoch': epoch,
                'train_loss': float(train_loss),
                'train_acc': float(train_acc),
                'val_loss': float(val_loss),
                'val_acc': float(val_acc),
                'lr': float(current_lr)
            })
            
            print(f"Epoch {epoch:2d} - Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | "
                  f"Val: Loss={val_loss:.4f}, Acc={val_acc:.4f} | LR={current_lr:.6f}")
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                results['best_val_acc'] = float(best_val_acc)
                results['best_epoch'] = epoch
                
                # ä¿å­˜æ¨¡å‹
                safe_name = model_name.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '-').replace('+', '_')
                model_path = os.path.join(output_dir, f'{safe_name}_best.pth')
                torch.save(model.state_dict(), model_path)
                print(f"  ğŸ† æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}")
                patience_counter = 0
            else:
                patience_counter += 1
                
            if patience_counter >= 10:
                print(f"  â¹ï¸ æ—©åœè§¦å‘")
                break
        
        total_time = time.time() - start_time
        results['total_time'] = float(total_time)
        
        print(f"\nâœ… {model_name} è®­ç»ƒå®Œæˆ!")
        print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (Epoch {results['best_epoch']})")
        print(f"  è®­ç»ƒè€—æ—¶: {total_time:.1f}ç§’")
        
        return results
        
    except Exception as e:
        print(f"âŒ {model_name} è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒæ‰€æœ‰çœŸå®æ¨¡å‹')
    parser.add_argument('--train-file', type=str, required=True)
    parser.add_argument('--val-file', type=str, required=True)
    parser.add_argument('--output-dir', type=str, default='/home/cx/trading_data/all_real_models')
    
    args = parser.parse_args()
    
    os.makedirs(args.output_dir, exist_ok=True)
    
    print("="*80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰çœŸå®æ¨¡å‹ (LSTM + Transformer + åŒå¯¹æ¯”)")
    print("="*80)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    print(f"\nåŠ è½½æ•°æ®...")
    train_df = pd.read_csv(args.train_file, index_col=0)
    val_df = pd.read_csv(args.val_file, index_col=0)
    print(f"è®­ç»ƒé›†: {len(train_df)}, éªŒè¯é›†: {len(val_df)}")
    
    # å‡†å¤‡æ•°æ®
    feature_cols = FeatureConfig.get_all_features()
    input_dim = len(feature_cols)
    print(f"ç‰¹å¾æ•°é‡: {input_dim}")
    
    train_dataset = TradingDataset(train_df, feature_cols)
    val_dataset = TradingDataset(val_df, feature_cols)
    
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
    
    # è·å–æ‰€æœ‰æ¨¡å‹
    all_models = get_all_models(input_dim)
    print(f"\næ‰¾åˆ° {len(all_models)} ä¸ªæ¨¡å‹:")
    for i, name in enumerate(all_models.keys(), 1):
        print(f"  {i}. {name}")
    
    # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
    all_results = []
    
    for model_name, model_fn in all_models.items():
        result = train_single_model(model_name, model_fn, train_loader, val_loader, device, args.output_dir)
        if result:
            all_results.append(result)
    
    # ä¿å­˜ç»“æœ
    results_file = os.path.join(args.output_dir, 'all_models_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    print(f"\n{'='*80}")
    print("ğŸ“Š æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    print(f"{'='*80}")
    
    # æ’åºæ˜¾ç¤º
    all_results.sort(key=lambda x: x['best_val_acc'], reverse=True)
    
    print(f"\næ’åç»“æœ:")
    print("-" * 80)
    for i, result in enumerate(all_results, 1):
        print(f"{i}. {result['model_name']}: {result['best_val_acc']:.4f} "
              f"(Epoch {result['best_epoch']}, {result['total_params']:,} å‚æ•°, {result['total_time']:.1f}s)")
    
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")


if __name__ == "__main__":
    main()
