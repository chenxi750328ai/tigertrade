#!/usr/bin/env python3
"""
ä½¿ç”¨ä¸åŒé…ç½®è®­ç»ƒå¤šä¸ªTransformeræ¨¡å‹
"""

import sys
import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from datetime import datetime
import argparse
import json

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import TrainingConfig, FeatureConfig
from train_with_detailed_logging import ImprovedTransformer, TradingDataset, DetailedLogger, train_epoch, validate


def get_model_configs():
    """å®šä¹‰ä¸åŒçš„æ¨¡å‹é…ç½®"""
    configs = {
        'å°å‹æ¨¡å‹ (128ç»´)': {
            'hidden_dim': 128,
            'num_heads': 4,
            'num_layers': 2,
            'dropout': 0.1
        },
        'ä¸­å‹æ¨¡å‹ (256ç»´)': {
            'hidden_dim': 256,
            'num_heads': 8,
            'num_layers': 3,
            'dropout': 0.1
        },
        'å¤§å‹æ¨¡å‹ (512ç»´)': {
            'hidden_dim': 512,
            'num_heads': 8,
            'num_layers': 4,
            'dropout': 0.2
        },
        'æ·±å±‚æ¨¡å‹ (128ç»´-6å±‚)': {
            'hidden_dim': 128,
            'num_heads': 4,
            'num_layers': 6,
            'dropout': 0.15
        },
        'å®½å±‚æ¨¡å‹ (384ç»´-2å±‚)': {
            'hidden_dim': 384,
            'num_heads': 6,
            'num_layers': 2,
            'dropout': 0.1
        },
        'è¶…å¤§æ¨¡å‹ (768ç»´)': {
            'hidden_dim': 768,
            'num_heads': 8,
            'num_layers': 4,
            'dropout': 0.2
        },
    }
    return configs


def train_model_with_config(config_name, config, train_loader, val_loader, device, logger, output_dir, input_dim):
    """ä½¿ç”¨æŒ‡å®šé…ç½®è®­ç»ƒæ¨¡å‹"""
    logger.log(f"\n{'='*80}")
    logger.log(f"ğŸš€ å¼€å§‹è®­ç»ƒ: {config_name}")
    logger.log(f"{'='*80}")
    logger.log(f"é…ç½®: {config}")
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = ImprovedTransformer(
            input_dim=input_dim,
            hidden_dim=config['hidden_dim'],
            num_heads=config['num_heads'],
            num_layers=config['num_layers'],
            dropout=config['dropout'],
            num_classes=3
        ).to(device)
        
        # ç»Ÿè®¡å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        logger.log(f"æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        
        # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.Adam(model.parameters(), lr=TrainingConfig.LEARNING_RATE)
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        label_counts = np.bincount([label for _, label in train_loader.dataset])
        class_weights = torch.FloatTensor([1.0 / count if count > 0 else 1.0 for count in label_counts])
        class_weights = class_weights / class_weights.sum() * len(class_weights)
        class_weights = class_weights.to(device)
        
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='max', factor=0.5, patience=TrainingConfig.PATIENCE // 2
        )
        
        # è®­ç»ƒ
        best_val_acc = 0
        patience_counter = 0
        results = {
            'config_name': config_name,
            'config': config,
            'total_params': total_params,
            'epochs': [],
            'best_val_acc': 0,
            'best_epoch': 0,
            'total_time': 0
        }
        
        start_time = time.time()
        
        for epoch in range(1, min(TrainingConfig.MAX_EPOCHS, 30) + 1):  # æœ€å¤š30è½®
            logger.log(f"\nEpoch {epoch}")
            logger.log("-" * 80)
            
            # è®­ç»ƒ
            train_loss, train_acc = train_epoch(
                model, train_loader, criterion, optimizer, device, logger, epoch
            )
            
            # éªŒè¯
            val_loss, val_acc = validate(model, val_loader, criterion, device, logger, epoch)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(val_acc)
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•ç»“æœ
            epoch_result = {
                'epoch': epoch,
                'train_loss': float(train_loss),
                'train_acc': float(train_acc),
                'val_loss': float(val_loss),
                'val_acc': float(val_acc),
                'lr': float(current_lr)
            }
            results['epochs'].append(epoch_result)
            
            logger.log(f"Epoch {epoch:2d} - Train: Loss={train_loss:.4f}, Acc={train_acc:.4f} | "
                      f"Val: Loss={val_loss:.4f}, Acc={val_acc:.4f} | LR={current_lr:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                results['best_val_acc'] = float(best_val_acc)
                results['best_epoch'] = epoch
                
                # ä¿å­˜æ¨¡å‹
                safe_name = config_name.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '-')
                model_path = os.path.join(output_dir, f'{safe_name}_best.pth')
                torch.save(model.state_dict(), model_path)
                logger.log(f"  ğŸ† æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_val_acc:.4f}, æ¨¡å‹å·²ä¿å­˜")
                patience_counter = 0
            else:
                patience_counter += 1
                
            # æ—©åœ
            if patience_counter >= TrainingConfig.PATIENCE:
                logger.log(f"  â¹ï¸ æ—©åœè§¦å‘")
                break
        
        total_time = time.time() - start_time
        results['total_time'] = float(total_time)
        
        logger.log(f"\nâœ… {config_name} è®­ç»ƒå®Œæˆ!")
        logger.log(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (Epoch {results['best_epoch']})")
        logger.log(f"  è®­ç»ƒè€—æ—¶: {total_time:.2f}ç§’")
        logger.log(f"  å‚æ•°æ•°é‡: {total_params:,}")
        
        return results
        
    except Exception as e:
        logger.log_error(f"âŒ {config_name} è®­ç»ƒå¤±è´¥: {e}", e)
        import traceback
        traceback.print_exc()
        return None


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è®­ç»ƒå¤šä¸ªé…ç½®çš„æ¨¡å‹')
    parser.add_argument('--train-file', type=str, required=True, help='è®­ç»ƒæ•°æ®æ–‡ä»¶')
    parser.add_argument('--val-file', type=str, required=True, help='éªŒè¯æ•°æ®æ–‡ä»¶')
    parser.add_argument('--output-dir', type=str, default='/home/cx/trading_data/model_comparison', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    log_dir = os.path.join(args.output_dir, 'logs')
    os.makedirs(log_dir, exist_ok=True)
    
    # åˆ›å»ºæ—¥å¿—
    logger = DetailedLogger(log_dir)
    
    logger.log("="*80)
    logger.log("ğŸš€ å¼€å§‹è®­ç»ƒå¤šä¸ªé…ç½®çš„æ¨¡å‹")
    logger.log("="*80)
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() and TrainingConfig.DEVICE == 'cuda' else 'cpu')
    logger.log(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    logger.log(f"\nåŠ è½½æ•°æ®...")
    logger.log(f"è®­ç»ƒé›†: {args.train_file}")
    logger.log(f"éªŒè¯é›†: {args.val_file}")
    
    train_df = pd.read_csv(args.train_file, index_col=0)
    val_df = pd.read_csv(args.val_file, index_col=0)
    
    logger.log(f"è®­ç»ƒé›†å¤§å°: {len(train_df)}")
    logger.log(f"éªŒè¯é›†å¤§å°: {len(val_df)}")
    
    # å‡†å¤‡æ•°æ®é›†
    feature_cols = FeatureConfig.get_all_features()
    input_dim = len(feature_cols)
    logger.log(f"ç‰¹å¾æ•°é‡: {input_dim}")
    
    train_dataset = TradingDataset(train_df, feature_cols)
    val_dataset = TradingDataset(val_df, feature_cols)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=TrainingConfig.BATCH_SIZE,
        shuffle=True
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=TrainingConfig.BATCH_SIZE,
        shuffle=False
    )
    
    # è·å–æ‰€æœ‰é…ç½®
    all_configs = get_model_configs()
    logger.log(f"\næ‰¾åˆ° {len(all_configs)} ä¸ªæ¨¡å‹é…ç½®:")
    for name in all_configs.keys():
        logger.log(f"  - {name}")
    
    # è®­ç»ƒæ‰€æœ‰é…ç½®
    all_results = []
    
    for config_name, config in all_configs.items():
        result = train_model_with_config(
            config_name, 
            config, 
            train_loader, 
            val_loader, 
            device, 
            logger,
            args.output_dir,
            input_dim
        )
        
        if result:
            all_results.append(result)
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    results_file = os.path.join(args.output_dir, 'model_comparison_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    
    logger.log(f"\n{'='*80}")
    logger.log("ğŸ“Š æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    logger.log(f"{'='*80}")
    
    # æ’åºå¹¶æ˜¾ç¤ºç»“æœ
    all_results.sort(key=lambda x: x['best_val_acc'], reverse=True)
    
    logger.log(f"\næ’åç»“æœ (æŒ‰éªŒè¯å‡†ç¡®ç‡):")
    logger.log("-" * 80)
    for i, result in enumerate(all_results, 1):
        logger.log(f"{i}. {result['config_name']}: "
                  f"{result['best_val_acc']:.4f} (Epoch {result['best_epoch']}, "
                  f"{result['total_params']:,} å‚æ•°, {result['total_time']:.1f}s)")
    
    logger.log(f"\nç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ç”ŸæˆMarkdownæŠ¥å‘Š
    md_file = os.path.join(args.output_dir, 'comparison_report.md')
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write("# æ¨¡å‹é…ç½®å¯¹æ¯”æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## æ’åç»“æœ\n\n")
        f.write("| æ’å | æ¨¡å‹é…ç½® | éªŒè¯å‡†ç¡®ç‡ | æœ€ä½³Epoch | å‚æ•°æ•°é‡ | è®­ç»ƒæ—¶é—´(ç§’) |\n")
        f.write("|------|---------|-----------|----------|----------|-------------|\n")
        for i, result in enumerate(all_results, 1):
            f.write(f"| {i} | {result['config_name']} | {result['best_val_acc']:.4f} | "
                   f"{result['best_epoch']} | {result['total_params']:,} | {result['total_time']:.1f} |\n")
        
        f.write("\n## è¯¦ç»†é…ç½®\n\n")
        for result in all_results:
            f.write(f"### {result['config_name']}\n\n")
            f.write(f"- **éªŒè¯å‡†ç¡®ç‡**: {result['best_val_acc']:.4f}\n")
            f.write(f"- **æœ€ä½³Epoch**: {result['best_epoch']}\n")
            f.write(f"- **å‚æ•°æ•°é‡**: {result['total_params']:,}\n")
            f.write(f"- **è®­ç»ƒæ—¶é—´**: {result['total_time']:.1f}ç§’\n")
            f.write(f"- **é…ç½®**:\n")
            for k, v in result['config'].items():
                f.write(f"  - {k}: {v}\n")
            f.write("\n")
    
    logger.log(f"MarkdownæŠ¥å‘Šå·²ä¿å­˜åˆ°: {md_file}")


if __name__ == "__main__":
    main()
